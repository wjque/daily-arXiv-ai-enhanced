{"id": "2509.20903", "categories": ["cs.CG"], "pdf": "https://arxiv.org/pdf/2509.20903", "abs": "https://arxiv.org/abs/2509.20903", "authors": ["Nicol\u00e1s Honorato-Droguett", "Kazuhiro Kurita", "Tesshu Hanaka", "Hirotaka Ono", "Alexander Wolff"], "title": "Further Results on Rendering Geometric Intersection Graphs Sparse by Dispersion", "comment": "Submitted to WALCOM 2026", "summary": "Removing overlaps is a central task in domains such as scheduling,\nvisibility, and map labelling. This task can be modelled using graphs, where\noverlap removals correspond to enforcing a certain sparsity constraint on the\ngraph structure. We continue the study of the problem Geometric Graph Edit\nDistance, where the aim is to minimise the total cost of editing a geometric\nintersection graph to obtain a graph contained in a specific graph class. For\nus, the edit operation is the movement of objects, and the cost is the movement\ndistance. We present an algorithm for rendering the intersection graph of a set\nof unit circular arcs (i)~edgeless, (ii)~acyclic, and (iii)~$k$-clique-free in\n$O(n\\log n)$ time, where $n$ is the number of arcs. We also show that the\nproblem remains strongly NP-hard on unweighted interval graphs, solving an open\nproblem of [Honorato-Droguett et al., WADS 2025]. We complement this result by\nshowing that the problem is strongly NP-hard on tuples of $d$-balls and\n$d$-cubes, for any $d\\ge 2$. Finally, we present an XP algorithm (parameterised\nby the number of maximal cliques) for rendering the intersection graph of a set\nof weighted unit intervals edgeless.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u51e0\u4f55\u56fe\u7f16\u8f91\u8ddd\u79bb\u95ee\u9898\uff0c\u63d0\u51fa\u7b97\u6cd5\u5728O(n log n)\u65f6\u95f4\u5185\u5c06\u5355\u4f4d\u5706\u5f27\u7684\u4ea4\u56fe\u8f6c\u6362\u4e3a\u65e0\u8fb9\u3001\u65e0\u73af\u6216k-\u56e2\u81ea\u7531\u56fe\uff0c\u8bc1\u660e\u4e86\u8be5\u95ee\u9898\u5728\u65e0\u6743\u533a\u95f4\u56fe\u4e0a\u5f3aNP\u96be\uff0c\u5e76\u7ed9\u51fa\u4e86\u52a0\u6743\u5355\u4f4d\u533a\u95f4\u4ea4\u56fe\u7684XP\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8c03\u5ea6\u3001\u53ef\u89c6\u5316\u548c\u5730\u56fe\u6807\u6ce8\u7b49\u9886\u57df\u4e2d\u7684\u91cd\u53e0\u79fb\u9664\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u53ef\u5efa\u6a21\u4e3a\u56fe\u7f16\u8f91\u8ddd\u79bb\u95ee\u9898\uff0c\u76ee\u6807\u662f\u901a\u8fc7\u79fb\u52a8\u5bf9\u8c61\u6700\u5c0f\u5316\u7f16\u8f91\u6210\u672c\u3002", "method": "\u4f7f\u7528\u51e0\u4f55\u56fe\u7f16\u8f91\u8ddd\u79bb\u6a21\u578b\uff0c\u901a\u8fc7\u79fb\u52a8\u5bf9\u8c61\u6765\u7f16\u8f91\u51e0\u4f55\u4ea4\u56fe\u7ed3\u6784\uff0c\u63d0\u51fa\u9488\u5bf9\u5355\u4f4d\u5706\u5f27\u4ea4\u56fe\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u56fe\u7c7b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5f00\u53d1\u4e86O(n log n)\u65f6\u95f4\u7b97\u6cd5\u5904\u7406\u5355\u4f4d\u5706\u5f27\u4ea4\u56fe\uff1b\u8bc1\u660e\u4e86\u95ee\u9898\u5728\u65e0\u6743\u533a\u95f4\u56fe\u4e0a\u5f3aNP\u96be\uff1b\u7ed9\u51fa\u4e86\u52a0\u6743\u5355\u4f4d\u533a\u95f4\u4ea4\u56fe\u7684XP\u7b97\u6cd5\u3002", "conclusion": "\u51e0\u4f55\u56fe\u7f16\u8f91\u8ddd\u79bb\u95ee\u9898\u5728\u7279\u5b9a\u56fe\u7c7b\u4e0a\u53ef\u9ad8\u6548\u6c42\u89e3\uff0c\u4f46\u5728\u4e00\u822c\u56fe\u7c7b\u4e0a\u5177\u6709\u8ba1\u7b97\u96be\u5ea6\uff0c\u4e3a\u91cd\u53e0\u79fb\u9664\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u57fa\u7840\u3002"}}
{"id": "2509.20486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20486", "abs": "https://arxiv.org/abs/2509.20486", "authors": ["Sven Ochs", "Philip Sch\u00f6rner", "Marc Ren\u00e9 Zofka", "J. Marius Z\u00f6llner"], "title": "Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation", "comment": null, "summary": "Semantic segmentation of LiDAR data presents considerable challenges,\nparticularly when dealing with diverse sensor types and configurations.\nHowever, incorporating semantic information can significantly enhance the\naccuracy and robustness of LiDAR-based localization techniques for autonomous\nmobile systems. We propose an approach that integrates semantic camera data\nwith LiDAR segmentation to address this challenge. By projecting LiDAR points\ninto the semantic segmentation space of the camera, our method enhances the\nprecision and reliability of the LiDAR-based localization pipeline.\n  For validation, we utilize the CoCar NextGen platform from the FZI Research\nCenter for Information Technology, which offers diverse sensor modalities and\nconfigurations. The sensor setup of CoCar NextGen enables a thorough analysis\nof different sensor types. Our evaluation leverages the state-of-the-art\nDepth-Anything network for camera image segmentation and an adaptive\nsegmentation network for LiDAR segmentation. To establish a reliable ground\ntruth for LiDAR-based localization, we make us of a Global Navigation Satellite\nSystem (GNSS) solution with Real-Time Kinematic corrections (RTK).\nAdditionally, we conduct an extensive 55 km drive through the city of\nKarlsruhe, Germany, covering a variety of environments, including urban areas,\nmulti-lane roads, and rural highways. This multimodal approach paves the way\nfor more reliable and precise autonomous navigation systems, particularly in\ncomplex real-world environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u8bed\u4e49\u76f8\u673a\u6570\u636e\u548cLiDAR\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LiDAR\u70b9\u6295\u5f71\u5230\u76f8\u673a\u8bed\u4e49\u5206\u5272\u7a7a\u95f4\u6765\u63d0\u5347LiDAR\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "LiDAR\u6570\u636e\u7684\u8bed\u4e49\u5206\u5272\u5728\u5904\u7406\u4e0d\u540c\u4f20\u611f\u5668\u7c7b\u578b\u548c\u914d\u7f6e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4f46\u8bed\u4e49\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u5347LiDAR\u5b9a\u4f4d\u6280\u672f\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528CoCar NextGen\u5e73\u53f0\u7684\u591a\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u7ed3\u5408Depth-Anything\u7f51\u7edc\u8fdb\u884c\u76f8\u673a\u56fe\u50cf\u5206\u5272\u548c\u81ea\u9002\u5e94\u5206\u5272\u7f51\u7edc\u8fdb\u884cLiDAR\u5206\u5272\uff0c\u5c06LiDAR\u70b9\u6295\u5f71\u5230\u76f8\u673a\u8bed\u4e49\u5206\u5272\u7a7a\u95f4\u3002", "result": "\u5728\u5fb7\u56fd\u5361\u5c14\u65af\u9c81\u538455\u516c\u91cc\u7684\u591a\u6837\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5305\u62ec\u57ce\u5e02\u533a\u57df\u3001\u591a\u8f66\u9053\u9053\u8def\u548c\u4e61\u6751\u9ad8\u901f\u516c\u8def\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\u4e3a\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u66f4\u53ef\u9760\u548c\u7cbe\u786e\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.20488", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20488", "abs": "https://arxiv.org/abs/2509.20488", "authors": ["Atef Azaiez", "David A. Anisi", "Marie Farrell", "Matt Luckcuck"], "title": "Revisiting Formal Methods for Autonomous Robots: A Structured Survey", "comment": "Appeal accepted: MOD-66548 This is an appeal request regarding our\n  submission MOD-65174 - 6681725", "summary": "This paper presents the initial results from our structured literature review\non applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We\ndescribe our structured survey methodology; including database selection and\nassociated search strings, search filters and collaborative review of\nidentified papers. We categorise and enumerate the FM approaches and formalisms\nthat have been used for specification and verification of RAS. We investigate\nFM in the context of sub-symbolic AI-enabled RAS and examine the evolution of\nhow FM is used over time in this field. This work complements a pre-existing\nsurvey in this area and we examine how this research area has matured over\ntime. Specifically, our survey demonstrates that some trends have persisted as\nobserved in a previous survey. Additionally, it recognized new trends that were\nnot considered previously including a noticeable increase in adopting Formal\nSynthesis approaches as well as Probabilistic Verification Techniques.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u8c03\u67e5\u4e86\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u65b9\u6cd5\u5206\u7c7b\u3001\u5f62\u5f0f\u5316\u5de5\u5177\u4f7f\u7528\u60c5\u51b5\uff0c\u4ee5\u53ca\u8be5\u9886\u57df\u968f\u65f6\u95f4\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u8865\u5145\u73b0\u6709\u5173\u4e8e\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\u5e94\u7528\u9886\u57df\u7684\u8c03\u67e5\uff0c\u5206\u6790\u8be5\u7814\u7a76\u9886\u57df\u7684\u6210\u719f\u5ea6\u548c\u53d1\u5c55\u8d8b\u52bf\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u5e93\u9009\u62e9\u3001\u641c\u7d22\u5b57\u7b26\u4e32\u8bbe\u8ba1\u3001\u7b5b\u9009\u8fc7\u6ee4\u548c\u534f\u4f5c\u8bc4\u5ba1\u5df2\u8bc6\u522b\u8bba\u6587\u3002", "result": "\u8c03\u67e5\u663e\u793a\u67d0\u4e9b\u8d8b\u52bf\u4e0e\u5148\u524d\u8c03\u67e5\u4e00\u81f4\uff0c\u540c\u65f6\u8bc6\u522b\u51fa\u65b0\u8d8b\u52bf\uff0c\u5305\u62ec\u5f62\u5f0f\u5316\u7efc\u5408\u65b9\u6cd5\u548c\u6982\u7387\u9a8c\u8bc1\u6280\u672f\u7684\u663e\u8457\u589e\u52a0\u3002", "conclusion": "\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\u9886\u57df\u7684\u5e94\u7528\u6b63\u5728\u6210\u719f\uff0c\u51fa\u73b0\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u6280\u672f\u8d8b\u52bf\u3002"}}
{"id": "2509.20499", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20499", "abs": "https://arxiv.org/abs/2509.20499", "authors": ["Boqi Li", "Siyuan Li", "Weiyi Wang", "Anran Li", "Zhong Cao", "Henry X. Liu"], "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting", "comment": null, "summary": "With the rapid progress of foundation models and robotics, vision-language\nnavigation (VLN) has emerged as a key task for embodied agents with broad\npractical applications. We address VLN in continuous environments, a\nparticularly challenging setting where an agent must jointly interpret natural\nlanguage instructions, perceive its surroundings, and plan low-level actions.\nWe propose a zero-shot framework that integrates a simplified yet effective\nwaypoint predictor with a multimodal large language model (MLLM). The predictor\noperates on an abstract obstacle map, producing linearly reachable waypoints,\nwhich are incorporated into a dynamically updated topological graph with\nexplicit visitation records. The graph and visitation information are encoded\ninto the prompt, enabling reasoning over both spatial structure and exploration\nhistory to encourage exploration and equip MLLM with local path planning for\nerror correction. Extensive experiments on R2R-CE and RxR-CE show that our\nmethod achieves state-of-the-art zero-shot performance, with success rates of\n41% and 36%, respectively, outperforming prior state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u7b80\u5316\u7684\u8def\u5f84\u70b9\u9884\u6d4b\u5668\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6210\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u7684\u5173\u952e\u4efb\u52a1\u3002\u8fde\u7eed\u73af\u5883\u4e0b\u7684VLN\u7279\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u540c\u65f6\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u611f\u77e5\u73af\u5883\u548c\u89c4\u5212\u5e95\u5c42\u52a8\u4f5c\u3002", "method": "\u4f7f\u7528\u7b80\u5316\u7684\u8def\u5f84\u70b9\u9884\u6d4b\u5668\u5728\u62bd\u8c61\u969c\u788d\u5730\u56fe\u4e0a\u751f\u6210\u7ebf\u6027\u53ef\u8fbe\u8def\u5f84\u70b9\uff0c\u6784\u5efa\u52a8\u6001\u66f4\u65b0\u7684\u62d3\u6251\u56fe\u5e76\u8bb0\u5f55\u8bbf\u95ee\u5386\u53f2\u3002\u5c06\u56fe\u548c\u8bbf\u95ee\u4fe1\u606f\u7f16\u7801\u5230\u63d0\u793a\u4e2d\uff0c\u4f7fMLLM\u80fd\u591f\u8fdb\u884c\u7a7a\u95f4\u7ed3\u6784\u548c\u63a2\u7d22\u5386\u53f2\u7684\u63a8\u7406\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u6210\u529f\u7387\u5206\u522b\u8fbe\u523041%\u548c36%\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8def\u5f84\u70b9\u9884\u6d4b\u548cMLLM\u63a8\u7406\uff0c\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.20510", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20510", "abs": "https://arxiv.org/abs/2509.20510", "authors": ["Petr Trunin", "Diana Cafiso", "Anderson Brazil Nardin", "Trevor Exley", "Lucia Beccai"], "title": "MELEGROS: Monolithic Elephant-inspired Gripper with Optical Sensors", "comment": "15 pages, 6 figures. SI 18 pages, 19 figures. Submitted to Wiley\n  Advanced Science", "summary": "The elephant trunk exemplifies a natural gripper where structure, actuation,\nand sensing are seamlessly integrated. Inspired by the distal morphology of the\nAfrican elephant trunk, we present MELEGROS, a Monolithic ELEphant-inspired\nGRipper with Optical Sensors, emphasizing sensing as an intrinsic,\nco-fabricated capability. Unlike multi-material or tendon-based approaches,\nMELEGROS directly integrates six optical waveguide sensors and five pneumatic\nchambers into a pneumatically actuated lattice structure (12.5 mm cell size)\nusing a single soft resin and one continuous 3D print. This eliminates\nmechanical mismatches between sensors, actuators, and body, reducing model\nuncertainty and enabling simulation-guided sensor design and placement. Only\nfour iterations were required to achieve the final prototype, which features a\ncontinuous structure capable of elongation, compression, and bending while\ndecoupling tactile and proprioceptive signals. MELEGROS (132 g) lifts more than\ntwice its weight, performs bioinspired actions such as pinching, scooping, and\nreaching, and delicately grasps fragile items like grapes. The integrated\noptical sensors provide distinct responses to touch, bending, and chamber\ndeformation, enabling multifunctional perception. MELEGROS demonstrates a new\nparadigm for soft robotics where fully embedded sensing and continuous\nstructures inherently support versatile, bioinspired manipulation.", "AI": {"tldr": "MELEGROS\u662f\u4e00\u4e2a\u53d7\u5927\u8c61\u9f3b\u5b50\u542f\u53d1\u7684\u5355\u5757\u8f6f\u4f53\u6293\u53d6\u5668\uff0c\u901a\u8fc73D\u6253\u5370\u5c06\u5149\u5b66\u4f20\u611f\u5668\u548c\u6c14\u52a8\u8154\u96c6\u6210\u5230\u8fde\u7eed\u7ed3\u6784\u4e2d\uff0c\u5b9e\u73b0\u4e86\u591a\u529f\u80fd\u611f\u77e5\u548c\u751f\u7269\u542f\u53d1\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u53d7\u5927\u8c61\u9f3b\u5b50\u7ed3\u6784\u548c\u611f\u77e5\u4e00\u4f53\u5316\u7684\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5c06\u4f20\u611f\u4f5c\u4e3a\u5185\u5728\u5236\u9020\u80fd\u529b\u7684\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u907f\u514d\u4f20\u7edf\u591a\u6750\u6599\u6216\u808c\u8171\u9a71\u52a8\u65b9\u6cd5\u4e2d\u7684\u673a\u68b0\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5355\u4e00\u8f6f\u6811\u8102\u6750\u6599\uff0c\u901a\u8fc7\u8fde\u7eed3D\u6253\u5370\u5c066\u4e2a\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u5668\u548c5\u4e2a\u6c14\u52a8\u8154\u96c6\u6210\u5230\u6c14\u52a8\u9a71\u52a8\u7684\u6676\u683c\u7ed3\u6784\u4e2d\uff0812.5mm\u5355\u5143\u5c3a\u5bf8\uff09\uff0c\u5b9e\u73b0\u4f20\u611f\u3001\u9a71\u52a8\u548c\u7ed3\u6784\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u4ec5\u97004\u6b21\u8fed\u4ee3\u5c31\u5b8c\u6210\u6700\u7ec8\u539f\u578b\uff0c\u80fd\u591f\u4e3e\u91cd\u8d85\u8fc7\u81ea\u8eab\u91cd\u91cf\u4e24\u500d\uff0c\u6267\u884c\u634f\u53d6\u3001\u8200\u53d6\u548c\u4f38\u5c55\u7b49\u751f\u7269\u542f\u53d1\u52a8\u4f5c\uff0c\u5e76\u80fd\u8f7b\u67d4\u6293\u63e1\u8461\u8404\u7b49\u6613\u788e\u7269\u54c1\u3002\u96c6\u6210\u5149\u5b66\u4f20\u611f\u5668\u53ef\u533a\u5206\u89e6\u6478\u3001\u5f2f\u66f2\u548c\u8154\u4f53\u53d8\u5f62\u7684\u4e0d\u540c\u54cd\u5e94\u3002", "conclusion": "MELEGROS\u5c55\u793a\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u65b0\u8303\u5f0f\uff0c\u5b8c\u5168\u5d4c\u5165\u7684\u4f20\u611f\u548c\u8fde\u7eed\u7ed3\u6784\u56fa\u6709\u5730\u652f\u6301\u591a\u529f\u80fd\u3001\u751f\u7269\u542f\u53d1\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2509.20516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20516", "abs": "https://arxiv.org/abs/2509.20516", "authors": ["Prasanna Sriganesh", "Barath Satheeshkumar", "Anushree Sabnis", "Matthew Travers"], "title": "Action-Informed Estimation and Planning: Clearing Clutter on Staircases via Quadrupedal Pedipulation", "comment": null, "summary": "For robots to operate autonomously in densely cluttered environments, they\nmust reason about and potentially physically interact with obstacles to clear a\npath. Safely clearing a path on challenging terrain, such as a cluttered\nstaircase, requires controlled interaction. For example, a quadrupedal robot\nthat pushes objects out of the way with one leg while maintaining a stable\nstance with its three other legs. However, tightly coupled physical actions,\nsuch as one-legged pushing, create new constraints on the system that can be\ndifficult to predict at design time. In this work, we present a new method that\naddresses one such constraint, wherein the object being pushed by a quadrupedal\nrobot with one of its legs becomes occluded from the robot's sensors during\nmanipulation. To address this challenge, we present a tightly coupled\nperception-action framework that enables the robot to perceive clutter, reason\nabout feasible push paths, and execute the clearing maneuver. Our core\ncontribution is an interaction-aware state estimation loop that uses\nproprioceptive feedback regarding foot contact and leg position to predict an\nobject's displacement during the occlusion. This prediction guides the\nperception system to robustly re-detect the object after the interaction,\nclosing the loop between action and sensing to enable accurate tracking even\nafter partial pushes. Using this feedback allows the robot to learn from\nphysical outcomes, reclassifying an object as immovable if a push fails due to\nit being too heavy. We present results of implementing our approach on a Boston\nDynamics Spot robot that show our interaction-aware approach achieves higher\ntask success rates and tracking accuracy in pushing objects on stairs compared\nto open-loop baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5-\u52a8\u4f5c\u6846\u67b6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u80fd\u591f\u611f\u77e5\u969c\u788d\u7269\u3001\u63a8\u7406\u53ef\u884c\u7684\u63a8\u6324\u8def\u5f84\u5e76\u6267\u884c\u6e05\u7406\u64cd\u4f5c\uff0c\u7279\u522b\u89e3\u51b3\u4e86\u5728\u5355\u817f\u63a8\u6324\u8fc7\u7a0b\u4e2d\u7269\u4f53\u88ab\u906e\u6321\u65f6\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u81ea\u4e3b\u64cd\u4f5c\u65f6\uff0c\u9700\u8981\u4e0e\u969c\u788d\u7269\u8fdb\u884c\u7269\u7406\u4ea4\u4e92\u6765\u6e05\u7406\u8def\u5f84\u3002\u4f46\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\uff08\u5982\u6742\u4e71\u7684\u697c\u68af\uff09\u4e0a\u8fdb\u884c\u5355\u817f\u63a8\u6324\u65f6\uff0c\u88ab\u63a8\u6324\u7684\u7269\u4f53\u53ef\u80fd\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u88ab\u673a\u5668\u4eba\u81ea\u8eab\u906e\u6321\uff0c\u8fd9\u7ed9\u72b6\u6001\u4f30\u8ba1\u5e26\u6765\u4e86\u65b0\u7684\u7ea6\u675f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7d27\u5bc6\u8026\u5408\u7684\u611f\u77e5-\u52a8\u4f5c\u6846\u67b6\uff0c\u6838\u5fc3\u8d21\u732e\u662f\u4ea4\u4e92\u611f\u77e5\u7684\u72b6\u6001\u4f30\u8ba1\u5faa\u73af\uff0c\u5229\u7528\u672c\u4f53\u611f\u53d7\u53cd\u9988\uff08\u8db3\u90e8\u63a5\u89e6\u548c\u817f\u90e8\u4f4d\u7f6e\uff09\u6765\u9884\u6d4b\u7269\u4f53\u5728\u906e\u6321\u671f\u95f4\u7684\u4f4d\u79fb\uff0c\u6307\u5bfc\u611f\u77e5\u7cfb\u7edf\u5728\u4ea4\u4e92\u540e\u91cd\u65b0\u68c0\u6d4b\u7269\u4f53\u3002", "result": "\u5728\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f00\u73af\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u4ea4\u4e92\u611f\u77e5\u65b9\u6cd5\u5728\u697c\u68af\u4e0a\u63a8\u6324\u7269\u4f53\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5c06\u52a8\u4f5c\u4e0e\u611f\u77e5\u95ed\u73af\u8fde\u63a5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u8ddf\u8e2a\u7269\u4f53\u5373\u4f7f\u5728\u90e8\u5206\u63a8\u6324\u540e\uff0c\u5e76\u5141\u8bb8\u673a\u5668\u4eba\u4ece\u7269\u7406\u7ed3\u679c\u4e2d\u5b66\u4e60\uff0c\u4f8b\u5982\u5728\u63a8\u6324\u5931\u8d25\u65f6\u5c06\u7269\u4f53\u91cd\u65b0\u5206\u7c7b\u4e3a\u4e0d\u53ef\u79fb\u52a8\u3002"}}
{"id": "2509.20541", "categories": ["cs.RO", "I.2.9; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.20541", "abs": "https://arxiv.org/abs/2509.20541", "authors": ["Anujith Muraleedharan", "Anamika J H"], "title": "Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning", "comment": "Preprint. 8 pages, 3 figures, 1 table, 1 algorithm. CoRL 2025 style\n  (preprint). Code/data to be released", "summary": "Human feedback can greatly accelerate robot learning, but in real-world\nsettings, such feedback is costly and limited. Existing human-in-the-loop\nreinforcement learning (HiL-RL) methods often assume abundant feedback,\nlimiting their practicality for physical robot deployment. In this work, we\nintroduce SPARQ, a progress-aware query policy that requests feedback only when\nlearning stagnates or worsens, thereby reducing unnecessary oracle calls. We\nevaluate SPARQ on a simulated UR5 cube-picking task in PyBullet, comparing\nagainst three baselines: no feedback, random querying, and always querying. Our\nexperiments show that SPARQ achieves near-perfect task success, matching the\nperformance of always querying while consuming about half the feedback budget.\nIt also provides more stable and efficient learning than random querying, and\nsignificantly improves over training without feedback. These findings suggest\nthat selective, progress-based query strategies can make HiL-RL more efficient\nand scalable for robots operating under realistic human effort constraints.", "AI": {"tldr": "SPARQ\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u8fdb\u5ea6\u7684\u67e5\u8be2\u7b56\u7565\uff0c\u53ea\u5728\u5b66\u4e60\u505c\u6ede\u6216\u6076\u5316\u65f6\u8bf7\u6c42\u4eba\u7c7b\u53cd\u9988\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u53cd\u9988\u8c03\u7528\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u8282\u7701\u4e86\u7ea6\u4e00\u534a\u7684\u53cd\u9988\u9884\u7b97\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u4eba\u7c7b\u53cd\u9988\u6210\u672c\u9ad8\u6602\u4e14\u6709\u9650\uff0c\u73b0\u6709\u7684\u4eba\u673a\u534f\u540c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u53cd\u9988\u5145\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u7269\u7406\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faSPARQ\u7b56\u7565\uff0c\u901a\u8fc7\u76d1\u6d4b\u5b66\u4e60\u8fdb\u5ea6\uff0c\u4ec5\u5728\u6027\u80fd\u505c\u6ede\u6216\u4e0b\u964d\u65f6\u8bf7\u6c42\u4eba\u7c7b\u53cd\u9988\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u67e5\u8be2\u3002\u5728PyBullet\u6a21\u62df\u73af\u5883\u4e2d\u5bf9UR5\u673a\u68b0\u81c2\u7acb\u65b9\u4f53\u6293\u53d6\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "SPARQ\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u6027\u80fd\u4e0e\u59cb\u7ec8\u67e5\u8be2\u7b56\u7565\u76f8\u5f53\uff0c\u4f46\u53ea\u6d88\u8017\u7ea6\u4e00\u534a\u7684\u53cd\u9988\u9884\u7b97\u3002\u76f8\u6bd4\u968f\u673a\u67e5\u8be2\u63d0\u4f9b\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u4e8e\u65e0\u53cd\u9988\u8bad\u7ec3\u3002", "conclusion": "\u57fa\u4e8e\u8fdb\u5ea6\u7684\u9009\u62e9\u6027\u67e5\u8be2\u7b56\u7565\u80fd\u591f\u4f7fHiL-RL\u5728\u73b0\u5b9e\u4eba\u7c7b\u52aa\u529b\u7ea6\u675f\u4e0b\u66f4\u52a0\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u3002"}}
{"id": "2509.20550", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20550", "abs": "https://arxiv.org/abs/2509.20550", "authors": ["Srinidhi Kalgundi Srinivas", "Yash Shukla", "Adam Arnold", "Sachin Chitta"], "title": "GraspFactory: A Large Object-Centric Grasping Dataset", "comment": null, "summary": "Robotic grasping is a crucial task in industrial automation, where robots are\nincreasingly expected to handle a wide range of objects. However, a significant\nchallenge arises when robot grasping models trained on limited datasets\nencounter novel objects. In real-world environments such as warehouses or\nmanufacturing plants, the diversity of objects can be vast, and grasping models\nneed to generalize to this diversity. Training large, generalizable\nrobot-grasping models requires geometrically diverse datasets. In this paper,\nwe introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps\ncollectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85\ngrippers (with 33,710 objects). GraspFactory is designed for training\ndata-intensive models, and we demonstrate the generalization capabilities of\none such model trained on a subset of GraspFactory in both simulated and\nreal-world settings. The dataset and tools are made available for download at\nhttps://graspfactory.github.io/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GraspFactory\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc71.09\u4ebf\u4e2a6-DoF\u6293\u53d6\u59ff\u6001\uff0c\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u6293\u53d6\u6a21\u578b\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\u9047\u5230\u65b0\u7269\u4f53\u65f6\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4ed3\u5e93\u548c\u5236\u9020\u73af\u5883\u4e2d\u7269\u4f53\u591a\u6837\u6027\u5927\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b1.09\u4ebf\u4e2a6-DoF\u6293\u53d6\u59ff\u6001\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6GraspFactory\uff0c\u6db5\u76d6Franka Panda\u548cRobotiq 2F-85\u5939\u5177\u768414,690\u548c33,710\u4e2a\u7269\u4f53\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u57fa\u4e8eGraspFactory\u5b50\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6570\u636e\u96c6\u548c\u5de5\u5177\u5df2\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "GraspFactory\u6570\u636e\u96c6\u4e3a\u8bad\u7ec3\u6570\u636e\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u6293\u53d6\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2509.20593", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20593", "abs": "https://arxiv.org/abs/2509.20593", "authors": ["Song Ma", "Richard Bucknall", "Yuanchang Liu"], "title": "Uncertainty-Aware Active Source Tracking of Marine Pollution using Unmanned Surface Vehicles", "comment": "Accepted for presentation at Oceantech: Marine Robotics & Science\n  Workshop, IROS 2025", "summary": "This paper proposes an uncertainty-aware marine pollution source tracking\nframework for unmanned surface vehicles (USVs). By integrating high-fidelity\nmarine pollution dispersion simulation with informative path planning\ntechniques, we demonstrate effective identification of pollution sources in\nmarine environments. The proposed approach is implemented based on Robot\nOperating System (ROS), processing real-time sensor data to update\nprobabilistic source location estimates. The system progressively refines the\nestimation of source location while quantifying uncertainty levels in its\npredictions. Experiments conducted in simulated environments with varying\nsource locations, flow conditions, and starting positions demonstrate the\nframework's ability to localise pollution sources with high accuracy. Results\nshow that the proposed approach achieves reliable source localisation\nefficiently. This work contributes to the development of full autonomous\nenvironmental monitoring capabilities essential for rapid response to marine\npollution incidents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u6c34\u9762\u8247\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6d77\u6d0b\u6c61\u67d3\u6e90\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u4fdd\u771f\u6c61\u67d3\u6269\u6563\u6a21\u62df\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u6280\u672f\uff0c\u5b9e\u73b0\u6d77\u6d0b\u73af\u5883\u4e2d\u6c61\u67d3\u6e90\u7684\u6709\u6548\u8bc6\u522b\u3002", "motivation": "\u5f00\u53d1\u5168\u81ea\u4e3b\u73af\u5883\u76d1\u6d4b\u80fd\u529b\uff0c\u4e3a\u5feb\u901f\u54cd\u5e94\u6d77\u6d0b\u6c61\u67d3\u4e8b\u4ef6\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "method": "\u57fa\u4e8eROS\u7cfb\u7edf\u5b9e\u73b0\uff0c\u96c6\u6210\u9ad8\u4fdd\u771f\u6d77\u6d0b\u6c61\u67d3\u6269\u6563\u6a21\u62df\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u6280\u672f\uff0c\u5b9e\u65f6\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\u66f4\u65b0\u6982\u7387\u6e90\u4f4d\u7f6e\u4f30\u8ba1\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u6c61\u67d3\u6e90\uff0c\u5b9e\u73b0\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u6e90\u5b9a\u4f4d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6d77\u6d0b\u6c61\u67d3\u6e90\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5168\u81ea\u4e3b\u73af\u5883\u76d1\u6d4b\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.20623", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20623", "abs": "https://arxiv.org/abs/2509.20623", "authors": ["Satyajeet Das", "Darren Chiu", "Zhehui Huang", "Lars Lindemann", "Gaurav S. Sukhatme"], "title": "Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation", "comment": null, "summary": "Reinforcement learning has enabled significant progress in complex domains\nsuch as coordinating and navigating multiple quadrotors. However, even\nwell-trained policies remain vulnerable to collisions in obstacle-rich\nenvironments. Addressing these infrequent but critical safety failures through\nretraining or fine-tuning is costly and risks degrading previously learned\nskills. Inspired by activation steering in large language models and latent\nediting in computer vision, we introduce a framework for inference-time Latent\nActivation Editing (LAE) that refines the behavior of pre-trained policies\nwithout modifying their weights or architecture. The framework operates in two\nstages: (i) an online classifier monitors intermediate activations to detect\nstates associated with undesired behaviors, and (ii) an activation editing\nmodule that selectively modifies flagged activations to shift the policy\ntowards safer regimes. In this work, we focus on improving safety in\nmulti-quadrotor navigation. We hypothesize that amplifying a policy's internal\nperception of risk can induce safer behaviors. We instantiate this idea through\na latent collision world model trained to predict future pre-collision\nactivations, thereby prompting earlier and more cautious avoidance responses.\nExtensive simulations and real-world Crazyflie experiments demonstrate that LAE\nachieves statistically significant reduction in collisions (nearly 90% fewer\ncumulative collisions compared to the unedited baseline) and substantially\nincreases the fraction of collision-free trajectories, while preserving task\ncompletion. More broadly, our results establish LAE as a lightweight paradigm,\nfeasible on resource-constrained hardware, for post-deployment refinement of\nlearned robot policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u6f5c\u5728\u6fc0\u6d3b\u7f16\u8f91\u6846\u67b6\uff0c\u5728\u4e0d\u4fee\u6539\u9884\u8bad\u7ec3\u7b56\u7565\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u7f16\u8f91\u4e2d\u95f4\u6fc0\u6d3b\u6765\u63d0\u5347\u591a\u56db\u65cb\u7ffc\u5bfc\u822a\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e2d\u4ecd\u5bb9\u6613\u53d1\u751f\u78b0\u649e\uff0c\u4f46\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u7834\u574f\u5df2\u5b66\u6280\u80fd\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5728\u7ebf\u5206\u7c7b\u5668\u68c0\u6d4b\u4e0e\u4e0d\u826f\u884c\u4e3a\u76f8\u5173\u7684\u6fc0\u6d3b\u72b6\u6001\uff0c\u6fc0\u6d3b\u7f16\u8f91\u6a21\u5757\u9009\u62e9\u6027\u4fee\u6539\u8fd9\u4e9b\u6fc0\u6d3b\u4ee5\u8f6c\u5411\u66f4\u5b89\u5168\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u663e\u793a\u78b0\u649e\u663e\u8457\u51cf\u5c11\uff08\u6bd4\u57fa\u7ebf\u51cf\u5c11\u8fd190%\uff09\uff0c\u78b0\u649e\u81ea\u7531\u8f68\u8ff9\u6bd4\u4f8b\u5927\u5e45\u589e\u52a0\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "conclusion": "LAE\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8303\u5f0f\uff0c\u53ef\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u90e8\u7f72\u540e\u4f18\u5316\u3002"}}
{"id": "2509.20635", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20635", "abs": "https://arxiv.org/abs/2509.20635", "authors": ["Matheus P. Angarola", "Francisco Affonso", "Marcelo Becker"], "title": "Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments", "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Legged robots must exhibit robust and agile locomotion across diverse,\nunstructured terrains, a challenge exacerbated under blind locomotion settings\nwhere terrain information is unavailable. This work introduces a hierarchical\nreinforcement learning framework that leverages terrain-specialized policies\nand curriculum learning to enhance agility and tracking performance in complex\nenvironments. We validated our method on simulation, where our approach\noutperforms a generalist policy by up to 16% in success rate and achieves lower\ntracking errors as the velocity target increases, particularly on low-friction\nand discontinuous terrains, demonstrating superior adaptability and robustness\nacross mixed-terrain scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5730\u5f62\u4e13\u7528\u7b56\u7565\u548c\u8bfe\u7a0b\u5b66\u4e60\u6765\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u654f\u6377\u6027\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u9700\u8981\u5728\u591a\u6837\u5316\u7684\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e0a\u5c55\u73b0\u9c81\u68d2\u548c\u654f\u6377\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u8fd9\u5728\u76f2\u8fd0\u52a8\u8bbe\u7f6e\u4e0b\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5730\u5f62\u4fe1\u606f\u4e0d\u53ef\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5730\u5f62\u4e13\u7528\u7b56\u7565\u548c\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u6765\u589e\u5f3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6bd4\u901a\u7528\u7b56\u7565\u6210\u529f\u7387\u63d0\u9ad816%\uff0c\u5728\u901f\u5ea6\u76ee\u6807\u589e\u52a0\u65f6\u8ddf\u8e2a\u8bef\u5dee\u66f4\u4f4e\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6469\u64e6\u548c\u4e0d\u8fde\u7eed\u5730\u5f62\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6df7\u5408\u5730\u5f62\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u7684\u76f2\u8fd0\u52a8\u6027\u80fd\u3002"}}
{"id": "2509.20646", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20646", "abs": "https://arxiv.org/abs/2509.20646", "authors": ["Sun Zhaole", "Xiaofeng Mao", "Jihong Zhu", "Yuanlong Zhang", "Robert B. Fisher"], "title": "Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation", "comment": "An IEEE conference paper currently under review", "summary": "Dexterous in-hand manipulation remains a foundational challenge in robotics,\nwith progress often constrained by the prevailing paradigm of imitating the\nhuman hand. This anthropomorphic approach creates two critical barriers: 1) it\nlimits robotic capabilities to tasks humans can already perform, and 2) it\nmakes data collection for learning-based methods exceedingly difficult. Both\nchallenges are caused by traditional force-closure which requires coordinating\ncomplex, multi-point contacts based on friction, normal force, and gravity to\ngrasp an object. This makes teleoperated demonstrations unstable and amplifies\nthe sim-to-real gap for reinforcement learning. In this work, we propose a\nparadigm shift: moving away from replicating human mechanics toward the design\nof novel robotic embodiments. We introduce the \\textbf{S}uction\n\\textbf{Leap}-Hand (SLeap Hand), a multi-fingered hand featuring integrated\nfingertip suction cups that realize a new form of suction-enabled dexterity. By\nreplacing complex force-closure grasps with stable, single-point adhesion, our\ndesign fundamentally simplifies in-hand teleoperation and facilitates the\ncollection of high-quality demonstration data. More importantly, this\nsuction-based embodiment unlocks a new class of dexterous skills that are\ndifficult or even impossible for the human hand, such as one-handed paper\ncutting and in-hand writing. Our work demonstrates that by moving beyond\nanthropomorphic constraints, novel embodiments can not only lower the barrier\nfor collecting robust manipulation data but also enable the stable,\nsingle-handed completion of tasks that would typically require two human hands.\nOur webpage is https://sites.google.com/view/sleaphand.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u624b\u8bbe\u8ba1SLeap Hand\uff0c\u901a\u8fc7\u6307\u5c16\u5438\u76d8\u53d6\u4ee3\u4f20\u7edf\u7684\u529b\u95ed\u5408\u6293\u53d6\uff0c\u7b80\u5316\u4e86\u624b\u5185\u9065\u64cd\u4f5c\uff0c\u5e76\u5b9e\u73b0\u4e86\u4eba\u7c7b\u624b\u96be\u4ee5\u5b8c\u6210\u7684\u7075\u5de7\u6280\u80fd\u3002", "motivation": "\u4f20\u7edf\u4eff\u4eba\u624b\u673a\u5668\u4eba\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u969c\u788d\uff1a\u9650\u5236\u4e86\u673a\u5668\u4eba\u53ea\u80fd\u5b8c\u6210\u4eba\u7c7b\u80fd\u505a\u7684\u4efb\u52a1\uff0c\u4ee5\u53ca\u4f7f\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u6570\u636e\u6536\u96c6\u6781\u5176\u56f0\u96be\u3002\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\u4f20\u7edf\u7684\u529b\u95ed\u5408\u6293\u53d6\u9700\u8981\u534f\u8c03\u590d\u6742\u7684\u591a\u70b9\u63a5\u89e6\u3002", "method": "\u8bbe\u8ba1SLeap Hand\u591a\u6307\u624b\uff0c\u5728\u6307\u5c16\u96c6\u6210\u5438\u76d8\uff0c\u5b9e\u73b0\u57fa\u4e8e\u5438\u529b\u7684\u7075\u5de7\u64cd\u4f5c\u3002\u7528\u7a33\u5b9a\u7684\u5355\u70b9\u5438\u9644\u53d6\u4ee3\u590d\u6742\u7684\u529b\u95ed\u5408\u6293\u53d6\u3002", "result": "\u8be5\u8bbe\u8ba1\u7b80\u5316\u4e86\u624b\u5185\u9065\u64cd\u4f5c\uff0c\u4fbf\u4e8e\u6536\u96c6\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\uff0c\u5e76\u89e3\u9501\u4e86\u4eba\u7c7b\u624b\u96be\u4ee5\u5b8c\u6210\u7684\u65b0\u6280\u80fd\u7c7b\u522b\uff0c\u5982\u5355\u624b\u526a\u7eb8\u548c\u624b\u5185\u4e66\u5199\u3002", "conclusion": "\u901a\u8fc7\u8d85\u8d8a\u4eff\u4eba\u7ea6\u675f\uff0c\u65b0\u578b\u673a\u5668\u4eba\u672c\u4f53\u4e0d\u4ec5\u80fd\u964d\u4f4e\u6536\u96c6\u7a33\u5065\u64cd\u4f5c\u6570\u636e\u7684\u95e8\u69db\uff0c\u8fd8\u80fd\u7a33\u5b9a\u5730\u5355\u624b\u5b8c\u6210\u901a\u5e38\u9700\u8981\u53cc\u624b\u7684\u4eba\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2509.20653", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20653", "abs": "https://arxiv.org/abs/2509.20653", "authors": ["Congkai Shen", "Siyuan Yu", "Yifan Weng", "Haoran Ma", "Chen Li", "Hiroshi Yasuda", "James Dallas", "Michael Thompson", "John Subosits", "Tulga Ersal"], "title": "Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills", "comment": null, "summary": "This study introduces a haptic shared control framework designed to teach\nhuman drivers advanced driving skills. In this context, shared control refers\nto a driving mode where the human driver collaborates with an autonomous\ndriving system to control the steering of a vehicle simultaneously. Advanced\ndriving skills are those necessary to safely push the vehicle to its handling\nlimits in high-performance driving such as racing and emergency obstacle\navoidance. Previous research has demonstrated the performance and safety\nbenefits of shared control schemes using both subjective and objective\nevaluations. However, these schemes have not been assessed for their impact on\nskill acquisition on complex and demanding tasks. Prior research on long-term\nskill acquisition either applies haptic shared control to simple tasks or\nemploys other feedback methods like visual and auditory aids. To bridge this\ngap, this study creates a cyber racing coach framework based on the haptic\nshared control paradigm and evaluates its performance in helping human drivers\nacquire high-performance driving skills. The framework introduces (1) an\nautonomous driving system that is capable of cooperating with humans in a\nhighly performant driving scenario; and (2) a haptic shared control mechanism\nalong with a fading scheme to gradually reduce the steering assistance from\nautonomy based on the human driver's performance during training. Two\nbenchmarks are considered: self-learning (no assistance) and full assistance\nduring training. Results from a human subject study indicate that the proposed\nframework helps human drivers develop superior racing skills compared to the\nbenchmarks, resulting in better performance and consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u7684\u8d5b\u535a\u8d5b\u8f66\u6559\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u5e2e\u52a9\u4eba\u7c7b\u9a7e\u9a76\u5458\u5b66\u4e60\u9ad8\u6027\u80fd\u9a7e\u9a76\u6280\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u6bd4\u65e0\u8f85\u52a9\u548c\u5168\u8f85\u52a9\u8bad\u7ec3\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u5171\u4eab\u63a7\u5236\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u4efb\u52a1\u6216\u4f7f\u7528\u89c6\u89c9\u542c\u89c9\u53cd\u9988\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u9ad8\u6027\u80fd\u9a7e\u9a76\u6280\u80fd\u83b7\u53d6\u6548\u679c\u7684\u8bc4\u4f30\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u521b\u5efa\u4e86\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u6846\u67b6\uff0c\u5305\u62ec\uff1a(1)\u80fd\u5728\u9ad8\u6027\u80fd\u9a7e\u9a76\u573a\u666f\u4e2d\u4e0e\u4eba\u534f\u4f5c\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff1b(2)\u57fa\u4e8e\u9a7e\u9a76\u5458\u8868\u73b0\u9010\u6b65\u51cf\u5c11\u8f6c\u5411\u8f85\u52a9\u7684\u6d88\u9000\u673a\u5236\u3002", "result": "\u4eba\u7c7b\u53d7\u8bd5\u8005\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u65e0\u8f85\u52a9\u548c\u5168\u8f85\u52a9\u8bad\u7ec3\u57fa\u51c6\uff0c\u80fd\u5e2e\u52a9\u9a7e\u9a76\u5458\u53d1\u5c55\u51fa\u66f4\u4f18\u8d8a\u7684\u8d5b\u8f66\u6280\u80fd\uff0c\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u6846\u67b6\u80fd\u6709\u6548\u4fc3\u8fdb\u4eba\u7c7b\u9a7e\u9a76\u5458\u5728\u9ad8\u6027\u80fd\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6280\u80fd\u83b7\u53d6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u9a7e\u9a76\u6280\u80fd\u8bad\u7ec3\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.20656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20656", "abs": "https://arxiv.org/abs/2509.20656", "authors": ["Junzhe Wang", "Jiarui Xie", "Pengfei Hao", "Zheng Li", "Yi Cai"], "title": "EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation", "comment": "8 pages, 14 figures, submitted to ICRA 2026", "summary": "Reliable brain-computer interface (BCI) control of robots provides an\nintuitive and accessible means of human-robot interaction, particularly\nvaluable for individuals with motor impairments. However, existing BCI-Robot\nsystems face major limitations: electroencephalography (EEG) signals are noisy\nand unstable, target selection is often predefined and inflexible, and most\nstudies remain restricted to simulation without closed-loop validation. These\nissues hinder real-world deployment in assistive scenarios. To address them, we\npropose a closed-loop BCI-AR-Robot system that integrates motor imagery\n(MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic\ngrasping for zero-touch operation. A 14-channel EEG headset enabled\nindividualized MI calibration, a smartphone-based AR interface supported\nmulti-target navigation with direction-congruent feedback to enhance stability,\nand the robotic arm combined decision outputs with vision-based pose estimation\nfor autonomous grasping. Experiments are conducted to validate the framework:\nMI training achieved 93.1 percent accuracy with an average information transfer\nrate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained\ncontrol (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with\nstatic, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2\npercent success rate with good efficiency and strong user-reported control.\nThese results show that AR feedback substantially stabilizes EEG-based control\nand that the proposed framework enables robust zero-touch grasping, advancing\nassistive robotic applications and future modes of human-robot interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73afBCI-AR-\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u57fa\u4e8e\u8fd0\u52a8\u60f3\u8c61\u7684EEG\u89e3\u7801\u3001\u589e\u5f3a\u73b0\u5b9e\u795e\u7ecf\u53cd\u9988\u548c\u673a\u5668\u4eba\u6293\u53d6\uff0c\u5b9e\u73b0\u4e86\u96f6\u63a5\u89e6\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86EEG\u63a7\u5236\u7684\u7a33\u5b9a\u6027\u548c\u673a\u5668\u4eba\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709BCI-\u673a\u5668\u4eba\u7cfb\u7edf\u5b58\u5728EEG\u4fe1\u53f7\u566a\u58f0\u5927\u3001\u76ee\u6807\u9009\u62e9\u9884\u5b9a\u4e49\u4e0d\u7075\u6d3b\u3001\u7f3a\u4e4f\u95ed\u73af\u9a8c\u8bc1\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5728\u8f85\u52a9\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u4f7f\u752814\u901a\u9053EEG\u8033\u673a\u8fdb\u884c\u4e2a\u6027\u5316MI\u6821\u51c6\uff0c\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684AR\u754c\u9762\u652f\u6301\u591a\u76ee\u6807\u5bfc\u822a\u548c\u65b9\u5411\u4e00\u81f4\u53cd\u9988\uff0c\u673a\u5668\u4eba\u624b\u81c2\u7ed3\u5408\u51b3\u7b56\u8f93\u51fa\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u4f4d\u59ff\u4f30\u8ba1\u5b9e\u73b0\u81ea\u4e3b\u6293\u53d6\u3002", "result": "MI\u8bad\u7ec3\u51c6\u786e\u7387\u8fbe93.1%\uff0c\u5e73\u5747ITR\u4e3a14.8 bit/min\uff1bAR\u795e\u7ecf\u53cd\u9988\u663e\u8457\u6539\u5584\u6301\u7eed\u63a7\u5236\uff08SCI=0.210\uff09\uff0c\u6700\u9ad8ITR\u8fbe21.3 bit/min\uff1b\u95ed\u73af\u6293\u53d6\u6210\u529f\u738797.2%\uff0c\u7528\u6237\u62a5\u544a\u63a7\u5236\u611f\u5f3a\u3002", "conclusion": "AR\u53cd\u9988\u663e\u8457\u7a33\u5b9a\u4e86\u57fa\u4e8eEEG\u7684\u63a7\u5236\uff0c\u6240\u63d0\u6846\u67b6\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u96f6\u63a5\u89e6\u6293\u53d6\uff0c\u63a8\u8fdb\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u5e94\u7528\u548c\u672a\u6765\u4eba\u673a\u4ea4\u4e92\u6a21\u5f0f\u3002"}}
{"id": "2509.20674", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20674", "abs": "https://arxiv.org/abs/2509.20674", "authors": ["Zeyu Han", "Shuocheng Yang", "Minghan Zhu", "Fang Zhang", "Shaobing Xu", "Maani Ghaffari", "Jianqiang Wang"], "title": "Equi-RO: A 4D mmWave Radar Odometry via Equivariant Networks", "comment": null, "summary": "Autonomous vehicles and robots rely on accurate odometry estimation in\nGPS-denied environments. While LiDARs and cameras struggle under extreme\nweather, 4D mmWave radar emerges as a robust alternative with all-weather\noperability and velocity measurement. In this paper, we introduce Equi-RO, an\nequivariant network-based framework for 4D radar odometry. Our algorithm\npre-processes Doppler velocity into invariant node and edge features in the\ngraph, and employs separate networks for equivariant and invariant feature\nprocessing. A graph-based architecture enhances feature aggregation in sparse\nradar data, improving inter-frame correspondence. Experiments on the\nopen-source dataset and self-collected dataset show Equi-RO outperforms\nstate-of-the-art algorithms in accuracy and robustness. Overall, our method\nachieves 10.7% and 20.0% relative improvements in translation and rotation\naccuracy, respectively, compared to the best baseline on the open-source\ndataset.", "AI": {"tldr": "Equi-RO\u662f\u4e00\u4e2a\u57fa\u4e8e\u7b49\u53d8\u7f51\u7edc\u76844D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u666e\u52d2\u901f\u5ea6\u9884\u5904\u7406\u4e3a\u56fe\u4e2d\u7684\u4e0d\u53d8\u8282\u70b9\u548c\u8fb9\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u5206\u79bb\u7f51\u7edc\u5904\u7406\u7b49\u53d8\u548c\u4e0d\u53d8\u7279\u5f81\uff0c\u5728\u7a00\u758f\u96f7\u8fbe\u6570\u636e\u4e2d\u63d0\u9ad8\u4e86\u5e27\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u5728\u5e73\u79fb\u548c\u65cb\u8f6c\u7cbe\u5ea6\u4e0a\u5206\u522b\u63d0\u5347\u4e8610.7%\u548c20.0%\u3002", "motivation": "\u5728GPS\u4fe1\u53f7\u7f3a\u5931\u7684\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u7684\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u3002\u867d\u7136\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u5728\u6781\u7aef\u5929\u6c14\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f464D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5177\u6709\u5168\u5929\u5019\u64cd\u4f5c\u80fd\u529b\u548c\u901f\u5ea6\u6d4b\u91cf\u80fd\u529b\uff0c\u6210\u4e3a\u4e00\u79cd\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faEqui-RO\u6846\u67b6\uff0c\u5c06\u591a\u666e\u52d2\u901f\u5ea6\u9884\u5904\u7406\u4e3a\u56fe\u4e2d\u7684\u4e0d\u53d8\u8282\u70b9\u548c\u8fb9\u7279\u5f81\uff0c\u91c7\u7528\u5206\u79bb\u7f51\u7edc\u5206\u522b\u5904\u7406\u7b49\u53d8\u548c\u4e0d\u53d8\u7279\u5f81\uff0c\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u67b6\u6784\u589e\u5f3a\u7a00\u758f\u96f7\u8fbe\u6570\u636e\u4e2d\u7684\u7279\u5f81\u805a\u5408\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEqui-RO\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u79fb\u548c\u65cb\u8f6c\u7cbe\u5ea6\u5206\u522b\u76f8\u5bf9\u63d0\u5347\u4e8610.7%\u548c20.0%\u3002", "conclusion": "Equi-RO\u6846\u67b6\u901a\u8fc7\u7b49\u53d8\u7f51\u7edc\u548c\u57fa\u4e8e\u56fe\u7684\u7279\u5f81\u5904\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e864D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3aGPS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20681", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20681", "abs": "https://arxiv.org/abs/2509.20681", "authors": ["Wei-Teng Chu", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation", "comment": null, "summary": "Implicit representations have been widely applied in robotics for obstacle\navoidance and path planning. In this paper, we explore the problem of\nconstructing an implicit distance representation from a single image. Past\nmethods for implicit surface reconstruction, such as \\emph{NeuS} and its\nvariants generally require a large set of multi-view images as input, and\nrequire long training times. In this work, we propose Fast Image-to-Neural\nSurface (FINS), a lightweight framework that can reconstruct high-fidelity\nsurfaces and SDF fields based on a single or a small set of images. FINS\nintegrates a multi-resolution hash grid encoder with lightweight geometry and\ncolor heads, making the training via an approximate second-order optimizer\nhighly efficient and capable of converging within a few seconds. Additionally,\nwe achieve the construction of a neural surface requiring only a single RGB\nimage, by leveraging pre-trained foundation models to estimate the geometry\ninherent in the image. Our experiments demonstrate that under the same\nconditions, our method outperforms state-of-the-art baselines in both\nconvergence speed and accuracy on surface reconstruction and SDF field\nestimation. Moreover, we demonstrate the applicability of FINS for robot\nsurface following tasks and show its scalability to a variety of benchmark\ndatasets.", "AI": {"tldr": "FINS\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u6216\u5c11\u91cf\u56fe\u50cf\u5feb\u901f\u91cd\u5efa\u9ad8\u4fdd\u771f\u8868\u9762\u548cSDF\u573a\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ec5\u9700\u51e0\u79d2\uff0c\u5728\u6536\u655b\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9690\u5f0f\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u5982NeuS\u9700\u8981\u5927\u91cf\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u957f\u65f6\u95f4\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4ece\u5355\u5f20\u56fe\u50cf\u5feb\u901f\u6784\u5efa\u9690\u5f0f\u8ddd\u79bb\u8868\u793a\u7684\u95ee\u9898\u3002", "method": "FINS\u6574\u5408\u4e86\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u7f16\u7801\u5668\u4e0e\u8f7b\u91cf\u7ea7\u51e0\u4f55\u548c\u989c\u8272\u5934\uff0c\u4f7f\u7528\u8fd1\u4f3c\u4e8c\u9636\u4f18\u5316\u5668\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u4ece\u5355\u5f20RGB\u56fe\u50cf\u4f30\u8ba1\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\uff0cFINS\u5728\u8868\u9762\u91cd\u5efa\u548cSDF\u573a\u4f30\u8ba1\u7684\u6536\u655b\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u8868\u9762\u8ddf\u968f\u4efb\u52a1\u3002", "conclusion": "FINS\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5355\u56fe\u50cf\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.20688", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20688", "abs": "https://arxiv.org/abs/2509.20688", "authors": ["Shouren Mao", "Minghao Qin", "Wei Dong", "Huajian Liu", "Yongzhuo Gao"], "title": "RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks", "comment": "Joint first authors: Shouren Mao and Minghao Qin. Published in\n  IEEE/RSJ IROS 2024. This arXiv version adds a joint first-authorship note to\n  correct an omission in the IEEE Xplore version. No technical changes. Please\n  cite the IEEE version", "summary": "Neural architecture search (NAS) has shown great promise in automatically\ndesigning lightweight models. However, conventional approaches are insufficient\nin training the supernet and pay little attention to actual robot hardware\nresources. To meet such challenges, we propose RAM-NAS, a resource-aware\nmulti-objective NAS method that focuses on improving the supernet pretrain and\nresource-awareness on robot hardware devices. We introduce the concept of\nsubnets mutual distillation, which refers to mutually distilling all subnets\nsampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge\nDistillation (DKD) loss to enhance logits distillation performance. To expedite\nthe search process with consideration for hardware resources, we used data from\nthree types of robotic edge hardware to train Latency Surrogate predictors.\nThese predictors facilitated the estimation of hardware inference latency\nduring the search phase, enabling a unified multi-objective evolutionary search\nto balance model accuracy and latency trade-offs. Our discovered model family,\nRAM-NAS models, can achieve top-1 accuracy ranging from 76.7% to 81.4% on\nImageNet. In addition, the resource-aware multi-objective NAS we employ\nsignificantly reduces the model's inference latency on edge hardware for\nrobots. We conducted experiments on downstream tasks to verify the scalability\nof our methods. The inference time for detection and segmentation is reduced on\nall three hardware types compared to MobileNetv3-based methods. Our work fills\nthe gap in NAS for robot hardware resource-aware.", "AI": {"tldr": "RAM-NAS\u662f\u4e00\u79cd\u8d44\u6e90\u611f\u77e5\u7684\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6539\u8fdb\u8d85\u7f51\u7edc\u9884\u8bad\u7ec3\u548c\u673a\u5668\u4eba\u786c\u4ef6\u8bbe\u5907\u7684\u8d44\u6e90\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u5b50\u7f51\u4e92\u84b8\u998f\u548c\u5ef6\u8fdf\u4ee3\u7406\u9884\u6d4b\u5668\u6765\u5e73\u8861\u6a21\u578b\u7cbe\u5ea6\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\u5728\u8bad\u7ec3\u8d85\u7f51\u7edc\u65b9\u9762\u4e0d\u8db3\uff0c\u4e14\u5f88\u5c11\u5173\u6ce8\u5b9e\u9645\u673a\u5668\u4eba\u786c\u4ef6\u8d44\u6e90\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8003\u8651\u6a21\u578b\u7cbe\u5ea6\u548c\u786c\u4ef6\u63a8\u7406\u5ef6\u8fdf\u7684NAS\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5b50\u7f51\u4e92\u84b8\u998f\u6982\u5ff5\uff0c\u4f7f\u7528\u4e09\u660e\u6cbb\u89c4\u5219\u91c7\u6837\u5b50\u7f51\u8fdb\u884c\u76f8\u4e92\u84b8\u998f\uff1b\u91c7\u7528\u89e3\u8026\u77e5\u8bc6\u84b8\u998f\u635f\u5931\u589e\u5f3alogits\u84b8\u998f\u6027\u80fd\uff1b\u4f7f\u7528\u4e09\u79cd\u673a\u5668\u4eba\u8fb9\u7f18\u786c\u4ef6\u6570\u636e\u8bad\u7ec3\u5ef6\u8fdf\u4ee3\u7406\u9884\u6d4b\u5668\uff0c\u5728\u641c\u7d22\u9636\u6bb5\u4f30\u8ba1\u786c\u4ef6\u63a8\u7406\u5ef6\u8fdf\uff1b\u91c7\u7528\u7edf\u4e00\u7684\u591a\u76ee\u6807\u8fdb\u5316\u641c\u7d22\u5e73\u8861\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u6743\u8861\u3002", "result": "RAM-NAS\u6a21\u578b\u5728ImageNet\u4e0a\u8fbe\u523076.7%\u523081.4%\u7684top-1\u51c6\u786e\u7387\uff1b\u76f8\u6bd4\u57fa\u4e8eMobileNetv3\u7684\u65b9\u6cd5\uff0c\u5728\u6240\u6709\u4e09\u79cd\u786c\u4ef6\u7c7b\u578b\u4e0a\u68c0\u6d4b\u548c\u5206\u5272\u7684\u63a8\u7406\u65f6\u95f4\u5747\u6709\u6240\u51cf\u5c11\uff1b\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u5728\u673a\u5668\u4eba\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "RAM-NAS\u586b\u8865\u4e86NAS\u5728\u673a\u5668\u4eba\u786c\u4ef6\u8d44\u6e90\u611f\u77e5\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u8d44\u6e90\u611f\u77e5\u7684\u591a\u76ee\u6807NAS\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.20689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20689", "abs": "https://arxiv.org/abs/2509.20689", "authors": ["Chathura Semasinghe", "Siavash Rezazadeh"], "title": "Incorporating Human-Inspired Ankle Characteristics in a Forced-Oscillation-Based Reduced-Order Model for Walking", "comment": null, "summary": "This paper extends the forced-oscillation-based reduced-order model of\nwalking to a model with ankles and feet. A human-inspired paradigm was designed\nfor the ankle dynamics, which results in improved gait characteristics compared\nto the point-foot model. In addition, it was shown that while the proposed\nmodel can stabilize against large errors in initial conditions through\ncombination of foot placement and ankle strategies, the model is able to\nstabilize against small perturbations without relying on the foot placement\ncontrol and solely through the designed proprioceptive ankle scheme. This novel\nproperty, which is also observed in humans, can help in better understanding of\nanthropomorphic walking and its stabilization mechanisms.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u57fa\u4e8e\u5f3a\u8feb\u632f\u8361\u7684\u6b65\u884c\u7b80\u5316\u6a21\u578b\uff0c\u52a0\u5165\u4e86\u8e1d\u5173\u8282\u548c\u8db3\u90e8\u3002\u8bbe\u8ba1\u4e86\u4eba\u4f53\u542f\u53d1\u7684\u8e1d\u5173\u8282\u52a8\u529b\u5b66\u8303\u5f0f\uff0c\u76f8\u6bd4\u70b9\u8db3\u6a21\u578b\u6539\u5584\u4e86\u6b65\u6001\u7279\u6027\u3002\u6a21\u578b\u80fd\u901a\u8fc7\u8db3\u90e8\u653e\u7f6e\u548c\u8e1d\u5173\u8282\u7b56\u7565\u7ec4\u5408\u7a33\u5b9a\u5927\u521d\u59cb\u8bef\u5dee\uff0c\u4ec5\u901a\u8fc7\u672c\u4f53\u611f\u89c9\u8e1d\u5173\u8282\u65b9\u6848\u7a33\u5b9a\u5c0f\u6270\u52a8\u3002", "motivation": "\u6269\u5c55\u6b65\u884c\u7b80\u5316\u6a21\u578b\u4ee5\u5305\u542b\u8e1d\u5173\u8282\u548c\u8db3\u90e8\uff0c\u6539\u8fdb\u70b9\u8db3\u6a21\u578b\u7684\u6b65\u6001\u7279\u6027\uff0c\u66f4\u597d\u5730\u7406\u89e3\u4eba\u4f53\u884c\u8d70\u53ca\u5176\u7a33\u5b9a\u673a\u5236\u3002", "method": "\u91c7\u7528\u4eba\u4f53\u542f\u53d1\u7684\u8e1d\u5173\u8282\u52a8\u529b\u5b66\u8303\u5f0f\uff0c\u7ed3\u5408\u8db3\u90e8\u653e\u7f6e\u548c\u8e1d\u5173\u8282\u7b56\u7565\uff0c\u8bbe\u8ba1\u672c\u4f53\u611f\u89c9\u8e1d\u5173\u8282\u63a7\u5236\u65b9\u6848\u3002", "result": "\u4e0e\u70b9\u8db3\u6a21\u578b\u76f8\u6bd4\uff0c\u65b0\u6a21\u578b\u5177\u6709\u6539\u8fdb\u7684\u6b65\u6001\u7279\u6027\uff0c\u80fd\u7a33\u5b9a\u5927\u521d\u59cb\u8bef\u5dee\u548c\u5c0f\u6270\u52a8\uff0c\u4e14\u5c0f\u6270\u52a8\u4e0b\u65e0\u9700\u4f9d\u8d56\u8db3\u90e8\u653e\u7f6e\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u6a21\u62df\u4eba\u7c7b\u884c\u8d70\u7684\u7a33\u5b9a\u673a\u5236\uff0c\u7279\u522b\u662f\u4ec5\u901a\u8fc7\u8e1d\u5173\u8282\u7b56\u7565\u7a33\u5b9a\u5c0f\u6270\u52a8\u7684\u7279\u6027\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u62df\u4eba\u884c\u8d70\u53ca\u5176\u7a33\u5b9a\u673a\u5236\u3002"}}
{"id": "2509.20696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20696", "abs": "https://arxiv.org/abs/2509.20696", "authors": ["Qingpeng Li", "Chengrui Zhu", "Yanming Wu", "Xin Yuan", "Zhen Zhang", "Jian Yang", "Yong Liu"], "title": "RuN: Residual Policy for Natural Humanoid Locomotion", "comment": null, "summary": "Enabling humanoid robots to achieve natural and dynamic locomotion across a\nwide range of speeds, including smooth transitions from walking to running,\npresents a significant challenge. Existing deep reinforcement learning methods\ntypically require the policy to directly track a reference motion, forcing a\nsingle policy to simultaneously learn motion imitation, velocity tracking, and\nstability maintenance. To address this, we introduce RuN, a novel decoupled\nresidual learning framework. RuN decomposes the control task by pairing a\npre-trained Conditional Motion Generator, which provides a kinematically\nnatural motion prior, with a reinforcement learning policy that learns a\nlightweight residual correction to handle dynamical interactions. Experiments\nin simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN\nachieves stable, natural gaits and smooth walk-run transitions across a broad\nvelocity range (0-2.5 m/s), outperforming state-of-the-art methods in both\ntraining efficiency and final performance.", "AI": {"tldr": "RuN\u662f\u4e00\u4e2a\u89e3\u8026\u7684\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u7684\u6761\u4ef6\u8fd0\u52a8\u751f\u6210\u5668\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u914d\u5bf9\uff0c\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u4ece\u8d70\u5230\u8dd1\u7684\u81ea\u7136\u52a8\u6001\u8fd0\u52a8\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u5e7f\u6cdb\u901f\u5ea6\u8303\u56f4\u5185\u5b9e\u73b0\u81ea\u7136\u52a8\u6001\u8fd0\u52a8\uff08\u5305\u62ec\u4ece\u8d70\u5230\u8dd1\u7684\u5e73\u6ed1\u8fc7\u6e21\uff09\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5355\u4e00\u7b56\u7565\u540c\u65f6\u5b66\u4e60\u8fd0\u52a8\u6a21\u4eff\u3001\u901f\u5ea6\u8ddf\u8e2a\u548c\u7a33\u5b9a\u6027\u7ef4\u6301\u3002", "method": "\u5f15\u5165RuN\u6846\u67b6\uff0c\u5c06\u63a7\u5236\u4efb\u52a1\u5206\u89e3\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6761\u4ef6\u8fd0\u52a8\u751f\u6210\u5668\u63d0\u4f9b\u8fd0\u52a8\u5148\u9a8c\uff0c\u914d\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b66\u4e60\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u4fee\u6b63\u6765\u5904\u7406\u52a8\u529b\u5b66\u4ea4\u4e92\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0cRuN\u57280-2.5 m/s\u901f\u5ea6\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u7a33\u5b9a\u81ea\u7136\u7684\u6b65\u6001\u548c\u5e73\u6ed1\u7684\u8d70\u8dd1\u8f6c\u6362\uff0c\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "RuN\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u63a7\u5236\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u81ea\u7136\u52a8\u6001\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20703", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20703", "abs": "https://arxiv.org/abs/2509.20703", "authors": ["Xiaoxiang Dong", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations", "comment": null, "summary": "Learning from human video demonstrations offers a scalable alternative to\nteleoperation or kinesthetic teaching, but poses challenges for robot\nmanipulators due to embodiment differences and joint feasibility constraints.\nWe address this problem by proposing the Joint Flow Trajectory Optimization\n(JFTO) framework for grasp pose generation and object trajectory imitation\nunder the video-based Learning-from-Demonstration (LfD) paradigm. Rather than\ndirectly imitating human hand motions, our method treats demonstrations as\nobject-centric guides, balancing three objectives: (i) selecting a feasible\ngrasp pose, (ii) generating object trajectories consistent with demonstrated\nmotions, and (iii) ensuring collision-free execution within robot kinematics.\nTo capture the multimodal nature of demonstrations, we extend flow matching to\n$\\SE(3)$ for probabilistic modeling of object trajectories, enabling\ndensity-aware imitation that avoids mode collapse. The resulting optimization\nintegrates grasp similarity, trajectory likelihood, and collision penalties\ninto a unified differentiable objective. We validate our approach in both\nsimulation and real-world experiments across diverse real-world manipulation\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86JFTO\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4ece\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u6293\u53d6\u548c\u64cd\u4f5c\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4eba\u673a\u5dee\u5f02\u548c\u5173\u8282\u53ef\u884c\u6027\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u4ece\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u5b66\u4e60\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u4eba\u673a\u5dee\u5f02\u548c\u5173\u8282\u53ef\u884c\u6027\u7ea6\u675f\uff0c\u76f4\u63a5\u6a21\u4eff\u4eba\u624b\u8fd0\u52a8\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u8054\u5408\u6d41\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u6f14\u793a\u89c6\u4e3a\u5bf9\u8c61\u4e2d\u5fc3\u6307\u5bfc\uff0c\u5e73\u8861\u4e09\u4e2a\u76ee\u6807\uff1a\u9009\u62e9\u53ef\u884c\u6293\u53d6\u59ff\u52bf\u3001\u751f\u6210\u4e0e\u6f14\u793a\u4e00\u81f4\u7684\u5bf9\u8c61\u8f68\u8ff9\u3001\u786e\u4fdd\u65e0\u78b0\u649e\u6267\u884c\u3002\u6269\u5c55\u6d41\u5339\u914d\u5230SE(3)\u8fdb\u884c\u6982\u7387\u5efa\u6a21\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "JFTO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u548c\u6982\u7387\u5efa\u6a21\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6293\u53d6\u548c\u8f68\u8ff9\u751f\u6210\u3002"}}
{"id": "2509.20705", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20705", "abs": "https://arxiv.org/abs/2509.20705", "authors": ["Reza Akhavian", "Mani Amani", "Johannes Mootz", "Robert Ashe", "Behrad Beheshti"], "title": "Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework", "comment": null, "summary": "The adoption of cyber-physical systems and jobsite intelligence that connects\ndesign models, real-time site sensing, and autonomous field operations can\ndramatically enhance digital management in the construction industry. This\npaper introduces BIM2RDT (Building Information Models to Robot-Ready Site\nDigital Twins), an agentic artificial intelligence (AI) framework designed to\ntransform static Building Information Modeling (BIM) into dynamic, robot-ready\ndigital twins (DTs) that prioritize safety during execution. The framework\nbridges the gap between pre-existing BIM data and real-time site conditions by\nintegrating three key data streams: geometric and semantic information from BIM\nmodels, activity data from IoT sensor networks, and visual-spatial data\ncollected by robots during site traversal. The methodology introduces\nSemantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that\nleverages large language model (LLM) reasoning. Unlike traditional methods,\nSG-ICP utilizes an LLM to infer object-specific, plausible orientation priors\nbased on BIM semantics, improving alignment accuracy by avoiding convergence on\nlocal minima. This creates a feedback loop where robot-collected data updates\nthe DT, which in turn optimizes paths for missions. The framework employs YOLOE\nobject detection and Shi-Tomasi corner detection to identify and track\nconstruction elements while using BIM geometry as a priori maps. The framework\nalso integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping\nsensor-detected safety events to the digital twin using IFC standards for\nintervention. Experiments demonstrate SG-ICP's superiority over standard ICP,\nachieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with\noccluded features, ensuring plausible orientations. HAV integration triggers\nwarnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.", "AI": {"tldr": "BIM2RDT\u6846\u67b6\u5c06\u9759\u6001BIM\u8f6c\u6362\u4e3a\u52a8\u6001\u3001\u673a\u5668\u4eba\u5c31\u7eea\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u901a\u8fc7\u6574\u5408BIM\u6570\u636e\u3001IoT\u4f20\u611f\u5668\u548c\u673a\u5668\u4eba\u89c6\u89c9\u6570\u636e\uff0c\u91c7\u7528SG-ICP\u70b9\u4e91\u914d\u51c6\u7b97\u6cd5\u63d0\u5347\u5bf9\u9f50\u7cbe\u5ea6\uff0c\u5e76\u96c6\u6210\u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709BIM\u6570\u636e\u4e0e\u5b9e\u65f6\u73b0\u573a\u6761\u4ef6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u5efa\u7b51\u884c\u4e1a\u6570\u5b57\u7ba1\u7406\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u6574\u5408BIM\u51e0\u4f55\u8bed\u4e49\u4fe1\u606f\u3001IoT\u6d3b\u52a8\u6570\u636e\u548c\u673a\u5668\u4eba\u89c6\u89c9\u7a7a\u95f4\u6570\u636e\uff0c\u5f00\u53d1SG-ICP\u70b9\u4e91\u914d\u51c6\u7b97\u6cd5\uff08\u5229\u7528LLM\u63a8\u7406\u5bf9\u8c61\u65b9\u5411\u5148\u9a8c\uff09\uff0c\u7ed3\u5408YOLOE\u76ee\u6807\u68c0\u6d4b\u548cShi-Tomasi\u89d2\u70b9\u68c0\u6d4b\uff0c\u96c6\u6210\u5b9e\u65f6HAV\u5b89\u5168\u76d1\u63a7\u3002", "result": "SG-ICP\u76f8\u6bd4\u6807\u51c6ICP\u5728\u906e\u6321\u573a\u666f\u4e0b\u5bf9\u9f50RMSE\u964d\u4f4e64.3%-88.3%\uff0cHAV\u96c6\u6210\u80fd\u5728\u8d85\u8fc7\u66b4\u9732\u9650\u503c\u65f6\u89e6\u53d1\u8b66\u544a\u3002", "conclusion": "BIM2RDT\u6846\u67b6\u6210\u529f\u5c06\u9759\u6001BIM\u8f6c\u5316\u4e3a\u52a8\u6001\u6570\u5b57\u5b6a\u751f\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u7cbe\u5ea6\u548c\u73b0\u573a\u5b89\u5168\u76d1\u63a7\u80fd\u529b\uff0c\u4e3a\u5efa\u7b51\u884c\u4e1a\u6570\u5b57\u5316\u7ba1\u7406\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20709", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20709", "abs": "https://arxiv.org/abs/2509.20709", "authors": ["Mani Amani", "Reza Akhavian"], "title": "Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor", "comment": null, "summary": "Integrating natural language (NL) prompts into robotic mission planning has\nattracted significant interest in recent years. In the construction domain,\nBuilding Information Models (BIM) encapsulate rich NL descriptions of the\nenvironment. We present a novel framework that fuses NL directives with\nBIM-derived semantic maps via a Beta-Bernoulli Bayesian fusion by interpreting\nthe LLM as a sensor: each obstacle's design-time repulsive coefficient is\ntreated as a Beta(alpha, beta) random variable and LLM-returned danger scores\nare incorporated as pseudo-counts to update alpha and beta. The resulting\nposterior mean yields a continuous, context-aware repulsive gain that augments\na Euclidean-distance-based potential field for cost heuristics. By adjusting\ngains based on sentiment and context inferred from user prompts, our method\nguides robots along safer, more context-aware paths. This provides a\nnumerically stable method that can chain multiple natural commands and prompts\nfrom construction workers and foreman to enable planning while giving\nflexibility to be integrated in any learned or classical AI framework.\nSimulation results demonstrate that this Beta-Bernoulli fusion yields both\nqualitative and quantitative improvements in path robustness and validity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0eBIM\u8bed\u4e49\u5730\u56fe\u878d\u5408\u7684\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7Beta-Bernoulli\u8d1d\u53f6\u65af\u878d\u5408\u5c06LLM\u89c6\u4e3a\u4f20\u611f\u5668\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6392\u65a5\u589e\u76ca\u6765\u6539\u8fdb\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u5728\u5efa\u7b51\u9886\u57df\uff0cBIM\u6a21\u578b\u5305\u542b\u4e30\u5bcc\u7684\u73af\u5883\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u9700\u8981\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u6709\u6548\u878d\u5408\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8def\u5f84\u89c4\u5212\u3002", "method": "\u4f7f\u7528Beta-Bernoulli\u8d1d\u53f6\u65af\u878d\u5408\u65b9\u6cd5\uff0c\u5c06LLM\u8fd4\u56de\u7684\u5371\u9669\u5206\u6570\u4f5c\u4e3a\u4f2a\u8ba1\u6570\u6765\u66f4\u65b0Beta\u5206\u5e03\u7684\u53c2\u6570\uff0c\u751f\u6210\u8fde\u7eed\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6392\u65a5\u589e\u76ca\uff0c\u589e\u5f3a\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u7684\u52bf\u573a\u6210\u672c\u542f\u53d1\u5f0f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8def\u5f84\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u503c\u7a33\u5b9a\u7684\u65b9\u5f0f\uff0c\u80fd\u591f\u94fe\u63a5\u591a\u4e2a\u81ea\u7136\u547d\u4ee4\u548c\u63d0\u793a\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u89c4\u5212\u66f4\u5b89\u5168\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8def\u5f84\uff0c\u5e76\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u5b66\u4e60\u6216\u7ecf\u5178AI\u6846\u67b6\u4e2d\u3002"}}
{"id": "2509.20717", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20717", "abs": "https://arxiv.org/abs/2509.20717", "authors": ["Zhenguo Sun", "Yibo Peng", "Yuan Meng", "Xukun Li", "Bo-Sheng Huang", "Zhenshan Bing", "Xinlong Wang", "Alois Knoll"], "title": "RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking", "comment": null, "summary": "Long-horizon, high-dynamic motion tracking on humanoids remains brittle\nbecause absolute joint commands cannot compensate model-plant mismatch, leading\nto error accumulation. We propose RobotDancing, a simple, scalable framework\nthat predicts residual joint targets to explicitly correct dynamics\ndiscrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and\nzero-shot sim-to-real--and uses a single-stage reinforcement learning (RL)\nsetup with a unified observation, reward, and hyperparameter configuration. We\nevaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and\nvalidate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy\nbehaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with\nhigh motion tracking quality.", "AI": {"tldr": "RobotDancing\u662f\u4e00\u4e2a\u7b80\u5355\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6b8b\u5dee\u5173\u8282\u76ee\u6807\u6765\u7ea0\u6b63\u52a8\u529b\u5b66\u5dee\u5f02\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u957f\u65f6\u7a0b\u9ad8\u52a8\u6001\u8fd0\u52a8\u8ddf\u8e2a\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u7684\u957f\u65f6\u7a0b\u9ad8\u52a8\u6001\u8fd0\u52a8\u8ddf\u8e2a\u4ecd\u7136\u8106\u5f31\uff0c\u56e0\u4e3a\u7edd\u5bf9\u5173\u8282\u6307\u4ee4\u65e0\u6cd5\u8865\u507f\u6a21\u578b-\u5b9e\u9645\u5dee\u5f02\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u4f7f\u7528\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5177\u6709\u7edf\u4e00\u7684\u89c2\u6d4b\u3001\u5956\u52b1\u548c\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u5305\u62ec\u8bad\u7ec3\u3001\u6a21\u62df\u5230\u6a21\u62df\u9a8c\u8bc1\u548c\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u90e8\u7f72\u3002", "result": "\u5728Unitree G1\u4e0a\u8bc4\u4f30\uff0c\u80fd\u591f\u8ddf\u8e2a\u591a\u5206\u949f\u7684\u9ad8\u80fd\u91cf\u884c\u4e3a\uff08\u8df3\u8dc3\u3001\u65cb\u8f6c\u3001\u4fa7\u624b\u7ffb\uff09\uff0c\u5e76\u4ee5\u9ad8\u8fd0\u52a8\u8ddf\u8e2a\u8d28\u91cf\u96f6\u6837\u672c\u90e8\u7f72\u5230\u786c\u4ef6\u3002", "conclusion": "RobotDancing\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u9ad8\u52a8\u6001\u8fd0\u52a8\u8ddf\u8e2a\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u90e8\u7f72\u3002"}}
{"id": "2509.20739", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20739", "abs": "https://arxiv.org/abs/2509.20739", "authors": ["Guoyang Zhao", "Yudong Li", "Weiqing Qi", "Kai Zhang", "Bonan Liu", "Kai Chen", "Haoang Li", "Jun Ma"], "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning", "comment": null, "summary": "Conventional SLAM pipelines for legged robot navigation are fragile under\nrapid motion, calibration demands, and sensor drift, while offering limited\nsemantic reasoning for task-driven exploration. To deal with these issues, we\npropose a vision-only, SLAM-free navigation framework that replaces dense\ngeometry with semantic reasoning and lightweight topological representations. A\nhierarchical vision-language perception module fuses scene-level context with\nobject-level cues for robust semantic inference. And a semantic-probabilistic\ntopological map supports coarse-to-fine planning: LLM-based global reasoning\nfor subgoal selection and vision-based local planning for obstacle avoidance.\nIntegrated with reinforcement-learning locomotion controllers, the framework is\ndeployable across diverse legged robot platforms. Experiments in simulation and\nreal-world settings demonstrate consistent improvements in semantic accuracy,\nplanning quality, and navigation success, while ablation studies further\nshowcase the necessity of both hierarchical perception and fine local planning.\nThis work introduces a new paradigm for SLAM-free, vision-language-driven\nnavigation, shifting robotic exploration from geometry-centric mapping to\nsemantics-driven decision making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u611f\u77e5\u7684SLAM-free\u5bfc\u822a\u6846\u67b6\uff0c\u4f7f\u7528\u8bed\u4e49\u63a8\u7406\u548c\u8f7b\u91cf\u7ea7\u62d3\u6251\u8868\u793a\u66ff\u4ee3\u4f20\u7edfSLAM\u7684\u5bc6\u96c6\u51e0\u4f55\u91cd\u5efa\uff0c\u901a\u8fc7\u5206\u5c42\u611f\u77e5\u548c\u6982\u7387\u62d3\u6251\u5730\u56fe\u5b9e\u73b0\u4ece\u5168\u5c40\u63a8\u7406\u5230\u5c40\u90e8\u89c4\u5212\u7684\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\u5728\u8db3\u5f0f\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u5b58\u5728\u5bf9\u5feb\u901f\u8fd0\u52a8\u654f\u611f\u3001\u6807\u5b9a\u8981\u6c42\u9ad8\u3001\u4f20\u611f\u5668\u6f02\u79fb\u7b49\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u8bed\u4e49\u9a71\u52a8\u7684\u5bfc\u822a\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7eaf\u89c6\u89c9SLAM-free\u6846\u67b6\uff0c\u5305\u542b\u5206\u5c42\u89c6\u89c9\u8bed\u8a00\u611f\u77e5\u6a21\u5757\uff08\u878d\u5408\u573a\u666f\u7ea7\u4e0a\u4e0b\u6587\u548c\u7269\u4f53\u7ea7\u7ebf\u7d22\uff09\u548c\u8bed\u4e49\u6982\u7387\u62d3\u6251\u5730\u56fe\uff0c\u7ed3\u5408LLM\u5168\u5c40\u63a8\u7406\u9009\u62e9\u5b50\u76ee\u6807\uff0c\u89c6\u89c9\u5c40\u90e8\u89c4\u5212\u907f\u969c\uff0c\u4e0e\u5f3a\u5316\u5b66\u4e60\u8fd0\u52a8\u63a7\u5236\u5668\u96c6\u6210\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0c\u5728\u8bed\u4e49\u51c6\u786e\u6027\u3001\u89c4\u5212\u8d28\u91cf\u548c\u5bfc\u822a\u6210\u529f\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5206\u5c42\u611f\u77e5\u548c\u7cbe\u7ec6\u5c40\u90e8\u89c4\u5212\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86SLAM-free\u3001\u89c6\u89c9\u8bed\u8a00\u9a71\u52a8\u5bfc\u822a\u7684\u65b0\u8303\u5f0f\uff0c\u5c06\u673a\u5668\u4eba\u63a2\u7d22\u4ece\u51e0\u4f55\u4e2d\u5fc3\u6620\u5c04\u8f6c\u5411\u8bed\u4e49\u9a71\u52a8\u51b3\u7b56\u3002"}}
{"id": "2509.20757", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20757", "abs": "https://arxiv.org/abs/2509.20757", "authors": ["Yuxuan Zhou", "Xingxing Li", "Shengyu Li", "Zhuohao Yan", "Chunxi Xia", "Shaoquan Feng"], "title": "MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM", "comment": null, "summary": "Visual SLAM is a cornerstone technique in robotics, autonomous driving and\nextended reality (XR), yet classical systems often struggle with low-texture\nenvironments, scale ambiguity, and degraded performance under challenging\nvisual conditions. Recent advancements in feed-forward neural network-based\npointmap regression have demonstrated the potential to recover high-fidelity 3D\nscene geometry directly from images, leveraging learned spatial priors to\novercome limitations of traditional multi-view geometry methods. However, the\nwidely validated advantages of probabilistic multi-sensor information fusion\nare often discarded in these pipelines. In this work, we propose\nMASt3R-Fusion,a multi-sensor-assisted visual SLAM framework that tightly\nintegrates feed-forward pointmap regression with complementary sensor\ninformation, including inertial measurements and GNSS data. The system\nintroduces Sim(3)-based visualalignment constraints (in the Hessian form) into\na universal metric-scale SE(3) factor graph for effective information fusion. A\nhierarchical factor graph design is developed, which allows both real-time\nsliding-window optimization and global optimization with aggressive loop\nclosures, enabling real-time pose tracking, metric-scale structure perception\nand globally consistent mapping. We evaluate our approach on both public\nbenchmarks and self-collected datasets, demonstrating substantial improvements\nin accuracy and robustness over existing visual-centered multi-sensor SLAM\nsystems. The code will be released open-source to support reproducibility and\nfurther research (https://github.com/GREAT-WHU/MASt3R-Fusion).", "AI": {"tldr": "MASt3R-Fusion\u662f\u4e00\u4e2a\u591a\u4f20\u611f\u5668\u8f85\u52a9\u7684\u89c6\u89c9SLAM\u6846\u67b6\uff0c\u5c06\u524d\u9988\u70b9\u4e91\u56de\u5f52\u4e0e\u60ef\u6027\u6d4b\u91cf\u548cGNSS\u6570\u636e\u7d27\u5bc6\u96c6\u6210\uff0c\u901a\u8fc7Sim(3)\u89c6\u89c9\u5bf9\u9f50\u7ea6\u675f\u548c\u5206\u5c42\u56e0\u5b50\u56fe\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u5b9e\u65f6\u59ff\u6001\u8ddf\u8e2a\u548c\u5168\u5c40\u4e00\u81f4\u7684\u5730\u56fe\u6784\u5efa\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9SLAM\u7cfb\u7edf\u5728\u4f4e\u7eb9\u7406\u73af\u5883\u548c\u6311\u6218\u6027\u89c6\u89c9\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u70b9\u4e91\u56de\u5f52\u65b9\u6cd5\u867d\u7136\u80fd\u6062\u590d\u9ad8\u4fdd\u771f3D\u573a\u666f\u51e0\u4f55\uff0c\u4f46\u5f80\u5f80\u4e22\u5f03\u4e86\u6982\u7387\u591a\u4f20\u611f\u5668\u4fe1\u606f\u878d\u5408\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faSim(3)\u89c6\u89c9\u5bf9\u9f50\u7ea6\u675f\uff0c\u5c06\u5176\u96c6\u6210\u5230\u901a\u7528\u5ea6\u91cf\u5c3a\u5ea6SE(3)\u56e0\u5b50\u56fe\u4e2d\uff0c\u91c7\u7528\u5206\u5c42\u56e0\u5b50\u56fe\u8bbe\u8ba1\uff0c\u652f\u6301\u5b9e\u65f6\u6ed1\u52a8\u7a97\u53e3\u4f18\u5316\u548c\u5177\u6709\u6fc0\u8fdb\u95ed\u73af\u7684\u5168\u5c40\u4f18\u5316\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u7684\u89c6\u89c9\u4e2d\u5fc3\u591a\u4f20\u611f\u5668SLAM\u7cfb\u7edf\uff0c\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MASt3R-Fusion\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u524d\u9988\u70b9\u4e91\u56de\u5f52\u4e0e\u591a\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u59ff\u6001\u8ddf\u8e2a\u3001\u5ea6\u91cf\u5c3a\u5ea6\u7ed3\u6784\u611f\u77e5\u548c\u5168\u5c40\u4e00\u81f4\u6620\u5c04\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2509.20766", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20766", "abs": "https://arxiv.org/abs/2509.20766", "authors": ["Gawon Lee", "Daesol Cho", "H. Jin Kim"], "title": "Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning", "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Multi-task reinforcement learning (MTRL) offers a promising approach to\nimprove sample efficiency and generalization by training agents across multiple\ntasks, enabling knowledge sharing between them. However, applying MTRL to\nrobotics remains challenging due to the high cost of collecting diverse task\ndata. To address this, we propose MT-L\\'evy, a novel exploration strategy that\nenhances sample efficiency in MTRL environments by combining behavior sharing\nacross tasks with temporally extended exploration inspired by L\\'evy flight.\nMT-L\\'evy leverages policies trained on related tasks to guide exploration\ntowards key states, while dynamically adjusting exploration levels based on\ntask success ratios. This approach enables more efficient state-space coverage,\neven in complex robotics environments. Empirical results demonstrate that\nMT-L\\'evy significantly improves exploration and sample efficiency, supported\nby quantitative and qualitative analyses. Ablation studies further highlight\nthe contribution of each component, showing that combining behavior sharing\nwith adaptive exploration strategies can significantly improve the practicality\nof MTRL in robotics applications.", "AI": {"tldr": "MT-L\u00e9vy\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u8de8\u4efb\u52a1\u884c\u4e3a\u5171\u4eab\u548c\u53d7L\u00e9vy\u98de\u884c\u542f\u53d1\u7684\u65f6\u5e8f\u6269\u5c55\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9762\u4e34\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u7b56\u7565\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "method": "MT-L\u00e9vy\u7ed3\u5408\u4e86\u8de8\u4efb\u52a1\u884c\u4e3a\u5171\u4eab\uff08\u5229\u7528\u76f8\u5173\u4efb\u52a1\u8bad\u7ec3\u7684\u7b56\u7565\u6307\u5bfc\u63a2\u7d22\uff09\u548cL\u00e9vy\u98de\u884c\u542f\u53d1\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\uff0c\u6839\u636e\u4efb\u52a1\u6210\u529f\u7387\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u6c34\u5e73\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660eMT-L\u00e9vy\u663e\u8457\u6539\u5584\u4e86\u63a2\u7d22\u6548\u7387\u548c\u6837\u672c\u6548\u7387\uff0c\u5728\u590d\u6742\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u3002", "conclusion": "\u7ed3\u5408\u884c\u4e3a\u5171\u4eab\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.20839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20839", "abs": "https://arxiv.org/abs/2509.20839", "authors": ["Jiaxuan He", "Jiamei Ren", "Chongshang Yan", "Wenjie Song"], "title": "SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation", "comment": null, "summary": "In target-driven navigation and autonomous exploration, reasonable prediction\nof unknown regions is crucial for efficient navigation and environment\nunderstanding. Existing methods mostly focus on single objects or geometric\noccupancy maps, lacking the ability to model room-level semantic structures. We\npropose SemSight, a probabilistic bird's-eye-view prediction model for\nmulti-level scene semantics. The model jointly infers structural layouts,\nglobal scene context, and target area distributions, completing semantic maps\nof unexplored areas while estimating probability maps for target categories. To\ntrain SemSight, we simulate frontier-driven exploration on 2,000 indoor layout\ngraphs, constructing a diverse dataset of 40,000 sequential egocentric\nobservations paired with complete semantic maps. We adopt an encoder-decoder\nnetwork as the core architecture and introduce a mask-constrained supervision\nstrategy. This strategy applies a binary mask of unexplored areas so that\nsupervision focuses only on unknown regions, forcing the model to infer\nsemantic structures from the observed context. Experimental results show that\nSemSight improves prediction performance for key functional categories in\nunexplored regions and outperforms non-mask-supervised approaches on metrics\nsuch as Structural Consistency (SC) and Region Recognition Accuracy (PA). It\nalso enhances navigation efficiency in closed-loop simulations, reducing the\nnumber of search steps when guiding robots toward target areas.", "AI": {"tldr": "SemSight\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u5c42\u7ea7\u573a\u666f\u8bed\u4e49\u7684\u6982\u7387\u6027\u9e1f\u77b0\u56fe\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u8054\u5408\u63a8\u65ad\u7ed3\u6784\u5e03\u5c40\u3001\u5168\u5c40\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u76ee\u6807\u533a\u57df\u5206\u5e03\uff0c\u5728\u5b8c\u6210\u672a\u63a2\u7d22\u533a\u57df\u8bed\u4e49\u5730\u56fe\u7684\u540c\u65f6\u4f30\u8ba1\u76ee\u6807\u7c7b\u522b\u7684\u6982\u7387\u56fe\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u5bf9\u8c61\u6216\u51e0\u4f55\u5360\u636e\u5730\u56fe\uff0c\u7f3a\u4e4f\u5bf9\u623f\u95f4\u7ea7\u8bed\u4e49\u7ed3\u6784\u7684\u5efa\u6a21\u80fd\u529b\u3002\u5728\u76ee\u6807\u9a71\u52a8\u5bfc\u822a\u548c\u81ea\u4e3b\u63a2\u7d22\u4e2d\uff0c\u5bf9\u672a\u77e5\u533a\u57df\u7684\u5408\u7406\u9884\u6d4b\u5bf9\u4e8e\u9ad8\u6548\u5bfc\u822a\u548c\u73af\u5883\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u4f5c\u4e3a\u6838\u5fc3\u67b6\u6784\uff0c\u5f15\u5165\u63a9\u7801\u7ea6\u675f\u76d1\u7763\u7b56\u7565\u3002\u8be5\u7b56\u7565\u5e94\u7528\u672a\u63a2\u7d22\u533a\u57df\u7684\u4e8c\u503c\u63a9\u7801\uff0c\u4f7f\u76d1\u7763\u4ec5\u5173\u6ce8\u672a\u77e5\u533a\u57df\uff0c\u8feb\u4f7f\u6a21\u578b\u4ece\u89c2\u5bdf\u5230\u7684\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\u8bed\u4e49\u7ed3\u6784\u3002\u57282000\u4e2a\u5ba4\u5185\u5e03\u5c40\u56fe\u4e0a\u6a21\u62df\u524d\u6cbf\u9a71\u52a8\u63a2\u7d22\uff0c\u6784\u5efa\u4e86\u5305\u542b40000\u4e2a\u5e8f\u5217\u5316\u81ea\u6211\u4e2d\u5fc3\u89c2\u5bdf\u4e0e\u5b8c\u6574\u8bed\u4e49\u5730\u56fe\u914d\u5bf9\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSemSight\u63d0\u9ad8\u4e86\u672a\u63a2\u7d22\u533a\u57df\u5173\u952e\u529f\u80fd\u7c7b\u522b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u7ed3\u6784\u4e00\u81f4\u6027(SC)\u548c\u533a\u57df\u8bc6\u522b\u51c6\u786e\u7387(PA)\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u975e\u63a9\u7801\u76d1\u7763\u65b9\u6cd5\u3002\u5728\u95ed\u73af\u4eff\u771f\u4e2d\u8fd8\u63d0\u9ad8\u4e86\u5bfc\u822a\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u5f15\u5bfc\u673a\u5668\u4eba\u671d\u5411\u76ee\u6807\u533a\u57df\u7684\u641c\u7d22\u6b65\u6570\u3002", "conclusion": "SemSight\u901a\u8fc7\u6982\u7387\u6027\u9e1f\u77b0\u56fe\u9884\u6d4b\u548c\u591a\u5c42\u7ea7\u8bed\u4e49\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u672a\u77e5\u533a\u57df\u8bed\u4e49\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5bfc\u822a\u6548\u7387\uff0c\u4e3a\u81ea\u4e3b\u63a2\u7d22\u548c\u76ee\u6807\u9a71\u52a8\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u73af\u5883\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.20841", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20841", "abs": "https://arxiv.org/abs/2509.20841", "authors": ["Dekun Lu", "Wei Gao", "Kui Jia"], "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation", "comment": "First two authors contribute equally. Project page:\n  https://sites.google.com/view/imaginationpolicy", "summary": "End-to-end robot manipulation policies offer significant potential for\nenabling embodied agents to understand and interact with the world. Unlike\ntraditional modular pipelines, end-to-end learning mitigates key limitations\nsuch as information loss between modules and feature misalignment caused by\nisolated optimization targets. Despite these advantages, existing end-to-end\nneural networks for robotic manipulation--including those based on large\nVLM/VLA models--remain insufficiently performant for large-scale practical\ndeployment. In this paper, we take a step towards an end-to-end manipulation\npolicy that is generalizable, accurate and reliable. To achieve this goal, we\npropose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for\nrobotic manipulation. Our formulation is used as the action representation of a\nneural policy, which can be trained in an end-to-end fashion. Such an action\nrepresentation is general, as it extends the standard end-effector pose action\nrepresentation and supports a diverse set of manipulation tasks in a unified\nmanner. The oriented keypoint in our method enables natural generalization to\nobjects with different shapes and sizes, while achieving sub-centimeter\naccuracy. Moreover, our formulation can easily handle multi-stage tasks,\nmulti-modal robot behaviors, and deformable objects. Extensive simulated and\nhardware experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86CoMOK\uff08\u94fe\u5f0f\u79fb\u52a8\u5b9a\u5411\u5173\u952e\u70b9\uff09\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u52a8\u4f5c\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u8bad\u7ec3\uff0c\u652f\u6301\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e9a\u5398\u7c73\u7ea7\u7cbe\u5ea6\u3002", "motivation": "\u7aef\u5230\u7aef\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u76f8\u6bd4\u4f20\u7edf\u6a21\u5757\u5316\u7ba1\u9053\u5177\u6709\u907f\u514d\u4fe1\u606f\u4e22\u5931\u548c\u7279\u5f81\u4e0d\u5bf9\u9f50\u7684\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u57fa\u4e8eVLM/VLA\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u901a\u7528\u3001\u51c6\u786e\u548c\u53ef\u9760\u7684\u7aef\u5230\u7aef\u64cd\u4f5c\u7b56\u7565\u3002", "method": "\u63d0\u51faCoMOK\uff08\u94fe\u5f0f\u79fb\u52a8\u5b9a\u5411\u5173\u952e\u70b9\uff09\u4f5c\u4e3a\u52a8\u4f5c\u8868\u793a\uff0c\u6269\u5c55\u4e86\u6807\u51c6\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\uff0c\u5b9a\u5411\u5173\u952e\u70b9\u80fd\u591f\u81ea\u7136\u6cdb\u5316\u5230\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\u3002", "result": "\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u591f\u5904\u7406\u591a\u9636\u6bb5\u4efb\u52a1\u3001\u591a\u6a21\u6001\u673a\u5668\u4eba\u884c\u4e3a\u548c\u53ef\u53d8\u5f62\u7269\u4f53\uff0c\u5b9e\u73b0\u4e9a\u5398\u7c73\u7ea7\u7cbe\u5ea6\u3002", "conclusion": "CoMOK\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u51c6\u786e\u4e14\u53ef\u9760\u7684\u7aef\u5230\u7aef\u7b56\u7565\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.20843", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20843", "abs": "https://arxiv.org/abs/2509.20843", "authors": ["Ziang Luo", "Kangan Qian", "Jiahua Wang", "Yuechen Luo", "Jinyu Miao", "Zheng Fu", "Yunlong Wang", "Sicong Jiang", "Zilin Huang", "Yifei Hu", "Yuhao Yang", "Hao Ye", "Mengmeng Yang", "Xiaojian Dong", "Kun Jiang", "Diange Yang"], "title": "MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases", "comment": "8 pages", "summary": "Vision-Language Models(VLMs) have demonstrated significant potential for\nend-to-end autonomous driving, yet a substantial gap remains between their\ncurrent capabilities and the reliability necessary for real-world deployment. A\ncritical challenge is their fragility, characterized by hallucinations and poor\ngeneralization in out-of-distribution (OOD) scenarios. To bridge this gap, we\nintroduce MTRDrive, a novel framework that integrates procedural driving\nexperiences with a dynamic toolkit to enhance generalization and proactive\ndecision-making.\n  MTRDrive addresses these limitations through a closed-loop system that\ncombines a memory-based experience retrieval mechanism with dynamic toolkits.\nThis synergy enables the model to interact more effectively with its\nenvironment, improving both reasoning and decision-making capabilities with the\nhelp of our memory-tool synergistic reasoning. Additionally, we introduce a new\nbenchmark based on complex Roadwork construction scenarios to rigorously\nevaluate zero-shot generalization.\n  Extensive experiments demonstrate the superior effectiveness of our approach.\nOn the public NAVSIM benchmark, our 3B-parameter MTRDrive model achieves an\nexceptional PDMS of 88.3 without chain-of-thought and sets a state-of-the-art\nperformance bar on high-level planning, with a driving metric score of 79.8\\%\nand a planning accuracy of 82.6\\%. Rigorous zero-shot evaluation on the new\nRoadwork-VLM benchmark shows a strong ability to reason robustly in unseen\nscenarios, achieving a driving metric score of 80.2\\%. These results highlight\nMTRDrive's potential to advance autonomous driving toward safer and more\nreliable systems.", "AI": {"tldr": "MTRDrive\u662f\u4e00\u4e2a\u96c6\u6210\u7a0b\u5e8f\u5316\u9a7e\u9a76\u7ecf\u9a8c\u548c\u52a8\u6001\u5de5\u5177\u5305\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e3b\u52a8\u51b3\u7b56\u80fd\u529b\uff0c\u5728NAVSIM\u548cRoadwork-VLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b58\u5728\u5e7b\u89c9\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u9700\u8981\u63d0\u5347\u5176\u53ef\u9760\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u95ed\u73af\u7cfb\u7edf\uff0c\u7ed3\u5408\u57fa\u4e8e\u8bb0\u5fc6\u7684\u7ecf\u9a8c\u68c0\u7d22\u673a\u5236\u548c\u52a8\u6001\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u8bb0\u5fc6-\u5de5\u5177\u534f\u540c\u63a8\u7406\u6765\u589e\u5f3a\u73af\u5883\u4ea4\u4e92\u3001\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c3B\u53c2\u6570\u7684MTRDrive\u6a21\u578b\u8fbe\u523088.3 PDMS\uff08\u65e0\u601d\u7ef4\u94fe\uff09\uff0c\u9a7e\u9a76\u6307\u6807\u5f97\u520679.8%\uff0c\u89c4\u5212\u51c6\u786e\u738782.6%\uff1b\u5728Roadwork-VLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u96f6\u6837\u672c\u6cdb\u5316\u8fbe\u523080.2%\u9a7e\u9a76\u6307\u6807\u5f97\u5206\u3002", "conclusion": "MTRDrive\u5c55\u793a\u4e86\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u5411\u66f4\u5b89\u5168\u53ef\u9760\u7cfb\u7edf\u53d1\u5c55\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.20917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20917", "abs": "https://arxiv.org/abs/2509.20917", "authors": ["Xiaohan Ye", "Kui Wu", "Zherong Pan", "Taku Komura"], "title": "Efficient Differentiable Contact Model with Long-range Influence", "comment": null, "summary": "With the maturation of differentiable physics, its role in various downstream\napplications: such as model predictive control, robotic design optimization,\nand neural PDE solvers, has become increasingly important. However, the\nderivative information provided by differentiable simulators can exhibit abrupt\nchanges or vanish altogether, impeding the convergence of gradient-based\noptimizers. In this work, we demonstrate that such erratic gradient behavior is\nclosely tied to the design of contact models. We further introduce a set of\nproperties that a contact model must satisfy to ensure well-behaved gradient\ninformation. Lastly, we present a practical contact model for differentiable\nrigid-body simulators that satisfies all of these properties while maintaining\ncomputational efficiency. Our experiments show that, even from simple\ninitializations, our contact model can discover complex, contact-rich control\nsignals, enabling the successful execution of a range of downstream locomotion\nand manipulation tasks.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u4e2d\u68af\u5ea6\u4fe1\u606f\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u63a5\u89e6\u6a21\u578b\u8bbe\u8ba1\u51c6\u5219\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6ee1\u8db3\u6240\u6709\u51c6\u5219\u7684\u9ad8\u6548\u63a5\u89e6\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68af\u5ea6\u4f18\u5316\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u5176\u63d0\u4f9b\u7684\u68af\u5ea6\u4fe1\u606f\u7ecf\u5e38\u51fa\u73b0\u7a81\u53d8\u6216\u6d88\u5931\uff0c\u963b\u788d\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5668\u6536\u655b\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u7684\u68af\u5ea6\u884c\u4e3a\u4e0e\u63a5\u89e6\u6a21\u578b\u7684\u8bbe\u8ba1\u5bc6\u5207\u76f8\u5173\u3002", "method": "\u9996\u5148\u5206\u6790\u4e86\u63a5\u89e6\u6a21\u578b\u4e0e\u68af\u5ea6\u884c\u4e3a\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u786e\u4fdd\u826f\u597d\u68af\u5ea6\u4fe1\u606f\u7684\u63a5\u89e6\u6a21\u578b\u8bbe\u8ba1\u51c6\u5219\u3002\u7136\u540e\u5f00\u53d1\u4e86\u4e00\u4e2a\u6ee1\u8db3\u6240\u6709\u51c6\u5219\u7684\u5b9e\u7528\u63a5\u89e6\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u7a33\u5b9a\u7684\u68af\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u4ece\u7b80\u5355\u521d\u59cb\u5316\u5f00\u59cb\uff0c\u65b0\u63a5\u89e6\u6a21\u578b\u4e5f\u80fd\u53d1\u73b0\u590d\u6742\u7684\u63a5\u89e6\u5bc6\u96c6\u578b\u63a7\u5236\u4fe1\u53f7\uff0c\u6210\u529f\u6267\u884c\u5404\u79cd\u4e0b\u6e38\u8fd0\u52a8\u548c\u63a7\u5236\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u6ee1\u8db3\u7279\u5b9a\u5c5e\u6027\u7684\u63a5\u89e6\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u4e2d\u7684\u68af\u5ea6\u884c\u4e3a\uff0c\u4e3a\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4f18\u5316\u57fa\u7840\u3002"}}
{"id": "2509.20938", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20938", "abs": "https://arxiv.org/abs/2509.20938", "authors": ["Jianbo Zhao", "Taiyu Ban", "Xiangjie Li", "Xingtai Gui", "Hangning Zhou", "Lei Liu", "Hongwei Zhao", "Bin Li"], "title": "Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement", "comment": null, "summary": "The inherent sequential modeling capabilities of autoregressive models make\nthem a formidable baseline for end-to-end planning in autonomous driving.\nNevertheless, their performance is constrained by a spatio-temporal\nmisalignment, as the planner must condition future actions on past sensory\ndata. This creates an inconsistent worldview, limiting the upper bound of\nperformance for an otherwise powerful approach. To address this, we propose a\nTime-Invariant Spatial Alignment (TISA) module that learns to project initial\nenvironmental features into a consistent ego-centric frame for each future time\nstep, effectively correcting the agent's worldview without explicit future\nscene prediction. In addition, we employ a kinematic action prediction head\n(i.e., acceleration and yaw rate) to ensure physically feasible trajectories.\nFinally, we introduce a multi-objective post-training stage using Direct\nPreference Optimization (DPO) to move beyond pure imitation. Our approach\nprovides targeted feedback on specific driving behaviors, offering a more\nfine-grained learning signal than the single, overall objective used in\nstandard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM\ndataset among autoregressive models. The video document is available at\nhttps://tisa-dpo-e2e.github.io/.", "AI": {"tldr": "\u63d0\u51faTISA\u6a21\u5757\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u65f6\u7a7a\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7DPO\u591a\u76ee\u6807\u540e\u8bad\u7ec3\u8d85\u8d8a\u7eaf\u6a21\u4eff\u5b66\u4e60\uff0c\u5728NAVSIM\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.8 PDMS\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7aef\u5230\u7aef\u89c4\u5212\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u53d7\u9650\u4e8e\u65f6\u7a7a\u4e0d\u5bf9\u9f50\u95ee\u9898\u2014\u2014\u89c4\u5212\u5668\u5fc5\u987b\u57fa\u4e8e\u8fc7\u53bb\u611f\u77e5\u6570\u636e\u9884\u6d4b\u672a\u6765\u52a8\u4f5c\uff0c\u9020\u6210\u4e16\u754c\u89c2\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u6027\u80fd\u4e0a\u9650\u3002", "method": "1) TISA\u6a21\u5757\u5c06\u521d\u59cb\u73af\u5883\u7279\u5f81\u6295\u5f71\u5230\u6bcf\u4e2a\u672a\u6765\u65f6\u95f4\u6b65\u7684\u4e00\u81f4\u81ea\u8f66\u5750\u6807\u7cfb\uff1b2) \u8fd0\u52a8\u5b66\u52a8\u4f5c\u9884\u6d4b\u5934\u786e\u4fdd\u7269\u7406\u53ef\u884c\u7684\u8f68\u8ff9\uff1b3) \u591a\u76ee\u6807DPO\u540e\u8bad\u7ec3\u9636\u6bb5\u63d0\u4f9b\u9488\u5bf9\u7279\u5b9a\u9a7e\u9a76\u884c\u4e3a\u7684\u7cbe\u7ec6\u53cd\u9988\u3002", "result": "\u5728NAVSIM\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u8fbe\u5230\u4e8689.8 PDMS\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TISA\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u65f6\u7a7a\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u7ed3\u5408DPO\u591a\u76ee\u6807\u8bad\u7ec3\u8d85\u8d8a\u4e86\u7eaf\u6a21\u4eff\u5b66\u4e60\u7684\u9650\u5236\uff0c\u4e3a\u81ea\u56de\u5f52\u89c4\u5212\u5668\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u5b66\u4e60\u4fe1\u53f7\u3002"}}
{"id": "2509.20964", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20964", "abs": "https://arxiv.org/abs/2509.20964", "authors": ["Rubaiyat Tasnim Chowdhury", "Nayan Bala", "Ronojoy Roy", "Tarek Mahmud"], "title": "BactoBot: A Low-Cost, Bacteria-Inspired Soft Underwater Robot for Marine Exploration", "comment": "8 pages, 4 figures. Project repository available at\n  https://github.com/rubaiyattasnim/BactoBot", "summary": "Traditional rigid underwater vehicles pose risks to delicate marine\necosystems. This paper presents BactoBot, a low-cost, soft underwater robot\ndesigned for safe and gentle marine exploration. Inspired by bacterial\nflagellar propulsion, BactoBot features 12 flexible, silicone-based arms\narranged on a 3D-printed dodecahedral frame. The design provides inherent\ncompliance, redundancy, and the potential for omnidirectional movement. The\nprototype was fabricated using accessible DIY methods, including food-grade\nsilicone molding, 3D printing, and off-the-shelf microcontrollers.\nWaterproofing and buoyancy calibration protocols were developed, and the robot\nwas successfully tested in a controlled water tank, demonstrating forward\nmotion and turning. The results validate the feasibility of replicating complex\nbiological locomotion at low cost. The project lays a foundation for\nenvironmentally conscious robotic tools, particularly for marine science in\nresource-constrained settings, and identifies pathways toward autonomous\noperation and field deployment.", "AI": {"tldr": "BactoBot\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u8f6f\u4f53\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u91c7\u7528\u4eff\u7ec6\u83cc\u97ad\u6bdb\u63a8\u8fdb\u768412\u4e2a\u67d4\u6027\u7845\u80f6\u81c2\u8bbe\u8ba1\uff0c\u5177\u6709\u56fa\u6709\u67d4\u987a\u6027\u3001\u5197\u4f59\u6027\u548c\u5168\u5411\u8fd0\u52a8\u6f5c\u529b\uff0c\u901a\u8fc7DIY\u65b9\u6cd5\u5236\u9020\u5e76\u5728\u6c34\u7bb1\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u6c34\u4e0b\u673a\u5668\u4eba\u5bf9\u8106\u5f31\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u6784\u6210\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u5b89\u5168\u6e29\u548c\u7684\u6d77\u6d0b\u63a2\u7d22\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u4eff\u7ec6\u83cc\u97ad\u6bdb\u63a8\u8fdb\u8bbe\u8ba1\uff0c12\u4e2a\u67d4\u6027\u7845\u80f6\u81c2\u5b89\u88c5\u57283D\u6253\u5370\u5341\u4e8c\u9762\u4f53\u6846\u67b6\u4e0a\uff0c\u4f7f\u7528\u98df\u54c1\u7ea7\u7845\u80f6\u6210\u578b\u30013D\u6253\u5370\u548c\u73b0\u6210\u5fae\u63a7\u5236\u5668\u7b49DIY\u65b9\u6cd5\u5236\u9020\uff0c\u5f00\u53d1\u4e86\u9632\u6c34\u548c\u6d6e\u529b\u6821\u51c6\u534f\u8bae\u3002", "result": "\u5728\u53d7\u63a7\u6c34\u7bb1\u4e2d\u6210\u529f\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u524d\u8fdb\u8fd0\u52a8\u548c\u8f6c\u5411\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u6210\u672c\u590d\u5236\u590d\u6742\u751f\u7269\u8fd0\u52a8\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u9879\u76ee\u4e3a\u73af\u5883\u53cb\u597d\u578b\u673a\u5668\u4eba\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u6d77\u6d0b\u79d1\u5b66\u7814\u7a76\uff0c\u5e76\u786e\u5b9a\u4e86\u5b9e\u73b0\u81ea\u4e3b\u64cd\u4f5c\u548c\u73b0\u573a\u90e8\u7f72\u7684\u8def\u5f84\u3002"}}
{"id": "2509.21006", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21006", "abs": "https://arxiv.org/abs/2509.21006", "authors": ["Konstantin Gubernatorov", "Artem Voronov", "Roman Voronov", "Sergei Pasynkov", "Stepan Perminov", "Ziang Guo", "Dzmitry Tsetserukou"], "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation", "comment": null, "summary": "We address natural language pick-and-place in unseen, unpredictable indoor\nenvironments with AnywhereVLA, a modular framework for mobile manipulation. A\nuser text prompt serves as an entry point and is parsed into a structured task\ngraph that conditions classical SLAM with LiDAR and cameras, metric semantic\nmapping, and a task-aware frontier exploration policy. An approach planner then\nselects visibility and reachability aware pre grasp base poses. For\ninteraction, a compact SmolVLA manipulation head is fine tuned on platform pick\nand place trajectories for the SO-101 by TheRobotStudio, grounding local visual\ncontext and sub-goals into grasp and place proposals. The full system runs\nfully onboard on consumer-level hardware, with Jetson Orin NX for perception\nand VLA and an Intel NUC for SLAM, exploration, and control, sustaining\nreal-time operation. We evaluated AnywhereVLA in a multi-room lab under static\nscenes and normal human motion. In this setting, the system achieves a $46\\%$\noverall task success rate while maintaining throughput on embedded compute. By\ncombining a classical stack with a fine-tuned VLA manipulation, the system\ninherits the reliability of geometry-based navigation with the agility and task\ngeneralization of language-conditioned manipulation.", "AI": {"tldr": "AnywhereVLA\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u64cd\u4f5c\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u6267\u884c\u81ea\u7136\u8bed\u8a00\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86\u7ecf\u5178SLAM\u4e0eVLA\u64cd\u4f5c\uff0c\u5728\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u884c\uff0c\u8fbe\u523046%\u7684\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u6267\u884c\u81ea\u7136\u8bed\u8a00\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u7684\u6311\u6218\uff0c\u7ed3\u5408\u51e0\u4f55\u5bfc\u822a\u7684\u53ef\u9760\u6027\u548c\u8bed\u8a00\u6761\u4ef6\u64cd\u4f5c\u7684\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u5316\u6846\u67b6\uff1a\u7528\u6237\u6587\u672c\u63d0\u793a\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u4efb\u52a1\u56fe\uff0c\u7ed3\u5408\u7ecf\u5178SLAM\uff08LiDAR\u548c\u76f8\u673a\uff09\u3001\u5ea6\u91cf\u8bed\u4e49\u6620\u5c04\u3001\u4efb\u52a1\u611f\u77e5\u8fb9\u754c\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u53ca\u57fa\u4e8eSmolVLA\u5fae\u8c03\u7684\u64cd\u4f5c\u5934\u8fdb\u884c\u6293\u53d6\u548c\u653e\u7f6e\u3002", "result": "\u5728\u591a\u623f\u95f4\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u5728\u9759\u6001\u573a\u666f\u548c\u6b63\u5e38\u4eba\u7c7b\u8fd0\u52a8\u4e0b\u8fbe\u523046%\u7684\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u8ba1\u7b97\u4e0a\u4fdd\u6301\u5b9e\u65f6\u541e\u5410\u91cf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7ecf\u5178\u5806\u6808\u548c\u5fae\u8c03VLA\u64cd\u4f5c\uff0c\u7cfb\u7edf\u7ee7\u627f\u4e86\u57fa\u4e8e\u51e0\u4f55\u5bfc\u822a\u7684\u53ef\u9760\u6027\uff0c\u540c\u65f6\u83b7\u5f97\u4e86\u8bed\u8a00\u6761\u4ef6\u64cd\u4f5c\u7684\u7075\u6d3b\u6027\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.21020", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21020", "abs": "https://arxiv.org/abs/2509.21020", "authors": ["Abdelaziz Shaarawy", "Cansu Erdogan", "Rustam Stolkin", "Alireza Rastegarpanah"], "title": "Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting", "comment": null, "summary": "Electric-vehicle (EV) battery disassembly requires precise multi-robot\ncoordination, short and reliable motions, and robust collision safety in\ncluttered, dynamic scenes. We propose a four-layer task-and-motion planning\n(TAMP) framework that couples symbolic task planning and cost- and\naccessibility-aware allocation with a TP-GMM-guided motion planner learned from\ndemonstrations. Stereo vision with YOLOv8 provides real-time component\nlocalization, while OctoMap-based 3D mapping and FCL(Flexible Collision\nLibrary) checks in MoveIt unify predictive digital-twin collision checking with\nreactive, vision-based avoidance. Validated on two UR10e robots across cable,\nbusbar, service plug, and three leaf-cell removals, the approach yields\nsubstantially more compact and safer motions than a default RRTConnect baseline\nunder identical perception and task assignments: average end-effector path\nlength drops by $-63.3\\%$ and makespan by $-8.1\\%$; per-arm swept volumes\nshrink (R1: $0.583\\rightarrow0.139\\,\\mathrm{m}^3$; R2:\n$0.696\\rightarrow0.252\\,\\mathrm{m}^3$), and mutual overlap decreases by $47\\%$\n($0.064\\rightarrow0.034\\,\\mathrm{m}^3$). These results highlight improved\nautonomy, precision, and safety for multi-robot EV battery disassembly in\nunstructured, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u5c42\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u62c6\u5378\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\uff0c\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u4efb\u52a1\u89c4\u5212\u548c\u57fa\u4e8e\u6f14\u793a\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u7d27\u51d1\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u62c6\u5378\u9700\u8981\u7cbe\u786e\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u3001\u77ed\u800c\u53ef\u9760\u7684\u8fd0\u52a8\uff0c\u4ee5\u53ca\u5728\u6742\u4e71\u52a8\u6001\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u78b0\u649e\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u56db\u5c42\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u7b26\u53f7\u4efb\u52a1\u89c4\u5212\u548c\u6210\u672c\u53ef\u53ca\u6027\u5206\u914d\uff0c\u4f7f\u7528TP-GMM\u5f15\u5bfc\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u548cYOLOv8\u8fdb\u884c\u5b9e\u65f6\u7ec4\u4ef6\u5b9a\u4f4d\uff0cOctoMap 3D\u6620\u5c04\u548cFCL\u78b0\u649e\u68c0\u6d4b\u7edf\u4e00\u9884\u6d4b\u6027\u6570\u5b57\u5b6a\u751f\u78b0\u649e\u68c0\u67e5\u4e0e\u53cd\u5e94\u6027\u89c6\u89c9\u907f\u969c\u3002", "result": "\u5728\u4e24\u4e2aUR10e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u9ed8\u8ba4RRTConnect\u57fa\u7ebf\uff0c\u5e73\u5747\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u957f\u5ea6\u51cf\u5c1163.3%\uff0c\u5236\u9020\u5468\u671f\u51cf\u5c118.1%\uff0c\u5355\u81c2\u626b\u63a0\u4f53\u79ef\u663e\u8457\u51cf\u5c0f\uff08R1: 0.583\u21920.139 m\u00b3; R2: 0.696\u21920.252 m\u00b3\uff09\uff0c\u76f8\u4e92\u91cd\u53e0\u51cf\u5c1147%\uff080.064\u21920.034 m\u00b3\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u673a\u5668\u4eba\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u62c6\u5378\u7684\u81ea\u4e3b\u6027\u3001\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.21027", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21027", "abs": "https://arxiv.org/abs/2509.21027", "authors": ["Sibo Li", "Qianyue Hao", "Yu Shang", "Yong Li"], "title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models", "comment": null, "summary": "Robotic world models are a promising paradigm for forecasting future\nenvironment states, yet their inference speed and the physical plausibility of\ngenerated trajectories remain critical bottlenecks, limiting their real-world\napplications. This stems from the redundancy of the prevailing frame-to-frame\ngeneration approach, where the model conducts costly computation on similar\nframes, as well as neglecting the semantic importance of key transitions. To\naddress this inefficiency, we propose KeyWorld, a framework that improves\ntext-conditioned robotic world models by concentrating transformers computation\non a few semantic key frames while employing a lightweight convolutional model\nto fill the intermediate frames. Specifically, KeyWorld first identifies\nsignificant transitions by iteratively simplifying the robot's motion\ntrajectories, obtaining the ground truth key frames. Then, a DiT model is\ntrained to reason and generate these physically meaningful key frames from\ntextual task descriptions. Finally, a lightweight interpolator efficiently\nreconstructs the full video by inpainting all intermediate frames. Evaluations\non the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\\times$\nacceleration compared to the frame-to-frame generation baseline, and focusing\non the motion-aware key frames further contributes to the physical validity of\nthe generated videos, especially on complex tasks. Our approach highlights a\npractical path toward deploying world models in real-time robotic control and\nother domains requiring both efficient and effective world models. Code is\nreleased at https://anonymous.4open.science/r/Keyworld-E43D.", "AI": {"tldr": "KeyWorld\u662f\u4e00\u4e2a\u6539\u8fdb\u6587\u672c\u6761\u4ef6\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u96c6\u4e2d\u5728\u5c11\u6570\u8bed\u4e49\u5173\u952e\u5e27\u4e0a\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u6a21\u578b\u586b\u5145\u4e2d\u95f4\u5e27\uff0c\u5b9e\u73b05.68\u500d\u52a0\u901f\u5e76\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u548c\u751f\u6210\u8f68\u8ff9\u7269\u7406\u5408\u7406\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5e27\u5230\u5e27\u751f\u6210\u65b9\u6cd5\u7684\u5197\u4f59\u8ba1\u7b97\u4ee5\u53ca\u5ffd\u89c6\u5173\u952e\u8fc7\u6e21\u7684\u8bed\u4e49\u91cd\u8981\u6027\u3002", "method": "1) \u901a\u8fc7\u8fed\u4ee3\u7b80\u5316\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u8bc6\u522b\u5173\u952e\u5e27\uff1b2) \u8bad\u7ec3DiT\u6a21\u578b\u4ece\u6587\u672c\u4efb\u52a1\u63cf\u8ff0\u751f\u6210\u7269\u7406\u610f\u4e49\u5173\u952e\u5e27\uff1b3) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u63d2\u503c\u5668\u9ad8\u6548\u91cd\u5efa\u5b8c\u6574\u89c6\u9891\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKeyWorld\u76f8\u6bd4\u5e27\u5230\u5e27\u751f\u6210\u57fa\u7ebf\u5b9e\u73b05.68\u500d\u52a0\u901f\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7279\u522b\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u548c\u5176\u4ed6\u9700\u8981\u9ad8\u6548\u6709\u6548\u4e16\u754c\u6a21\u578b\u7684\u9886\u57df\u90e8\u7f72\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.21045", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21045", "abs": "https://arxiv.org/abs/2509.21045", "authors": ["Mahya Ramezani", "M. Amin Alandihallaj", "Bar\u0131\u015f Can Yal\u00e7\u0131n", "Miguel Angel Olivares Mendez", "Holger Voos"], "title": "MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation", "comment": "Pre-print version submitted to IEEE IROS", "summary": "This paper presents an integrated Reinforcement Learning (RL) and Model\nPredictive Control (MPC) framework for autonomous satellite docking with a\npartially filled fuel tank. Traditional docking control faces challenges due to\nfuel sloshing in microgravity, which induces unpredictable forces affecting\nstability. To address this, we integrate Proximal Policy Optimization (PPO) and\nSoft Actor-Critic (SAC) RL algorithms with MPC, leveraging MPC's predictive\ncapabilities to accelerate RL training and improve control robustness. The\nproposed approach is validated through Zero-G Lab of SnT experiments for planar\nstabilization and high-fidelity numerical simulations for 6-DOF docking with\nfuel sloshing dynamics. Simulation results demonstrate that SAC-MPC achieves\nsuperior docking accuracy, higher success rates, and lower control effort,\noutperforming standalone RL and PPO-MPC methods. This study advances\nfuel-efficient and disturbance-resilient satellite docking, enhancing the\nfeasibility of on-orbit refueling and servicing missions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u71c3\u6599\u7bb1\u536b\u661f\u81ea\u4e3b\u5bf9\u63a5\u4e2d\u7684\u71c3\u6599\u6643\u52a8\u95ee\u9898\uff0c\u901a\u8fc7PPO\u548cSAC\u7b97\u6cd5\u4e0eMPC\u96c6\u6210\uff0c\u63d0\u9ad8\u63a7\u5236\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5bf9\u63a5\u63a7\u5236\u9762\u4e34\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u71c3\u6599\u6643\u52a8\u5f15\u8d77\u7684\u4e0d\u53ef\u9884\u6d4b\u529b\uff0c\u5f71\u54cd\u7a33\u5b9a\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u71c3\u6599\u6643\u52a8\u5e72\u6270\u7684\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u5728\u8f68\u52a0\u6ce8\u548c\u670d\u52a1\u4efb\u52a1\u3002", "method": "\u96c6\u6210PPO\u548cSAC\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5229\u7528MPC\u7684\u9884\u6d4b\u80fd\u529b\u52a0\u901fRL\u8bad\u7ec3\u5e76\u63d0\u9ad8\u63a7\u5236\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u96f6\u91cd\u529b\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u9a8c\u8bc1\u5e73\u9762\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u9ad8\u4fdd\u771f\u6570\u503c\u6a21\u62df\u9a8c\u8bc16\u81ea\u7531\u5ea6\u5bf9\u63a5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cSAC-MPC\u65b9\u6cd5\u5728\u5bf9\u63a5\u7cbe\u5ea6\u3001\u6210\u529f\u7387\u548c\u63a7\u5236\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff0c\u4f18\u4e8e\u5355\u72ec\u7684RL\u548cPPO-MPC\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u71c3\u6599\u9ad8\u6548\u548c\u6297\u5e72\u6270\u7684\u536b\u661f\u5bf9\u63a5\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u5728\u8f68\u52a0\u6ce8\u548c\u670d\u52a1\u4efb\u52a1\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.21073", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21073", "abs": "https://arxiv.org/abs/2509.21073", "authors": ["Simon Kristoffersson Lind", "Jialong Li", "Maj Stenmark", "Volker Kr\u00fcger"], "title": "Normalizing Flows are Capable Visuomotor Policy Learning Models", "comment": null, "summary": "The field of general purpose robotics has recently embraced powerful\nprobabilistic models, such as diffusion models, to model and learn complex\nbehaviors. However, these models often come with significant trade-offs, namely\nhigh computational costs for inference and a fundamental inability to quantify\noutput uncertainty. We argue that a model's trustworthiness, a critical factor\nfor reliable, general-purpose robotics, is inherently linked to its ability to\nprovide confidence measures.\n  In this work, we introduce Normalizing Flows Policy, a novel visuomotor\npolicy learning model based on Normalizing Flows. We show that Normalizing\nFlows are a natural and powerful alternative to diffusion models, providing\nboth a statistically sound measure of confidence and a highly efficient\ninference process. Through comprehensive experiments across four distinct\nsimulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves\nperformance comparable to, and often surpassing, Diffusion Policy, and it does\nso not only with improved sample efficiency but also with up to 30 times faster\ninference. Additionally, our ablation study validates several key architectural\nand training techniques that enable Normalizing Flows to perform well in this\ndomain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684Normalizing Flows Policy\uff0c\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u548c30\u500d\u63a8\u7406\u901f\u5ea6\u63d0\u5347", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u4eba\u7b56\u7565\u5b58\u5728\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u65e0\u6cd5\u91cf\u5316\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u800c\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u5bf9\u53ef\u9760\u901a\u7528\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981", "method": "\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u6784\u5efa\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u9006\u53d8\u6362\u63d0\u4f9b\u7edf\u8ba1\u4e0a\u5408\u7406\u7684\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u548c\u9ad8\u6548\u63a8\u7406", "result": "\u5728\u56db\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4e0e\u6269\u6563\u7b56\u7565\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u6837\u672c\u6548\u7387\u63d0\u5347\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe30\u500d", "conclusion": "\u5f52\u4e00\u5316\u6d41\u662f\u6269\u6563\u6a21\u578b\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u901a\u7528\u673a\u5668\u4eba\u9886\u57df"}}
{"id": "2509.21085", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21085", "abs": "https://arxiv.org/abs/2509.21085", "authors": ["Chenyu Zhao", "Jingao Xu", "Ciyu Ruan", "Haoyang Wang", "Shengbo Wang", "Jiaqi Li", "Jirong Zha", "Weijie Hong", "Zheng Yang", "Yunhao Liu", "Xiao-Ping Zhang", "Xinlei Chen"], "title": "Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect for Accurate Edge Detection", "comment": null, "summary": "Drone-based rapid and accurate environmental edge detection is highly\nadvantageous for tasks such as disaster relief and autonomous navigation.\nCurrent methods, using radars or cameras, raise deployment costs and burden\nlightweight drones with high computational demands. In this paper, we propose\nAirTouch, a system that transforms the ground effect from a stability \"foe\" in\ntraditional flight control views, into a \"friend\" for accurate and efficient\nedge detection. Our key insight is that analyzing drone basic attitude sensor\nreadings and flight commands allows us to detect ground effect changes. Such\nchanges typically indicate the drone flying over a boundary of two materials,\nmaking this information valuable for edge detection. We approach this insight\nthrough theoretical analysis, algorithm design, and implementation, fully\nleveraging the ground effect as a new sensing modality without compromising\ndrone flight stability, thereby achieving accurate and efficient scene edge\ndetection. We also compare this new sensing modality with vision-based methods\nto clarify its exclusive advantages in resource efficiency and detection\ncapability. Extensive evaluations demonstrate that our system achieves a high\ndetection accuracy with mean detection distance errors of 0.051m, outperforming\nthe baseline method performance by 86%. With such detection performance, our\nsystem requires only 43 mW power consumption, contributing to this new sensing\nmodality for low-cost and highly efficient edge detection.", "AI": {"tldr": "AirTouch\u7cfb\u7edf\u5c06\u5730\u9762\u6548\u5e94\u4ece\u98de\u884c\u63a7\u5236\u7684'\u654c\u4eba'\u8f6c\u53d8\u4e3a'\u670b\u53cb'\uff0c\u5229\u7528\u65e0\u4eba\u673a\u59ff\u6001\u4f20\u611f\u5668\u8bfb\u6570\u68c0\u6d4b\u5730\u9762\u6548\u5e94\u53d8\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u8fb9\u7f18\u68c0\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u96f7\u8fbe\u6216\u6444\u50cf\u5934\u7684\u65e0\u4eba\u673a\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u800c\u5730\u9762\u6548\u5e94\u901a\u5e38\u88ab\u89c6\u4e3a\u98de\u884c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u5c06\u5176\u8f6c\u5316\u4e3a\u65b0\u7684\u611f\u77e5\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u7cfb\u7edf\u5b9e\u73b0\uff0c\u5229\u7528\u65e0\u4eba\u673a\u57fa\u672c\u59ff\u6001\u4f20\u611f\u5668\u8bfb\u6570\u548c\u98de\u884c\u547d\u4ee4\u68c0\u6d4b\u5730\u9762\u6548\u5e94\u53d8\u5316\uff0c\u8fd9\u4e9b\u53d8\u5316\u901a\u5e38\u8868\u660e\u65e0\u4eba\u673a\u98de\u8d8a\u4e24\u79cd\u6750\u6599\u7684\u8fb9\u754c\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e73\u5747\u68c0\u6d4b\u8ddd\u79bb\u8bef\u5dee\u4e3a0.051\u7c73\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u534786%\uff0c\u4ec5\u970043\u6beb\u74e6\u529f\u8017\u3002", "conclusion": "\u5730\u9762\u6548\u5e94\u53ef\u4f5c\u4e3a\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7387\u8fb9\u7f18\u68c0\u6d4b\u7684\u65b0\u611f\u77e5\u6a21\u5f0f\uff0c\u5728\u8d44\u6e90\u6548\u7387\u548c\u68c0\u6d4b\u80fd\u529b\u65b9\u9762\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2509.21107", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21107", "abs": "https://arxiv.org/abs/2509.21107", "authors": ["William Barron", "Xiaoxiang Dong", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Cross-Modal Instructions for Robot Motion Generation", "comment": null, "summary": "Teaching robots novel behaviors typically requires motion demonstrations via\nteleoperation or kinaesthetic teaching, that is, physically guiding the robot.\nWhile recent work has explored using human sketches to specify desired\nbehaviors, data collection remains cumbersome, and demonstration datasets are\ndifficult to scale. In this paper, we introduce an alternative paradigm,\nLearning from Cross-Modal Instructions, where robots are shaped by\ndemonstrations in the form of rough annotations, which can contain free-form\ntext labels, and are used in lieu of physical motion. We introduce the\nCrossInstruct framework, which integrates cross-modal instructions as examples\ninto the context input to a foundational vision-language model (VLM). The VLM\nthen iteratively queries a smaller, fine-tuned model, and synthesizes the\ndesired motion over multiple 2D views. These are then subsequently fused into a\ncoherent distribution over 3D motion trajectories in the robot's workspace. By\nincorporating the reasoning of the large VLM with a fine-grained pointing\nmodel, CrossInstruct produces executable robot behaviors that generalize beyond\nthe environment of in the limited set of instruction examples. We then\nintroduce a downstream reinforcement learning pipeline that leverages\nCrossInstruct outputs to efficiently learn policies to complete fine-grained\ntasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and\nreal hardware, demonstrating effectiveness without additional fine-tuning and\nproviding a strong initialization for policies subsequently refined via\nreinforcement learning.", "AI": {"tldr": "CrossInstruct\u6846\u67b6\u4f7f\u7528\u8de8\u6a21\u6001\u6307\u4ee4\uff08\u5982\u6587\u672c\u6807\u7b7e\uff09\u66ff\u4ee3\u7269\u7406\u6f14\u793a\u6765\u6307\u5bfc\u673a\u5668\u4eba\u5b66\u4e60\u884c\u4e3a\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7cbe\u7ec6\u6307\u5411\u6a21\u578b\u751f\u62103D\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u652f\u6301\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u884c\u4e3a\u5b66\u4e60\u9700\u8981\u7269\u7406\u6f14\u793a\uff0c\u6570\u636e\u6536\u96c6\u56f0\u96be\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u63a2\u7d22\u4f7f\u7528\u7c97\u7565\u6807\u6ce8\uff08\u5982\u6587\u672c\u6807\u7b7e\uff09\u4f5c\u4e3a\u66ff\u4ee3\u6f14\u793a\u65b9\u5f0f\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u6536\u96c6\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faCrossInstruct\u6846\u67b6\uff0c\u5c06\u8de8\u6a21\u6001\u6307\u4ee4\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u5230\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\uff0cVLM\u8fed\u4ee3\u67e5\u8be2\u8f83\u5c0f\u7684\u5fae\u8c03\u6a21\u578b\uff0c\u5728\u591a\u4e2a2D\u89c6\u56fe\u4e2d\u5408\u6210\u671f\u671b\u8fd0\u52a8\uff0c\u7136\u540e\u878d\u5408\u62103D\u8fd0\u52a8\u8f68\u8ff9\u5206\u5e03\u3002", "result": "\u5728\u57fa\u51c6\u4eff\u771f\u4efb\u52a1\u548c\u771f\u5b9e\u786c\u4ef6\u4e0a\u7684\u4e25\u683c\u8bc4\u4f30\u8868\u660e\uff0cCrossInstruct\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u6709\u6548\u5de5\u4f5c\uff0c\u5e76\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u7b56\u7565\u521d\u59cb\u5316\u3002", "conclusion": "CrossInstruct\u901a\u8fc7\u8de8\u6a21\u6001\u6307\u4ee4\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u884c\u4e3a\u7684\u6709\u6548\u5b66\u4e60\uff0c\u8d85\u8d8a\u4e86\u6709\u9650\u6307\u4ee4\u793a\u4f8b\u7684\u73af\u5883\u9650\u5236\uff0c\u5e76\u4e3a\u7cbe\u7ec6\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u9014\u5f84\u3002"}}
{"id": "2509.21122", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21122", "abs": "https://arxiv.org/abs/2509.21122", "authors": ["Mingjiang Liu", "Hailong Huang"], "title": "Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study", "comment": "Accepted for presentation at the Advancements in Aerial Physical\n  Interaction Workshop of the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2025", "summary": "This paper addresses a drone ball-balancing task, in which a drone stabilizes\na ball atop a movable beam through cable-based interaction. We propose a\nhierarchical control framework that decouples high-level balancing policy from\nlow-level drone control, and train a reinforcement learning (RL) policy to\nhandle the high-level decision-making. Simulation results show that the RL\npolicy achieves superior performance compared to carefully tuned PID\ncontrollers within the same hierarchical structure. Through systematic\ncomparative analysis, we demonstrate that RL's advantage stems not from\nimproved parameter tuning or inherent nonlinear mapping capabilities, but from\nits ability to effectively utilize richer state observations. These findings\nunderscore the critical role of comprehensive state representation in\nlearning-based systems and suggest that enhanced sensing could be instrumental\nin improving controller performance.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9ad8\u5c42\u5e73\u8861\u7b56\u7565\uff0c\u5728\u65e0\u4eba\u673a\u7403\u5e73\u8861\u4efb\u52a1\u4e2d\u4f18\u4e8ePID\u63a7\u5236\u5668\uff0c\u4f18\u52bf\u6e90\u4e8e\u66f4\u6709\u6548\u5730\u5229\u7528\u72b6\u6001\u89c2\u6d4b\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u901a\u8fc7\u7f06\u7ef3\u4ea4\u4e92\u7a33\u5b9a\u7403\u4f53\u5728\u53ef\u79fb\u52a8\u6881\u4e0a\u7684\u5e73\u8861\u4efb\u52a1\uff0c\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff1a\u9ad8\u5c42\u5e73\u8861\u7b56\u7565\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f4e\u5c42\u65e0\u4eba\u673a\u63a7\u5236\u5206\u79bb\u5904\u7406\uff1b\u4e0ePID\u63a7\u5236\u5668\u5728\u76f8\u540c\u7ed3\u6784\u4e0b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u4f18\u4e8e\u7cbe\u5fc3\u8c03\u53c2\u7684PID\u63a7\u5236\u5668\uff1b\u4f18\u52bf\u4e3b\u8981\u6765\u81ea\u66f4\u6709\u6548\u5730\u5229\u7528\u4e30\u5bcc\u7684\u72b6\u6001\u89c2\u6d4b\u4fe1\u606f\uff0c\u800c\u975e\u53c2\u6570\u8c03\u4f18\u6216\u975e\u7ebf\u6027\u6620\u5c04\u80fd\u529b\u3002", "conclusion": "\u7efc\u5408\u72b6\u6001\u8868\u793a\u5728\u5b66\u4e60\u578b\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u53ef\u663e\u8457\u63d0\u5347\u63a7\u5236\u5668\u6027\u80fd\u3002"}}
{"id": "2509.21143", "categories": ["cs.RO", "cs.CL", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21143", "abs": "https://arxiv.org/abs/2509.21143", "authors": ["Junfeng Yan", "Biao Wu", "Meng Fang", "Ling Chen"], "title": "Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems", "comment": "10 pages, 5 figures,", "summary": "Multimodal agents have demonstrated strong performance in general GUI\ninteractions, but their application in automotive systems has been largely\nunexplored. In-vehicle GUIs present distinct challenges: drivers' limited\nattention, strict safety requirements, and complex location-based interaction\npatterns. To address these challenges, we introduce Automotive-ENV, the first\nhigh-fidelity benchmark and interaction environment tailored for vehicle GUIs.\nThis platform defines 185 parameterized tasks spanning explicit control,\nimplicit intent understanding, and safety-aware tasks, and provides structured\nmultimodal observations with precise programmatic checks for reproducible\nevaluation. Building on this benchmark, we propose ASURADA, a geo-aware\nmultimodal agent that integrates GPS-informed context to dynamically adjust\nactions based on location, environmental conditions, and regional driving\nnorms. Experiments show that geo-aware information significantly improves\nsuccess on safety-aware tasks, highlighting the importance of location-based\ncontext in automotive environments. We will release Automotive-ENV, complete\nwith all tasks and benchmarking tools, to further the development of safe and\nadaptive in-vehicle agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u8f66\u8f7dGUI\u7684\u9ad8\u4fdd\u771f\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0Automotive-ENV\u548c\u5730\u7406\u611f\u77e5\u591a\u6a21\u6001\u4ee3\u7406ASURADA\uff0c\u901a\u8fc7GPS\u4e0a\u4e0b\u6587\u96c6\u6210\u63d0\u5347\u8f66\u8f7d\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u4ee3\u7406\u5728\u901a\u7528GUI\u4ea4\u4e92\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8f66\u8f7d\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u8f66\u8f7dGUI\u9762\u4e34\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u6709\u9650\u3001\u4e25\u683c\u5b89\u5168\u8981\u6c42\u548c\u590d\u6742\u57fa\u4e8e\u4f4d\u7f6e\u7684\u4ea4\u4e92\u6a21\u5f0f\u7b49\u72ec\u7279\u6311\u6218\u3002", "method": "\u5f00\u53d1Automotive-ENV\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b185\u4e2a\u53c2\u6570\u5316\u4efb\u52a1\uff0c\u6db5\u76d6\u663e\u5f0f\u63a7\u5236\u3001\u9690\u5f0f\u610f\u56fe\u7406\u89e3\u548c\u5b89\u5168\u611f\u77e5\u4efb\u52a1\u3002\u63d0\u51faASURADA\u5730\u7406\u611f\u77e5\u591a\u6a21\u6001\u4ee3\u7406\uff0c\u96c6\u6210GPS\u4fe1\u606f\u6839\u636e\u4f4d\u7f6e\u3001\u73af\u5883\u6761\u4ef6\u548c\u533a\u57df\u9a7e\u9a76\u89c4\u8303\u52a8\u6001\u8c03\u6574\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5730\u7406\u611f\u77e5\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u611f\u77e5\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u7a81\u51fa\u4e86\u57fa\u4e8e\u4f4d\u7f6e\u7684\u4e0a\u4e0b\u6587\u5728\u8f66\u8f7d\u73af\u5883\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Automotive-ENV\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u548cASURADA\u4ee3\u7406\u4e3a\u5f00\u53d1\u5b89\u5168\u3001\u81ea\u9002\u5e94\u7684\u8f66\u8f7d\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u4f4d\u7f6e\u611f\u77e5\u5728\u8f66\u8f7dGUI\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.21145", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21145", "abs": "https://arxiv.org/abs/2509.21145", "authors": ["Md Faizal Karim", "Vignesh Vembar", "Keshab Patra", "Gaurav Singh", "K Madhava Krishna"], "title": "DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps", "comment": null, "summary": "Reliable dual-arm grasping is essential for manipulating large and complex\nobjects but remains a challenging problem due to stability, collision, and\ngeneralization requirements. Prior methods typically decompose the task into\ntwo independent grasp proposals, relying on region priors or heuristics that\nlimit generalization and provide no principled guarantee of stability. We\npropose DAGDiff, an end-to-end framework that directly denoises to grasp pairs\nin the SE(3) x SE(3) space. Our key insight is that stability and collision can\nbe enforced more effectively by guiding the diffusion process with classifier\nsignals, rather than relying on explicit region detection or object priors. To\nthis end, DAGDiff integrates geometry-, stability-, and collision-aware\nguidance terms that steer the generative process toward grasps that are\nphysically valid and force-closure compliant. We comprehensively evaluate\nDAGDiff through analytical force-closure checks, collision analysis, and\nlarge-scale physics-based simulations, showing consistent improvements over\nprevious work on these metrics. Finally, we demonstrate that our framework\ngenerates dual-arm grasps directly on real-world point clouds of previously\nunseen objects, which are executed on a heterogeneous dual-arm setup where two\nmanipulators reliably grasp and lift them.", "AI": {"tldr": "DAGDiff\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u53cc\u673a\u68b0\u81c2\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5728SE(3)\u00d7SE(3)\u7a7a\u95f4\u4e2d\u76f4\u63a5\u751f\u6210\u6293\u53d6\u5bf9\uff0c\u5229\u7528\u5206\u7c7b\u5668\u4fe1\u53f7\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u907f\u514d\u78b0\u649e\u3002", "motivation": "\u53ef\u9760\u7684\u53cc\u673a\u68b0\u81c2\u6293\u53d6\u5bf9\u4e8e\u64cd\u4f5c\u5927\u578b\u590d\u6742\u7269\u4f53\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u6293\u53d6\u63d0\u6848\uff0c\u4f9d\u8d56\u533a\u57df\u5148\u9a8c\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u4e14\u65e0\u6cd5\u63d0\u4f9b\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faDAGDiff\u6846\u67b6\uff0c\u76f4\u63a5\u5728SE(3)\u00d7SE(3)\u7a7a\u95f4\u4e2d\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u53bb\u566a\u751f\u6210\u6293\u53d6\u5bf9\uff0c\u96c6\u6210\u51e0\u4f55\u3001\u7a33\u5b9a\u6027\u548c\u78b0\u649e\u611f\u77e5\u7684\u5f15\u5bfc\u9879\uff0c\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u4ea7\u751f\u7269\u7406\u6709\u6548\u4e14\u6ee1\u8db3\u529b\u95ed\u5408\u8981\u6c42\u7684\u6293\u53d6\u3002", "result": "\u901a\u8fc7\u5206\u6790\u529b\u95ed\u5408\u68c0\u67e5\u3001\u78b0\u649e\u5206\u6790\u548c\u5927\u89c4\u6a21\u7269\u7406\u4eff\u771f\u5168\u9762\u8bc4\u4f30DAGDiff\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u70b9\u4e91\u4e0a\u76f4\u63a5\u751f\u6210\u53cc\u673a\u68b0\u81c2\u6293\u53d6\uff0c\u5728\u5f02\u6784\u53cc\u673a\u68b0\u81c2\u7cfb\u7edf\u4e0a\u6210\u529f\u6267\u884c\u3002", "conclusion": "DAGDiff\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u673a\u68b0\u81c2\u6293\u53d6\u7684\u7a33\u5b9a\u6027\u3001\u78b0\u649e\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21189", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21189", "abs": "https://arxiv.org/abs/2509.21189", "authors": ["Bhargav Chandaka", "Gloria X. Wang", "Haozhe Chen", "Henry Che", "Albert J. Zhai", "Shenlong Wang"], "title": "Human-like Navigation in a World Built for Humans", "comment": "CoRL 2025. Project website: https://reasonnav.github.io/", "summary": "When navigating in a man-made environment they haven't visited before--like\nan office building--humans employ behaviors such as reading signs and asking\nothers for directions. These behaviors help humans reach their destinations\nefficiently by reducing the need to search through large areas. Existing robot\nnavigation systems lack the ability to execute such behaviors and are thus\nhighly inefficient at navigating within large environments. We present\nReasonNav, a modular navigation system which integrates these human-like\nnavigation skills by leveraging the reasoning capabilities of a vision-language\nmodel (VLM). We design compact input and output abstractions based on\nnavigation landmarks, allowing the VLM to focus on language understanding and\nreasoning. We evaluate ReasonNav on real and simulated navigation tasks and\nshow that the agent successfully employs higher-order reasoning to navigate\nefficiently in large, complex buildings.", "AI": {"tldr": "ReasonNav\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u5bfc\u822a\u7cfb\u7edf\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u96c6\u6210\u4eba\u7c7b\u5bfc\u822a\u6280\u80fd\uff0c\u5728\u5927\u578b\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u7f3a\u4e4f\u4eba\u7c7b\u5bfc\u822a\u884c\u4e3a\uff08\u5982\u9605\u8bfb\u6807\u5fd7\u3001\u8be2\u95ee\u65b9\u5411\uff09\uff0c\u5bfc\u81f4\u5728\u5927\u578b\u73af\u5883\u4e2d\u5bfc\u822a\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5bfc\u822a\u5730\u6807\u7684\u7d27\u51d1\u8f93\u5165\u8f93\u51fa\u62bd\u8c61\uff0c\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e13\u6ce8\u4e8e\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\uff0c\u96c6\u6210\u4eba\u7c7b\u5bfc\u822a\u6280\u80fd\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u5bfc\u822a\u4efb\u52a1\u4e2d\u8bc4\u4f30\u663e\u793a\uff0c\u667a\u80fd\u4f53\u6210\u529f\u8fd0\u7528\u9ad8\u9636\u63a8\u7406\u5728\u5927\u578b\u590d\u6742\u5efa\u7b51\u4e2d\u9ad8\u6548\u5bfc\u822a\u3002", "conclusion": "ReasonNav\u901a\u8fc7\u96c6\u6210\u4eba\u7c7b\u5bfc\u822a\u884c\u4e3a\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u5927\u578b\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6548\u7387\u3002"}}
{"id": "2509.21210", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.21210", "abs": "https://arxiv.org/abs/2509.21210", "authors": ["Ali Kafili Gavgani", "Amin Talaeizadeh", "Aria Alasty", "Hossein Nejat Pishkenari", "Esmaeil Najafi"], "title": "Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis", "comment": null, "summary": "Conventional multi-rotors are under-actuated systems, hindering them from\nindependently controlling attitude from position. In this study, we present\nseveral distinct configurations that incorporate additional control inputs for\nmanipulating the angles of the propeller axes. This addresses the mentioned\nlimitations, making the systems \"omniorientational\". We comprehensively derived\ndetailed dynamic models for all introduced configurations and validated by a\nmethodology using Simscape Multibody simulations. Two controllers are designed:\na sliding mode controller for robust handling of disturbances and a novel\nPID-based controller with gravity compensation integrating linear and\nnon-linear allocators, designed for computational efficiency. A custom control\nallocation strategy is implemented to manage the input-non-affine nature of\nthese systems, seeking to maximize battery life by minimizing the \"Power\nConsumption Factor\" defined in this study. Moreover, the controllers\neffectively managed harsh disturbances and uncertainties. Simulations compare\nand analyze the proposed configurations and controllers, majorly considering\ntheir power consumption. Furthermore, we conduct a qualitative comparison to\nevaluate the impact of different types of uncertainties on the control system,\nhighlighting areas for potential model or hardware improvements. The analysis\nin this study provides a roadmap for future researchers to design\nomniorientational drones based on their design objectives, offering practical\ninsights into configuration selection and controller design. This research\naligns with the project SAC-1, one of the objectives of Sharif AgRoLab.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u578b\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u914d\u7f6e\uff0c\u901a\u8fc7\u589e\u52a0\u87ba\u65cb\u6868\u8f74\u89d2\u5ea6\u63a7\u5236\u8f93\u5165\u5b9e\u73b0\u5168\u5411\u59ff\u6001\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u65cb\u7ffc\u6b20\u9a71\u52a8\u7cfb\u7edf\u7684\u9650\u5236\u3002\u8bbe\u8ba1\u4e86\u6ed1\u6a21\u63a7\u5236\u5668\u548c\u65b0\u578bPID\u63a7\u5236\u5668\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9a\u5236\u63a7\u5236\u5206\u914d\u7b56\u7565\u6765\u4f18\u5316\u529f\u8017\u3002", "motivation": "\u4f20\u7edf\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u662f\u6b20\u9a71\u52a8\u7cfb\u7edf\uff0c\u65e0\u6cd5\u72ec\u7acb\u63a7\u5236\u59ff\u6001\u548c\u4f4d\u7f6e\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u589e\u52a0\u87ba\u65cb\u6868\u8f74\u89d2\u5ea6\u63a7\u5236\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u5168\u5411\u63a7\u5236\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u79cd\u914d\u7f6e\u65b9\u6848\uff0c\u8be6\u7ec6\u63a8\u5bfc\u4e86\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f7f\u7528Simscape Multibody\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002\u8bbe\u8ba1\u4e86\u6ed1\u6a21\u63a7\u5236\u5668\u548c\u65b0\u578bPID\u63a7\u5236\u5668\uff08\u542b\u91cd\u529b\u8865\u507f\u548c\u7ebf\u6027/\u975e\u7ebf\u6027\u5206\u914d\u5668\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9a\u5236\u63a7\u5236\u5206\u914d\u7b56\u7565\u6765\u7ba1\u7406\u8f93\u5165\u975e\u4eff\u5c04\u7279\u6027\u3002", "result": "\u63a7\u5236\u5668\u80fd\u6709\u6548\u5904\u7406\u5267\u70c8\u6270\u52a8\u548c\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u4eff\u771f\u6bd4\u8f83\u4e86\u4e0d\u540c\u914d\u7f6e\u548c\u63a7\u5236\u5668\u7684\u529f\u8017\u6027\u80fd\uff0c\u5e76\u8fdb\u884c\u4e86\u4e0d\u786e\u5b9a\u6027\u5bf9\u63a7\u5236\u7cfb\u7edf\u5f71\u54cd\u7684\u5b9a\u6027\u6bd4\u8f83\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u672a\u6765\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u5168\u5411\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u4e3a\u914d\u7f6e\u9009\u62e9\u548c\u63a7\u5236\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u662fSharif AgRoLab\u9879\u76eeSAC-1\u7684\u4e00\u90e8\u5206\u3002"}}
{"id": "2509.21231", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21231", "abs": "https://arxiv.org/abs/2509.21231", "authors": ["Jaehwi Jang", "Zhuoheng Wang", "Ziyi Zhou", "Feiyang Wu", "Ye Zhao"], "title": "SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation", "comment": "9 pages, 5 figures", "summary": "Arm end-effector stabilization is essential for humanoid loco-manipulation\ntasks, yet it remains challenging due to the high degrees of freedom and\ninherent dynamic instability of bipedal robot structures. Previous model-based\ncontrollers achieve precise end-effector control but rely on precise dynamics\nmodeling and estimation, which often struggle to capture real-world factors\n(e.g., friction and backlash) and thus degrade in practice. On the other hand,\nlearning-based methods can better mitigate these factors via exploration and\ndomain randomization, and have shown potential in real-world use. However, they\noften overfit to training conditions, requiring retraining with the entire\nbody, and still struggle to adapt to unseen scenarios. To address these\nchallenges, we propose a novel stable end-effector control (SEEC) framework\nwith model-enhanced residual learning that learns to achieve precise and robust\nend-effector compensation for lower-body induced disturbances through\nmodel-guided reinforcement learning (RL) with a perturbation generator. This\ndesign allows the upper-body policy to achieve accurate end-effector\nstabilization as well as adapt to unseen locomotion controllers with no\nadditional training. We validate our framework in different simulators and\ntransfer trained policies to the Booster T1 humanoid robot. Experiments\ndemonstrate that our method consistently outperforms baselines and robustly\nhandles diverse and demanding loco-manipulation tasks.", "AI": {"tldr": "\u63d0\u51faSEEC\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u589e\u5f3a\u7684\u6b8b\u5dee\u5b66\u4e60\u548c\u6270\u52a8\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u7cbe\u786e\u7a33\u5b9a\u63a7\u5236\uff0c\u80fd\u591f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7a33\u5b9a\u63a7\u5236\u7684\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u52a8\u529b\u5b66\u5efa\u6a21\u800c\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u56e0\u7d20\uff0c\u5b66\u4e60\u7c7b\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u8bad\u7ec3\u6761\u4ef6\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u573a\u666f\u3002", "method": "\u91c7\u7528\u6a21\u578b\u589e\u5f3a\u7684\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u578b\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u6270\u52a8\u751f\u6210\u5668\uff0c\u5b66\u4e60\u5bf9\u4e0b\u534a\u8eab\u5f15\u8d77\u6270\u52a8\u7684\u7cbe\u786e\u9c81\u68d2\u8865\u507f\u3002", "result": "\u5728\u4e0d\u540c\u4eff\u771f\u5668\u4e2d\u9a8c\u8bc1\uff0c\u5e76\u5c06\u8bad\u7ec3\u7b56\u7565\u8f6c\u79fb\u5230Booster T1\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u80fd\u9c81\u68d2\u5904\u7406\u591a\u6837\u5316\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "SEEC\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u9c81\u68d2\u7684\u672b\u7aef\u6267\u884c\u5668\u7a33\u5b9a\u63a7\u5236\uff0c\u4e14\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u3002"}}
{"id": "2509.21242", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21242", "abs": "https://arxiv.org/abs/2509.21242", "authors": ["Yutong Li", "Jieyi Zhang", "Wenqiang Xu", "Tutian Tang", "Cewu Lu"], "title": "FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration", "comment": "Presented at IROS 2025, details are available at\n  https://fsglove.robotflow.ai", "summary": "Accurate hand motion capture (MoCap) is vital for applications in robotics,\nvirtual reality, and biomechanics, yet existing systems face limitations in\ncapturing high-degree-of-freedom (DoF) joint kinematics and personalized hand\nshape. Commercial gloves offer up to 21 DoFs, which are insufficient for\ncomplex manipulations while neglecting shape variations that are critical for\ncontact-rich tasks. We present FSGlove, an inertial-based system that\nsimultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes\nvia DiffHCal, a novel calibration method. Each finger joint and the dorsum are\nequipped with IMUs, enabling high-resolution motion sensing. DiffHCal\nintegrates with the parametric MANO model through differentiable optimization,\nresolving joint kinematics, shape parameters, and sensor misalignment during a\nsingle streamlined calibration. The system achieves state-of-the-art accuracy,\nwith joint angle errors of less than 2.7 degree, and outperforms commercial\nalternatives in shape reconstruction and contact fidelity. FSGlove's\nopen-source hardware and software design ensures compatibility with current VR\nand robotics ecosystems, while its ability to capture subtle motions (e.g.,\nfingertip rubbing) bridges the gap between human dexterity and robotic\nimitation. Evaluated against Nokov optical MoCap, FSGlove advances hand\ntracking by unifying the kinematic and contact fidelity. Hardware design,\nsoftware, and more results are available at:\nhttps://sites.google.com/view/fsglove.", "AI": {"tldr": "FSGlove\u662f\u4e00\u4e2a\u57fa\u4e8e\u60ef\u6027\u7684\u624b\u90e8\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf\uff0c\u80fd\u540c\u65f6\u8ddf\u8e2a48\u4e2a\u81ea\u7531\u5ea6\u5e76\u91cd\u5efa\u4e2a\u6027\u5316\u624b\u90e8\u5f62\u72b6\uff0c\u901a\u8fc7DiffHCal\u6821\u51c6\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8fd0\u52a8\u6355\u6349\u548c\u5f62\u72b6\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u5546\u4e1a\u624b\u5957\u6700\u591a\u53ea\u80fd\u63d0\u4f9b21\u4e2a\u81ea\u7531\u5ea6\uff0c\u4e0d\u8db3\u4ee5\u6355\u6349\u590d\u6742\u7684\u624b\u90e8\u52a8\u4f5c\uff0c\u4e14\u5ffd\u7565\u4e86\u5bf9\u624b\u90e8\u63a5\u89e6\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u7684\u5f62\u72b6\u53d8\u5316\u3002", "method": "\u7cfb\u7edf\u5728\u6bcf\u4e2a\u624b\u6307\u5173\u8282\u548c\u624b\u80cc\u914d\u5907IMU\u4f20\u611f\u5668\uff0c\u901a\u8fc7DiffHCal\u65b9\u6cd5\u4e0e\u53c2\u6570\u5316MANO\u6a21\u578b\u96c6\u6210\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u5728\u4e00\u6b21\u6d41\u7ebf\u578b\u6821\u51c6\u4e2d\u89e3\u51b3\u5173\u8282\u8fd0\u52a8\u5b66\u3001\u5f62\u72b6\u53c2\u6570\u548c\u4f20\u611f\u5668\u9519\u4f4d\u95ee\u9898\u3002", "result": "\u7cfb\u7edf\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u5173\u8282\u89d2\u5ea6\u8bef\u5dee\u5c0f\u4e8e2.7\u5ea6\uff0c\u5728\u5f62\u72b6\u91cd\u5efa\u548c\u63a5\u89e6\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u5546\u4e1a\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u6355\u6349\u7ec6\u5fae\u52a8\u4f5c\u5982\u6307\u5c16\u6469\u64e6\u3002", "conclusion": "FSGlove\u901a\u8fc7\u7edf\u4e00\u8fd0\u52a8\u5b66\u548c\u63a5\u89e6\u4fdd\u771f\u5ea6\u63a8\u8fdb\u4e86\u624b\u90e8\u8ddf\u8e2a\u6280\u672f\uff0c\u5176\u5f00\u6e90\u786c\u4ef6\u548c\u8f6f\u4ef6\u8bbe\u8ba1\u786e\u4fdd\u4e0e\u5f53\u524dVR\u548c\u673a\u5668\u4eba\u751f\u6001\u7cfb\u7edf\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2509.21243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21243", "abs": "https://arxiv.org/abs/2509.21243", "authors": ["Jiyeon Koo", "Taewan Cho", "Hyunjoon Kang", "Eunseom Pyo", "Tae Gyun Oh", "Taeryang Kim", "Andrew Jaeyong Choi"], "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models demonstrate remarkable\ngeneralization in robotics but are restricted by their substantial size and\ncomputational cost, limiting real-world deployment. However, conventional\nlightweighting methods often sacrifice critical capabilities, particularly\nspatial reasoning. This creates a trade-off between efficiency and performance.\nTo address this challenge, our work reuses Register Tokens, which were\nintroduced for artifact removal in Vision Transformers but subsequently\ndiscarded. We suppose that these tokens contain essential spatial information\nand propose RetoVLA, a novel architecture that reuses them directly by\ninjecting them into the Action Expert.\n  RetoVLA maintains a lightweight structure while leveraging this repurposed\nspatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness\nthrough a series of comprehensive experiments. On our custom-built 7-DOF robot\narm, the model achieves a 17.1%p absolute improvement in success rates for\ncomplex manipulation tasks. Our results confirm that reusing Register Tokens\ndirectly enhances spatial reasoning, demonstrating that what was previously\ndiscarded as an artifact is in fact a valuable, unexplored resource for robotic\nintelligence. A video demonstration is available at:\nhttps://youtu.be/2CseBR-snZg", "AI": {"tldr": "RetoVLA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u7528Vision Transformer\u4e2d\u88ab\u4e22\u5f03\u7684Register Tokens\u6765\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ed3\u6784\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u867d\u7136\u6cdb\u5316\u80fd\u529b\u5f3a\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u8f7b\u91cf\u5316\u65b9\u6cd5\u4f1a\u727a\u7272\u5173\u952e\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u6743\u8861\u3002", "method": "\u63d0\u51faRetoVA\u67b6\u6784\uff0c\u91cd\u7528Vision Transformer\u4e2d\u4e3a\u53bb\u9664\u4f2a\u5f71\u800c\u5f15\u5165\u4f46\u968f\u540e\u88ab\u4e22\u5f03\u7684Register Tokens\uff0c\u5c06\u8fd9\u4e9b\u5305\u542b\u91cd\u8981\u7a7a\u95f4\u4fe1\u606f\u7684tokens\u76f4\u63a5\u6ce8\u5165\u5230Action Expert\u4e2d\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0a\uff0c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u7edd\u5bf9\u63d0\u5347\u4e8617.1\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u91cd\u7528Register Tokens\u80fd\u6709\u6548\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u3002", "conclusion": "\u88ab\u4e22\u5f03\u7684Register Tokens\u5b9e\u9645\u4e0a\u662f\u673a\u5668\u4eba\u667a\u80fd\u4e2d\u5b9d\u8d35\u7684\u672a\u5f00\u53d1\u8d44\u6e90\uff0c\u76f4\u63a5\u91cd\u7528\u5b83\u4eec\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6a21\u578b\u8f7b\u91cf\u5316\u3002"}}
{"id": "2509.21256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21256", "abs": "https://arxiv.org/abs/2509.21256", "authors": ["Huayi Zhou", "Kui Jia"], "title": "BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives", "comment": "under review", "summary": "Non-prehensile manipulation, encompassing ungraspable actions such as\npushing, poking, and pivoting, represents a critical yet underexplored domain\nin robotics due to its contact-rich and analytically intractable nature. In\nthis work, we revisit this problem from two novel perspectives. First, we move\nbeyond the usual single-arm setup and the strong assumption of favorable\nexternal dexterity such as walls, ramps, or edges. Instead, we advocate a\ngeneralizable dual-arm configuration and establish a suite of Bimanual\nNon-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the\nprevailing RL-based paradigm and propose a three-stage, RL-free framework to\nlearn non-prehensile skills. Specifically, we begin by extracting bimanual hand\nmotion trajectories from video demonstrations. Due to visual inaccuracies and\nmorphological gaps, these coarse trajectories are difficult to transfer\ndirectly to robotic end-effectors. To address this, we propose a geometry-aware\npost-optimization algorithm that refines raw motions into executable\nmanipulation primitives that conform to specific motion patterns. Beyond\ninstance-level reproduction, we further enable category-level generalization by\nparameterizing the learned primitives with object-relevant geometric\nattributes, particularly size, resulting in adaptable and general parameterized\nmanipulation primitives. We validate BiNoMaP across a range of representative\nbimanual tasks and diverse object categories, demonstrating its effectiveness,\nefficiency, versatility, and superior generalization capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86BiNoMaP\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u624b\u673a\u5668\u4eba\u914d\u7f6e\u548c\u51e0\u4f55\u611f\u77e5\u4f18\u5316\u65b9\u6cd5\uff0c\u4ece\u89c6\u9891\u6f14\u793a\u4e2d\u5b66\u4e60\u975e\u6293\u53d6\u64cd\u4f5c\u6280\u80fd\uff0c\u5b9e\u73b0\u7c7b\u522b\u7ea7\u6cdb\u5316\u3002", "motivation": "\u975e\u6293\u53d6\u64cd\u4f5c\u5728\u673a\u5668\u4eba\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u81c2\u914d\u7f6e\u6216\u5916\u90e8\u8f85\u52a9\u7ed3\u6784\uff0c\u4e14\u4e3b\u8981\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4e09\u9636\u6bb5\u65e0\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u4ece\u89c6\u9891\u63d0\u53d6\u53cc\u624b\u8fd0\u52a8\u8f68\u8ff9\uff0c\u51e0\u4f55\u611f\u77e5\u540e\u4f18\u5316\u751f\u6210\u53ef\u6267\u884c\u64cd\u4f5c\u539f\u8bed\uff0c\u53c2\u6570\u5316\u539f\u8bed\u5b9e\u73b0\u7c7b\u522b\u7ea7\u6cdb\u5316\u3002", "result": "\u5728\u591a\u79cd\u4ee3\u8868\u6027\u53cc\u4eba\u4efb\u52a1\u548c\u4e0d\u540c\u7269\u4f53\u7c7b\u522b\u4e0a\u9a8c\u8bc1\u4e86BiNoMaP\u7684\u6709\u6548\u6027\u3001\u6548\u7387\u3001\u591a\u529f\u80fd\u6027\u548c\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BiNoMaP\u4e3a\u53cc\u624b\u673a\u5668\u4eba\u975e\u6293\u53d6\u64cd\u4f5c\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5f3a\u5316\u5b66\u4e60\u5373\u53ef\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u6280\u80fd\u5e76\u5b9e\u73b0\u6cdb\u5316\u3002"}}
{"id": "2509.21264", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21264", "abs": "https://arxiv.org/abs/2509.21264", "authors": ["Babak Salamat", "Dominik Mattern", "Sebastian-Sven Olzem", "Gerhard Elsbacher", "Christian Seidel", "Andrea M. Tonello"], "title": "\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)", "comment": null, "summary": "We propose $\\text{GMP}^{3}$, a multiphase global path planning framework that\ngenerates dynamically feasible three-dimensional trajectories for unmanned\naerial vehicles (UAVs) operating in cluttered environments. The framework\nextends traditional path planning from Euclidean position spaces to the Lie\ngroup $\\mathrm{SE}(3)$, allowing joint learning of translational motion and\nrotational dynamics. A modified Bellman-based operator is introduced to support\nreinforcement learning (RL) policy updates while leveraging prior trajectory\ninformation for improved convergence. $\\text{GMP}^{3}$ is designed as a\ndistributed framework in which agents influence each other and share policy\ninformation along the trajectory: each agent refines its assigned segment and\nshares with its neighbors via a consensus-based scheme, enabling cooperative\npolicy updates and convergence toward a path shaped globally even under\nkinematic constraints. We also propose DroneManager, a modular ground control\nsoftware that interfaces the planner with real UAV platforms via the MAVLink\nprotocol, supporting real-time deployment and feedback. Simulation studies and\nindoor flight experiments validate the effectiveness of the proposed method in\nconstrained 3D environments, demonstrating reliable obstacle avoidance and\nsmooth, feasible trajectories across both position and orientation. The\nopen-source implementation is available at\nhttps://github.com/Domattee/DroneManager", "AI": {"tldr": "GMP\u00b3\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u751f\u6210\u52a8\u6001\u53ef\u884c\u7684\u4e09\u7ef4\u8f68\u8ff9\u3002\u8be5\u6846\u67b6\u5c06\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u4ece\u6b27\u51e0\u91cc\u5f97\u4f4d\u7f6e\u7a7a\u95f4\u6269\u5c55\u5230SE(3)\u674e\u7fa4\uff0c\u8054\u5408\u5b66\u4e60\u5e73\u79fb\u8fd0\u52a8\u548c\u65cb\u8f6c\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u4ee3\u7406\u534f\u4f5c\u5b9e\u73b0\u5168\u5c40\u8def\u5f84\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f4d\u7f6e\u7a7a\u95f4\uff0c\u96be\u4ee5\u5904\u7406\u65e0\u4eba\u673a\u5728\u590d\u6742\u4e09\u7ef4\u73af\u5883\u4e2d\u7684\u52a8\u6001\u7ea6\u675f\u548c\u59ff\u6001\u63a7\u5236\u9700\u6c42\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8003\u8651\u5e73\u79fb\u548c\u65cb\u8f6c\u52a8\u529b\u5b66\uff0c\u5e76\u652f\u6301\u591a\u4ee3\u7406\u534f\u4f5c\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u674e\u7fa4SE(3)\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5f15\u5165\u6539\u8fdb\u7684Bellman\u7b97\u5b50\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u66f4\u65b0\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\u4f7f\u4ee3\u7406\u901a\u8fc7\u5171\u8bc6\u673a\u5236\u5171\u4eab\u7b56\u7565\u4fe1\u606f\uff0c\u6bcf\u4e2a\u4ee3\u7406\u4f18\u5316\u5176\u5206\u914d\u6bb5\u5e76\u4e0e\u90bb\u5c45\u534f\u4f5c\u3002", "result": "\u4eff\u771f\u548c\u5ba4\u5185\u98de\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u53d7\u9650\u4e09\u7ef4\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u969c\u788d\u7269\u907f\u8ba9\u548c\u4f4d\u7f6e\u3001\u59ff\u6001\u7684\u5e73\u6ed1\u53ef\u884c\u8f68\u8ff9\u3002", "conclusion": "GMP\u00b3\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e09\u7ef4\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u674e\u7fa4\u8868\u793a\u548c\u5206\u5e03\u5f0f\u534f\u4f5c\u5b9e\u73b0\u4e86\u52a8\u6001\u53ef\u884c\u7684\u8def\u5f84\u751f\u6210\uff0c\u5e76\u901a\u8fc7DroneManager\u8f6f\u4ef6\u652f\u6301\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2509.21281", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21281", "abs": "https://arxiv.org/abs/2509.21281", "authors": ["Luis Augenstein", "No\u00e9mie Jaquier", "Tamim Asfour", "Leonel Rozo"], "title": "Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds", "comment": "8 pages, 6 figures, 1 table", "summary": "Human-like motion generation for robots often draws inspiration from\nbiomechanical studies, which often categorize complex human motions into\nhierarchical taxonomies. While these taxonomies provide rich structural\ninformation about how movements relate to one another, this information is\nfrequently overlooked in motion generation models, leading to a disconnect\nbetween the generated motions and their underlying hierarchical structure. This\npaper introduces the \\ac{gphdm}, a novel approach that learns latent\nrepresentations preserving both the hierarchical structure of motions and their\ntemporal dynamics to ensure physical consistency. Our model achieves this by\nextending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to\nthe hyperbolic manifold and integrating it with taxonomy-aware inductive\nbiases. Building on this geometry- and taxonomy-aware frameworks, we propose\nthree novel mechanisms for generating motions that are both\ntaxonomically-structured and physically-consistent: two probabilistic recursive\napproaches and a method based on pullback-metric geodesics. Experiments on\ngenerating realistic motion sequences on the hand grasping taxonomy show that\nthe proposed GPHDM faithfully encodes the underlying taxonomy and temporal\ndynamics, and generates novel physically-consistent trajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GPHDM\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u9ad8\u65af\u8fc7\u7a0b\u52a8\u529b\u5b66\u6a21\u578b\u6269\u5c55\u5230\u53cc\u66f2\u6d41\u5f62\u5e76\u6574\u5408\u5206\u7c7b\u5b66\u611f\u77e5\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u6765\u751f\u6210\u65e2\u4fdd\u6301\u8fd0\u52a8\u5c42\u6b21\u7ed3\u6784\u53c8\u5177\u6709\u7269\u7406\u4e00\u81f4\u6027\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u7ecf\u5e38\u5ffd\u89c6\u751f\u7269\u529b\u5b66\u7814\u7a76\u4e2d\u590d\u6742\u7684\u8fd0\u52a8\u5c42\u6b21\u5206\u7c7b\u5b66\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8fd0\u52a8\u4e0e\u5e95\u5c42\u5c42\u6b21\u7ed3\u6784\u8131\u8282\u3002", "method": "\u6269\u5c55GPDM\u7684\u52a8\u6001\u5148\u9a8c\u5230\u53cc\u66f2\u6d41\u5f62\uff0c\u6574\u5408\u5206\u7c7b\u5b66\u611f\u77e5\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u63d0\u51fa\u4e09\u79cd\u65b0\u673a\u5236\uff1a\u4e24\u79cd\u6982\u7387\u9012\u5f52\u65b9\u6cd5\u548c\u57fa\u4e8e\u62c9\u56de\u5ea6\u91cf\u6d4b\u5730\u7ebf\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u624b\u90e8\u6293\u53d6\u5206\u7c7b\u5b66\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPHDM\u80fd\u591f\u5fe0\u5b9e\u7f16\u7801\u5e95\u5c42\u5206\u7c7b\u5b66\u548c\u65f6\u5e8f\u52a8\u6001\uff0c\u751f\u6210\u65b0\u9896\u7684\u7269\u7406\u4e00\u81f4\u8f68\u8ff9\u3002", "conclusion": "GPHDM\u6210\u529f\u5730\u5c06\u8fd0\u52a8\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u5e8f\u52a8\u6001\u7ed3\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u5206\u7c7b\u5b66\u7ed3\u6784\u5316\u548c\u7269\u7406\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
