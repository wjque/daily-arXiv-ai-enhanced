<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 49]
- [cs.CG](#cs.CG) [Total: 1]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation](https://arxiv.org/abs/2509.20486)
*Sven Ochs,Philip Schörner,Marc René Zofka,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 提出了一种融合语义相机数据和LiDAR分割的方法，通过将LiDAR点投影到相机语义分割空间来提升LiDAR定位的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: LiDAR数据的语义分割在处理不同传感器类型和配置时面临挑战，但语义信息可以显著提升LiDAR定位技术的准确性和鲁棒性。

Method: 使用CoCar NextGen平台的多传感器配置，结合Depth-Anything网络进行相机图像分割和自适应分割网络进行LiDAR分割，将LiDAR点投影到相机语义分割空间。

Result: 在德国卡尔斯鲁厄55公里的多样化环境中进行验证，包括城市区域、多车道道路和乡村高速公路，证明了该方法的有效性。

Conclusion: 这种多模态方法为复杂现实环境中更可靠和精确的自主导航系统铺平了道路。

Abstract: Semantic segmentation of LiDAR data presents considerable challenges,
particularly when dealing with diverse sensor types and configurations.
However, incorporating semantic information can significantly enhance the
accuracy and robustness of LiDAR-based localization techniques for autonomous
mobile systems. We propose an approach that integrates semantic camera data
with LiDAR segmentation to address this challenge. By projecting LiDAR points
into the semantic segmentation space of the camera, our method enhances the
precision and reliability of the LiDAR-based localization pipeline.
  For validation, we utilize the CoCar NextGen platform from the FZI Research
Center for Information Technology, which offers diverse sensor modalities and
configurations. The sensor setup of CoCar NextGen enables a thorough analysis
of different sensor types. Our evaluation leverages the state-of-the-art
Depth-Anything network for camera image segmentation and an adaptive
segmentation network for LiDAR segmentation. To establish a reliable ground
truth for LiDAR-based localization, we make us of a Global Navigation Satellite
System (GNSS) solution with Real-Time Kinematic corrections (RTK).
Additionally, we conduct an extensive 55 km drive through the city of
Karlsruhe, Germany, covering a variety of environments, including urban areas,
multi-lane roads, and rural highways. This multimodal approach paves the way
for more reliable and precise autonomous navigation systems, particularly in
complex real-world environments.

</details>


### [2] [Revisiting Formal Methods for Autonomous Robots: A Structured Survey](https://arxiv.org/abs/2509.20488)
*Atef Azaiez,David A. Anisi,Marie Farrell,Matt Luckcuck*

Main category: cs.RO

TL;DR: 本文通过结构化文献综述方法，调查了形式化方法在机器人自主系统中的应用，包括方法分类、形式化工具使用情况，以及该领域随时间的发展趋势。


<details>
  <summary>Details</summary>
Motivation: 补充现有关于形式化方法在机器人自主系统应用领域的调查，分析该研究领域的成熟度和发展趋势。

Method: 采用结构化文献综述方法，包括数据库选择、搜索字符串设计、筛选过滤和协作评审已识别论文。

Result: 调查显示某些趋势与先前调查一致，同时识别出新趋势，包括形式化综合方法和概率验证技术的显著增加。

Conclusion: 形式化方法在机器人自主系统领域的应用正在成熟，出现了新的研究方向和技术趋势。

Abstract: This paper presents the initial results from our structured literature review
on applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We
describe our structured survey methodology; including database selection and
associated search strings, search filters and collaborative review of
identified papers. We categorise and enumerate the FM approaches and formalisms
that have been used for specification and verification of RAS. We investigate
FM in the context of sub-symbolic AI-enabled RAS and examine the evolution of
how FM is used over time in this field. This work complements a pre-existing
survey in this area and we examine how this research area has matured over
time. Specifically, our survey demonstrates that some trends have persisted as
observed in a previous survey. Additionally, it recognized new trends that were
not considered previously including a noticeable increase in adopting Formal
Synthesis approaches as well as Probabilistic Verification Techniques.

</details>


### [3] [Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting](https://arxiv.org/abs/2509.20499)
*Boqi Li,Siyuan Li,Weiyi Wang,Anran Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: 提出了一种零样本视觉语言导航框架，结合简化的路径点预测器和多模态大语言模型，在连续环境中实现高效的导航性能。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型和机器人技术的快速发展，视觉语言导航成为具身智能体的关键任务。连续环境下的VLN特别具有挑战性，需要同时理解自然语言指令、感知环境和规划底层动作。

Method: 使用简化的路径点预测器在抽象障碍地图上生成线性可达路径点，构建动态更新的拓扑图并记录访问历史。将图和访问信息编码到提示中，使MLLM能够进行空间结构和探索历史的推理。

Result: 在R2R-CE和RxR-CE数据集上的实验显示，该方法实现了最先进的零样本性能，成功率分别达到41%和36%，优于先前的最先进方法。

Conclusion: 该框架通过结合路径点预测和MLLM推理，在连续环境中实现了高效的视觉语言导航，展示了零样本方法的强大潜力。

Abstract: With the rapid progress of foundation models and robotics, vision-language
navigation (VLN) has emerged as a key task for embodied agents with broad
practical applications. We address VLN in continuous environments, a
particularly challenging setting where an agent must jointly interpret natural
language instructions, perceive its surroundings, and plan low-level actions.
We propose a zero-shot framework that integrates a simplified yet effective
waypoint predictor with a multimodal large language model (MLLM). The predictor
operates on an abstract obstacle map, producing linearly reachable waypoints,
which are incorporated into a dynamically updated topological graph with
explicit visitation records. The graph and visitation information are encoded
into the prompt, enabling reasoning over both spatial structure and exploration
history to encourage exploration and equip MLLM with local path planning for
error correction. Extensive experiments on R2R-CE and RxR-CE show that our
method achieves state-of-the-art zero-shot performance, with success rates of
41% and 36%, respectively, outperforming prior state-of-the-art methods.

</details>


### [4] [MELEGROS: Monolithic Elephant-inspired Gripper with Optical Sensors](https://arxiv.org/abs/2509.20510)
*Petr Trunin,Diana Cafiso,Anderson Brazil Nardin,Trevor Exley,Lucia Beccai*

Main category: cs.RO

TL;DR: MELEGROS是一个受大象鼻子启发的单块软体抓取器，通过3D打印将光学传感器和气动腔集成到连续结构中，实现了多功能感知和生物启发的操作能力。


<details>
  <summary>Details</summary>
Motivation: 受大象鼻子结构和感知一体化的启发，旨在开发一种将传感作为内在制造能力的软体机器人，避免传统多材料或肌腱驱动方法中的机械不匹配问题。

Method: 使用单一软树脂材料，通过连续3D打印将6个光学波导传感器和5个气动腔集成到气动驱动的晶格结构中（12.5mm单元尺寸），实现传感、驱动和结构的无缝集成。

Result: 仅需4次迭代就完成最终原型，能够举重超过自身重量两倍，执行捏取、舀取和伸展等生物启发动作，并能轻柔抓握葡萄等易碎物品。集成光学传感器可区分触摸、弯曲和腔体变形的不同响应。

Conclusion: MELEGROS展示了软体机器人的新范式，完全嵌入的传感和连续结构固有地支持多功能、生物启发的操作能力。

Abstract: The elephant trunk exemplifies a natural gripper where structure, actuation,
and sensing are seamlessly integrated. Inspired by the distal morphology of the
African elephant trunk, we present MELEGROS, a Monolithic ELEphant-inspired
GRipper with Optical Sensors, emphasizing sensing as an intrinsic,
co-fabricated capability. Unlike multi-material or tendon-based approaches,
MELEGROS directly integrates six optical waveguide sensors and five pneumatic
chambers into a pneumatically actuated lattice structure (12.5 mm cell size)
using a single soft resin and one continuous 3D print. This eliminates
mechanical mismatches between sensors, actuators, and body, reducing model
uncertainty and enabling simulation-guided sensor design and placement. Only
four iterations were required to achieve the final prototype, which features a
continuous structure capable of elongation, compression, and bending while
decoupling tactile and proprioceptive signals. MELEGROS (132 g) lifts more than
twice its weight, performs bioinspired actions such as pinching, scooping, and
reaching, and delicately grasps fragile items like grapes. The integrated
optical sensors provide distinct responses to touch, bending, and chamber
deformation, enabling multifunctional perception. MELEGROS demonstrates a new
paradigm for soft robotics where fully embedded sensing and continuous
structures inherently support versatile, bioinspired manipulation.

</details>


### [5] [Action-Informed Estimation and Planning: Clearing Clutter on Staircases via Quadrupedal Pedipulation](https://arxiv.org/abs/2509.20516)
*Prasanna Sriganesh,Barath Satheeshkumar,Anushree Sabnis,Matthew Travers*

Main category: cs.RO

TL;DR: 提出了一种感知-动作框架，使四足机器人在密集杂乱环境中能够感知障碍物、推理可行的推挤路径并执行清理操作，特别解决了在单腿推挤过程中物体被遮挡时的状态估计问题。


<details>
  <summary>Details</summary>
Motivation: 机器人在密集杂乱环境中自主操作时，需要与障碍物进行物理交互来清理路径。但在具有挑战性的地形（如杂乱的楼梯）上进行单腿推挤时，被推挤的物体可能在操作过程中被机器人自身遮挡，这给状态估计带来了新的约束。

Method: 开发了一个紧密耦合的感知-动作框架，核心贡献是交互感知的状态估计循环，利用本体感受反馈（足部接触和腿部位置）来预测物体在遮挡期间的位移，指导感知系统在交互后重新检测物体。

Result: 在波士顿动力Spot机器人上的实验结果表明，与开环基线相比，该交互感知方法在楼梯上推挤物体时实现了更高的任务成功率和跟踪精度。

Conclusion: 通过将动作与感知闭环连接，该方法能够准确跟踪物体即使在部分推挤后，并允许机器人从物理结果中学习，例如在推挤失败时将物体重新分类为不可移动。

Abstract: For robots to operate autonomously in densely cluttered environments, they
must reason about and potentially physically interact with obstacles to clear a
path. Safely clearing a path on challenging terrain, such as a cluttered
staircase, requires controlled interaction. For example, a quadrupedal robot
that pushes objects out of the way with one leg while maintaining a stable
stance with its three other legs. However, tightly coupled physical actions,
such as one-legged pushing, create new constraints on the system that can be
difficult to predict at design time. In this work, we present a new method that
addresses one such constraint, wherein the object being pushed by a quadrupedal
robot with one of its legs becomes occluded from the robot's sensors during
manipulation. To address this challenge, we present a tightly coupled
perception-action framework that enables the robot to perceive clutter, reason
about feasible push paths, and execute the clearing maneuver. Our core
contribution is an interaction-aware state estimation loop that uses
proprioceptive feedback regarding foot contact and leg position to predict an
object's displacement during the occlusion. This prediction guides the
perception system to robustly re-detect the object after the interaction,
closing the loop between action and sensing to enable accurate tracking even
after partial pushes. Using this feedback allows the robot to learn from
physical outcomes, reclassifying an object as immovable if a push fails due to
it being too heavy. We present results of implementing our approach on a Boston
Dynamics Spot robot that show our interaction-aware approach achieves higher
task success rates and tracking accuracy in pushing objects on stairs compared
to open-loop baselines.

</details>


### [6] [Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2509.20541)
*Anujith Muraleedharan,Anamika J H*

Main category: cs.RO

TL;DR: SPARQ是一种基于学习进度的查询策略，只在学习停滞或恶化时请求人类反馈，显著减少了不必要的反馈调用，在保持高性能的同时节省了约一半的反馈预算。


<details>
  <summary>Details</summary>
Motivation: 现实世界中人类反馈成本高昂且有限，现有的人机协同强化学习方法假设反馈充足，限制了其在物理机器人部署中的实用性。

Method: 提出SPARQ策略，通过监测学习进度，仅在性能停滞或下降时请求人类反馈，避免不必要的查询。在PyBullet模拟环境中对UR5机械臂立方体抓取任务进行评估。

Result: SPARQ实现了接近完美的任务成功率，性能与始终查询策略相当，但只消耗约一半的反馈预算。相比随机查询提供更稳定高效的学习，显著优于无反馈训练。

Conclusion: 基于进度的选择性查询策略能够使HiL-RL在现实人类努力约束下更加高效和可扩展。

Abstract: Human feedback can greatly accelerate robot learning, but in real-world
settings, such feedback is costly and limited. Existing human-in-the-loop
reinforcement learning (HiL-RL) methods often assume abundant feedback,
limiting their practicality for physical robot deployment. In this work, we
introduce SPARQ, a progress-aware query policy that requests feedback only when
learning stagnates or worsens, thereby reducing unnecessary oracle calls. We
evaluate SPARQ on a simulated UR5 cube-picking task in PyBullet, comparing
against three baselines: no feedback, random querying, and always querying. Our
experiments show that SPARQ achieves near-perfect task success, matching the
performance of always querying while consuming about half the feedback budget.
It also provides more stable and efficient learning than random querying, and
significantly improves over training without feedback. These findings suggest
that selective, progress-based query strategies can make HiL-RL more efficient
and scalable for robots operating under realistic human effort constraints.

</details>


### [7] [GraspFactory: A Large Object-Centric Grasping Dataset](https://arxiv.org/abs/2509.20550)
*Srinidhi Kalgundi Srinivas,Yash Shukla,Adam Arnold,Sachin Chitta*

Main category: cs.RO

TL;DR: 本文介绍了GraspFactory数据集，包含超过1.09亿个6-DoF抓取姿态，用于训练数据密集型机器人抓取模型，并在模拟和真实环境中验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人抓取模型在有限数据集上训练后遇到新物体时的泛化问题，特别是在仓库和制造环境中物体多样性大的挑战。

Method: 构建包含1.09亿个6-DoF抓取姿态的大规模数据集GraspFactory，涵盖Franka Panda和Robotiq 2F-85夹具的14,690和33,710个物体。

Result: 在模拟和真实环境中验证了基于GraspFactory子集训练的模型的泛化能力，数据集和工具已公开提供。

Conclusion: GraspFactory数据集为训练数据密集型机器人抓取模型提供了基础，有助于解决机器人抓取在多样化环境中的泛化问题。

Abstract: Robotic grasping is a crucial task in industrial automation, where robots are
increasingly expected to handle a wide range of objects. However, a significant
challenge arises when robot grasping models trained on limited datasets
encounter novel objects. In real-world environments such as warehouses or
manufacturing plants, the diversity of objects can be vast, and grasping models
need to generalize to this diversity. Training large, generalizable
robot-grasping models requires geometrically diverse datasets. In this paper,
we introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps
collectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85
grippers (with 33,710 objects). GraspFactory is designed for training
data-intensive models, and we demonstrate the generalization capabilities of
one such model trained on a subset of GraspFactory in both simulated and
real-world settings. The dataset and tools are made available for download at
https://graspfactory.github.io/.

</details>


### [8] [Uncertainty-Aware Active Source Tracking of Marine Pollution using Unmanned Surface Vehicles](https://arxiv.org/abs/2509.20593)
*Song Ma,Richard Bucknall,Yuanchang Liu*

Main category: cs.RO

TL;DR: 提出了一种基于无人水面艇的不确定性感知海洋污染源追踪框架，通过结合高保真污染扩散模拟和信息路径规划技术，实现海洋环境中污染源的有效识别。


<details>
  <summary>Details</summary>
Motivation: 开发全自主环境监测能力，为快速响应海洋污染事件提供技术支持。

Method: 基于ROS系统实现，集成高保真海洋污染扩散模拟和信息路径规划技术，实时处理传感器数据更新概率源位置估计。

Result: 在模拟环境中进行的实验表明，该方法能够高精度定位污染源，实现可靠且高效的源定位。

Conclusion: 该框架为海洋污染源追踪提供了有效的解决方案，推动了全自主环境监测能力的发展。

Abstract: This paper proposes an uncertainty-aware marine pollution source tracking
framework for unmanned surface vehicles (USVs). By integrating high-fidelity
marine pollution dispersion simulation with informative path planning
techniques, we demonstrate effective identification of pollution sources in
marine environments. The proposed approach is implemented based on Robot
Operating System (ROS), processing real-time sensor data to update
probabilistic source location estimates. The system progressively refines the
estimation of source location while quantifying uncertainty levels in its
predictions. Experiments conducted in simulated environments with varying
source locations, flow conditions, and starting positions demonstrate the
framework's ability to localise pollution sources with high accuracy. Results
show that the proposed approach achieves reliable source localisation
efficiently. This work contributes to the development of full autonomous
environmental monitoring capabilities essential for rapid response to marine
pollution incidents.

</details>


### [9] [Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation](https://arxiv.org/abs/2509.20623)
*Satyajeet Das,Darren Chiu,Zhehui Huang,Lars Lindemann,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: 提出了一种推理时潜在激活编辑框架，在不修改预训练策略权重的情况下，通过检测和编辑中间激活来提升多四旋翼导航的安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练的策略在复杂环境中仍容易发生碰撞，但重新训练或微调成本高且可能破坏已学技能。

Method: 两阶段框架：在线分类器检测与不良行为相关的激活状态，激活编辑模块选择性修改这些激活以转向更安全的行为模式。

Result: 仿真和真实实验显示碰撞显著减少（比基线减少近90%），碰撞自由轨迹比例大幅增加，同时保持任务完成能力。

Conclusion: LAE是一种轻量级范式，可在资源受限硬件上实现学习机器人策略的部署后优化。

Abstract: Reinforcement learning has enabled significant progress in complex domains
such as coordinating and navigating multiple quadrotors. However, even
well-trained policies remain vulnerable to collisions in obstacle-rich
environments. Addressing these infrequent but critical safety failures through
retraining or fine-tuning is costly and risks degrading previously learned
skills. Inspired by activation steering in large language models and latent
editing in computer vision, we introduce a framework for inference-time Latent
Activation Editing (LAE) that refines the behavior of pre-trained policies
without modifying their weights or architecture. The framework operates in two
stages: (i) an online classifier monitors intermediate activations to detect
states associated with undesired behaviors, and (ii) an activation editing
module that selectively modifies flagged activations to shift the policy
towards safer regimes. In this work, we focus on improving safety in
multi-quadrotor navigation. We hypothesize that amplifying a policy's internal
perception of risk can induce safer behaviors. We instantiate this idea through
a latent collision world model trained to predict future pre-collision
activations, thereby prompting earlier and more cautious avoidance responses.
Extensive simulations and real-world Crazyflie experiments demonstrate that LAE
achieves statistically significant reduction in collisions (nearly 90% fewer
cumulative collisions compared to the unedited baseline) and substantially
increases the fraction of collision-free trajectories, while preserving task
completion. More broadly, our results establish LAE as a lightweight paradigm,
feasible on resource-constrained hardware, for post-deployment refinement of
learned robot policies.

</details>


### [10] [Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments](https://arxiv.org/abs/2509.20635)
*Matheus P. Angarola,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出了一种分层强化学习框架，利用地形专用策略和课程学习来提升腿式机器人在复杂环境中的敏捷性和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人需要在多样化的非结构化地形上展现鲁棒和敏捷的运动能力，这在盲运动设置下更具挑战性，因为地形信息不可用。

Method: 采用分层强化学习框架，结合地形专用策略和课程学习方法来增强机器人在复杂环境中的运动能力。

Result: 在仿真中验证，该方法比通用策略成功率提高16%，在速度目标增加时跟踪误差更低，特别是在低摩擦和不连续地形上表现优异。

Conclusion: 该方法在混合地形场景中展现出卓越的适应性和鲁棒性，能够有效提升腿式机器人的盲运动性能。

Abstract: Legged robots must exhibit robust and agile locomotion across diverse,
unstructured terrains, a challenge exacerbated under blind locomotion settings
where terrain information is unavailable. This work introduces a hierarchical
reinforcement learning framework that leverages terrain-specialized policies
and curriculum learning to enhance agility and tracking performance in complex
environments. We validated our method on simulation, where our approach
outperforms a generalist policy by up to 16% in success rate and achieves lower
tracking errors as the velocity target increases, particularly on low-friction
and discontinuous terrains, demonstrating superior adaptability and robustness
across mixed-terrain scenarios.

</details>


### [11] [Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation](https://arxiv.org/abs/2509.20646)
*Sun Zhaole,Xiaofeng Mao,Jihong Zhu,Yuanlong Zhang,Robert B. Fisher*

Main category: cs.RO

TL;DR: 本文提出了一种新型机器人手设计SLeap Hand，通过指尖吸盘取代传统的力闭合抓取，简化了手内遥操作，并实现了人类手难以完成的灵巧技能。


<details>
  <summary>Details</summary>
Motivation: 传统仿人手机器人存在两个关键障碍：限制了机器人只能完成人类能做的任务，以及使基于学习的方法数据收集极其困难。这些挑战源于传统的力闭合抓取需要协调复杂的多点接触。

Method: 设计SLeap Hand多指手，在指尖集成吸盘，实现基于吸力的灵巧操作。用稳定的单点吸附取代复杂的力闭合抓取。

Result: 该设计简化了手内遥操作，便于收集高质量演示数据，并解锁了人类手难以完成的新技能类别，如单手剪纸和手内书写。

Conclusion: 通过超越仿人约束，新型机器人本体不仅能降低收集稳健操作数据的门槛，还能稳定地单手完成通常需要双手的人类任务。

Abstract: Dexterous in-hand manipulation remains a foundational challenge in robotics,
with progress often constrained by the prevailing paradigm of imitating the
human hand. This anthropomorphic approach creates two critical barriers: 1) it
limits robotic capabilities to tasks humans can already perform, and 2) it
makes data collection for learning-based methods exceedingly difficult. Both
challenges are caused by traditional force-closure which requires coordinating
complex, multi-point contacts based on friction, normal force, and gravity to
grasp an object. This makes teleoperated demonstrations unstable and amplifies
the sim-to-real gap for reinforcement learning. In this work, we propose a
paradigm shift: moving away from replicating human mechanics toward the design
of novel robotic embodiments. We introduce the \textbf{S}uction
\textbf{Leap}-Hand (SLeap Hand), a multi-fingered hand featuring integrated
fingertip suction cups that realize a new form of suction-enabled dexterity. By
replacing complex force-closure grasps with stable, single-point adhesion, our
design fundamentally simplifies in-hand teleoperation and facilitates the
collection of high-quality demonstration data. More importantly, this
suction-based embodiment unlocks a new class of dexterous skills that are
difficult or even impossible for the human hand, such as one-handed paper
cutting and in-hand writing. Our work demonstrates that by moving beyond
anthropomorphic constraints, novel embodiments can not only lower the barrier
for collecting robust manipulation data but also enable the stable,
single-handed completion of tasks that would typically require two human hands.
Our webpage is https://sites.google.com/view/sleaphand.

</details>


### [12] [Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills](https://arxiv.org/abs/2509.20653)
*Congkai Shen,Siyuan Yu,Yifan Weng,Haoran Ma,Chen Li,Hiroshi Yasuda,James Dallas,Michael Thompson,John Subosits,Tulga Ersal*

Main category: cs.RO

TL;DR: 提出了一种基于触觉共享控制的赛博赛车教练框架，通过人机协作帮助人类驾驶员学习高性能驾驶技能，结果显示该框架比无辅助和全辅助训练效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有共享控制方案主要关注简单任务或使用视觉听觉反馈，缺乏对复杂高性能驾驶技能获取效果的评估，需要填补这一研究空白。

Method: 创建了触觉共享控制框架，包括：(1)能在高性能驾驶场景中与人协作的自动驾驶系统；(2)基于驾驶员表现逐步减少转向辅助的消退机制。

Result: 人类受试者研究表明，该框架相比无辅助和全辅助训练基准，能帮助驾驶员发展出更优越的赛车技能，获得更好的性能和一致性。

Conclusion: 触觉共享控制框架能有效促进人类驾驶员在高性能驾驶任务中的技能获取，证明了其在复杂驾驶技能训练中的价值。

Abstract: This study introduces a haptic shared control framework designed to teach
human drivers advanced driving skills. In this context, shared control refers
to a driving mode where the human driver collaborates with an autonomous
driving system to control the steering of a vehicle simultaneously. Advanced
driving skills are those necessary to safely push the vehicle to its handling
limits in high-performance driving such as racing and emergency obstacle
avoidance. Previous research has demonstrated the performance and safety
benefits of shared control schemes using both subjective and objective
evaluations. However, these schemes have not been assessed for their impact on
skill acquisition on complex and demanding tasks. Prior research on long-term
skill acquisition either applies haptic shared control to simple tasks or
employs other feedback methods like visual and auditory aids. To bridge this
gap, this study creates a cyber racing coach framework based on the haptic
shared control paradigm and evaluates its performance in helping human drivers
acquire high-performance driving skills. The framework introduces (1) an
autonomous driving system that is capable of cooperating with humans in a
highly performant driving scenario; and (2) a haptic shared control mechanism
along with a fading scheme to gradually reduce the steering assistance from
autonomy based on the human driver's performance during training. Two
benchmarks are considered: self-learning (no assistance) and full assistance
during training. Results from a human subject study indicate that the proposed
framework helps human drivers develop superior racing skills compared to the
benchmarks, resulting in better performance and consistency.

</details>


### [13] [EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation](https://arxiv.org/abs/2509.20656)
*Junzhe Wang,Jiarui Xie,Pengfei Hao,Zheng Li,Yi Cai*

Main category: cs.RO

TL;DR: 提出了一种闭环BCI-AR-机器人系统，整合了基于运动想象的EEG解码、增强现实神经反馈和机器人抓取，实现了零接触操作，显著提高了EEG控制的稳定性和机器人抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有BCI-机器人系统存在EEG信号噪声大、目标选择预定义不灵活、缺乏闭环验证等问题，阻碍了在辅助场景中的实际部署。

Method: 使用14通道EEG耳机进行个性化MI校准，基于智能手机的AR界面支持多目标导航和方向一致反馈，机器人手臂结合决策输出和基于视觉的位姿估计实现自主抓取。

Result: MI训练准确率达93.1%，平均ITR为14.8 bit/min；AR神经反馈显著改善持续控制（SCI=0.210），最高ITR达21.3 bit/min；闭环抓取成功率97.2%，用户报告控制感强。

Conclusion: AR反馈显著稳定了基于EEG的控制，所提框架实现了鲁棒的零接触抓取，推进了辅助机器人应用和未来人机交互模式。

Abstract: Reliable brain-computer interface (BCI) control of robots provides an
intuitive and accessible means of human-robot interaction, particularly
valuable for individuals with motor impairments. However, existing BCI-Robot
systems face major limitations: electroencephalography (EEG) signals are noisy
and unstable, target selection is often predefined and inflexible, and most
studies remain restricted to simulation without closed-loop validation. These
issues hinder real-world deployment in assistive scenarios. To address them, we
propose a closed-loop BCI-AR-Robot system that integrates motor imagery
(MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic
grasping for zero-touch operation. A 14-channel EEG headset enabled
individualized MI calibration, a smartphone-based AR interface supported
multi-target navigation with direction-congruent feedback to enhance stability,
and the robotic arm combined decision outputs with vision-based pose estimation
for autonomous grasping. Experiments are conducted to validate the framework:
MI training achieved 93.1 percent accuracy with an average information transfer
rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained
control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with
static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2
percent success rate with good efficiency and strong user-reported control.
These results show that AR feedback substantially stabilizes EEG-based control
and that the proposed framework enables robust zero-touch grasping, advancing
assistive robotic applications and future modes of human-robot interaction.

</details>


### [14] [Equi-RO: A 4D mmWave Radar Odometry via Equivariant Networks](https://arxiv.org/abs/2509.20674)
*Zeyu Han,Shuocheng Yang,Minghan Zhu,Fang Zhang,Shaobing Xu,Maani Ghaffari,Jianqiang Wang*

Main category: cs.RO

TL;DR: Equi-RO是一个基于等变网络的4D毫米波雷达里程计框架，通过将多普勒速度预处理为图中的不变节点和边特征，并使用分离网络处理等变和不变特征，在稀疏雷达数据中提高了帧间对应关系，在公开数据集上相比最佳基线方法在平移和旋转精度上分别提升了10.7%和20.0%。


<details>
  <summary>Details</summary>
Motivation: 在GPS信号缺失的环境中，自动驾驶车辆和机器人需要准确的里程计估计。虽然激光雷达和相机在极端天气下表现不佳，但4D毫米波雷达具有全天候操作能力和速度测量能力，成为一种鲁棒的替代方案。

Method: 提出Equi-RO框架，将多普勒速度预处理为图中的不变节点和边特征，采用分离网络分别处理等变和不变特征，使用基于图的架构增强稀疏雷达数据中的特征聚合。

Result: 在公开数据集和自收集数据集上的实验表明，Equi-RO在准确性和鲁棒性方面优于最先进的算法。在公开数据集上，相比最佳基线方法，平移和旋转精度分别相对提升了10.7%和20.0%。

Conclusion: Equi-RO框架通过等变网络和基于图的特征处理，有效提升了4D毫米波雷达里程计的精度和鲁棒性，为GPS缺失环境下的自主导航提供了可靠解决方案。

Abstract: Autonomous vehicles and robots rely on accurate odometry estimation in
GPS-denied environments. While LiDARs and cameras struggle under extreme
weather, 4D mmWave radar emerges as a robust alternative with all-weather
operability and velocity measurement. In this paper, we introduce Equi-RO, an
equivariant network-based framework for 4D radar odometry. Our algorithm
pre-processes Doppler velocity into invariant node and edge features in the
graph, and employs separate networks for equivariant and invariant feature
processing. A graph-based architecture enhances feature aggregation in sparse
radar data, improving inter-frame correspondence. Experiments on the
open-source dataset and self-collected dataset show Equi-RO outperforms
state-of-the-art algorithms in accuracy and robustness. Overall, our method
achieves 10.7% and 20.0% relative improvements in translation and rotation
accuracy, respectively, compared to the best baseline on the open-source
dataset.

</details>


### [15] [Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation](https://arxiv.org/abs/2509.20681)
*Wei-Teng Chu,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: FINS是一个轻量级框架，能够从单张或少量图像快速重建高保真表面和SDF场，训练时间仅需几秒，在收敛速度和精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统隐式表面重建方法如NeuS需要大量多视角图像和长时间训练，限制了实际应用。本文旨在解决从单张图像快速构建隐式距离表示的问题。

Method: FINS整合了多分辨率哈希网格编码器与轻量级几何和颜色头，使用近似二阶优化器实现高效训练。利用预训练基础模型从单张RGB图像估计几何信息。

Result: 实验表明，在相同条件下，FINS在表面重建和SDF场估计的收敛速度和精度上均优于现有最优方法，并成功应用于机器人表面跟随任务。

Conclusion: FINS是一个高效的单图像神经表面重建框架，在速度和精度上均有显著提升，展示了在机器人应用中的良好可扩展性。

Abstract: Implicit representations have been widely applied in robotics for obstacle
avoidance and path planning. In this paper, we explore the problem of
constructing an implicit distance representation from a single image. Past
methods for implicit surface reconstruction, such as \emph{NeuS} and its
variants generally require a large set of multi-view images as input, and
require long training times. In this work, we propose Fast Image-to-Neural
Surface (FINS), a lightweight framework that can reconstruct high-fidelity
surfaces and SDF fields based on a single or a small set of images. FINS
integrates a multi-resolution hash grid encoder with lightweight geometry and
color heads, making the training via an approximate second-order optimizer
highly efficient and capable of converging within a few seconds. Additionally,
we achieve the construction of a neural surface requiring only a single RGB
image, by leveraging pre-trained foundation models to estimate the geometry
inherent in the image. Our experiments demonstrate that under the same
conditions, our method outperforms state-of-the-art baselines in both
convergence speed and accuracy on surface reconstruction and SDF field
estimation. Moreover, we demonstrate the applicability of FINS for robot
surface following tasks and show its scalability to a variety of benchmark
datasets.

</details>


### [16] [RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks](https://arxiv.org/abs/2509.20688)
*Shouren Mao,Minghao Qin,Wei Dong,Huajian Liu,Yongzhuo Gao*

Main category: cs.RO

TL;DR: RAM-NAS是一种资源感知的多目标神经架构搜索方法，专注于改进超网络预训练和机器人硬件设备的资源感知能力，通过子网互蒸馏和延迟代理预测器来平衡模型精度和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索方法在训练超网络方面不足，且很少关注实际机器人硬件资源。需要开发能够同时考虑模型精度和硬件推理延迟的NAS方法。

Method: 提出子网互蒸馏概念，使用三明治规则采样子网进行相互蒸馏；采用解耦知识蒸馏损失增强logits蒸馏性能；使用三种机器人边缘硬件数据训练延迟代理预测器，在搜索阶段估计硬件推理延迟；采用统一的多目标进化搜索平衡精度和延迟权衡。

Result: RAM-NAS模型在ImageNet上达到76.7%到81.4%的top-1准确率；相比基于MobileNetv3的方法，在所有三种硬件类型上检测和分割的推理时间均有所减少；显著降低了模型在机器人边缘硬件上的推理延迟。

Conclusion: RAM-NAS填补了NAS在机器人硬件资源感知方面的空白，通过资源感知的多目标NAS显著提升了模型在边缘硬件上的性能表现。

Abstract: Neural architecture search (NAS) has shown great promise in automatically
designing lightweight models. However, conventional approaches are insufficient
in training the supernet and pay little attention to actual robot hardware
resources. To meet such challenges, we propose RAM-NAS, a resource-aware
multi-objective NAS method that focuses on improving the supernet pretrain and
resource-awareness on robot hardware devices. We introduce the concept of
subnets mutual distillation, which refers to mutually distilling all subnets
sampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge
Distillation (DKD) loss to enhance logits distillation performance. To expedite
the search process with consideration for hardware resources, we used data from
three types of robotic edge hardware to train Latency Surrogate predictors.
These predictors facilitated the estimation of hardware inference latency
during the search phase, enabling a unified multi-objective evolutionary search
to balance model accuracy and latency trade-offs. Our discovered model family,
RAM-NAS models, can achieve top-1 accuracy ranging from 76.7% to 81.4% on
ImageNet. In addition, the resource-aware multi-objective NAS we employ
significantly reduces the model's inference latency on edge hardware for
robots. We conducted experiments on downstream tasks to verify the scalability
of our methods. The inference time for detection and segmentation is reduced on
all three hardware types compared to MobileNetv3-based methods. Our work fills
the gap in NAS for robot hardware resource-aware.

</details>


### [17] [Incorporating Human-Inspired Ankle Characteristics in a Forced-Oscillation-Based Reduced-Order Model for Walking](https://arxiv.org/abs/2509.20689)
*Chathura Semasinghe,Siavash Rezazadeh*

Main category: cs.RO

TL;DR: 本文扩展了基于强迫振荡的步行简化模型，加入了踝关节和足部。设计了人体启发的踝关节动力学范式，相比点足模型改善了步态特性。模型能通过足部放置和踝关节策略组合稳定大初始误差，仅通过本体感觉踝关节方案稳定小扰动。


<details>
  <summary>Details</summary>
Motivation: 扩展步行简化模型以包含踝关节和足部，改进点足模型的步态特性，更好地理解人体行走及其稳定机制。

Method: 采用人体启发的踝关节动力学范式，结合足部放置和踝关节策略，设计本体感觉踝关节控制方案。

Result: 与点足模型相比，新模型具有改进的步态特性，能稳定大初始误差和小扰动，且小扰动下无需依赖足部放置控制。

Conclusion: 提出的模型能模拟人类行走的稳定机制，特别是仅通过踝关节策略稳定小扰动的特性，有助于更好地理解拟人行走及其稳定机制。

Abstract: This paper extends the forced-oscillation-based reduced-order model of
walking to a model with ankles and feet. A human-inspired paradigm was designed
for the ankle dynamics, which results in improved gait characteristics compared
to the point-foot model. In addition, it was shown that while the proposed
model can stabilize against large errors in initial conditions through
combination of foot placement and ankle strategies, the model is able to
stabilize against small perturbations without relying on the foot placement
control and solely through the designed proprioceptive ankle scheme. This novel
property, which is also observed in humans, can help in better understanding of
anthropomorphic walking and its stabilization mechanisms.

</details>


### [18] [RuN: Residual Policy for Natural Humanoid Locomotion](https://arxiv.org/abs/2509.20696)
*Qingpeng Li,Chengrui Zhu,Yanming Wu,Xin Yuan,Zhen Zhang,Jian Yang,Yong Liu*

Main category: cs.RO

TL;DR: RuN是一个解耦的残差学习框架，通过将预训练的条件运动生成器与强化学习策略配对，实现了人形机器人从走到跑的自然动态运动。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在广泛速度范围内实现自然动态运动（包括从走到跑的平滑过渡）的挑战，现有方法需要单一策略同时学习运动模仿、速度跟踪和稳定性维持。

Method: 引入RuN框架，将控制任务分解：使用预训练的条件运动生成器提供运动先验，配合强化学习策略学习轻量级残差修正来处理动力学交互。

Result: 在Unitree G1人形机器人上的仿真和真实实验表明，RuN在0-2.5 m/s速度范围内实现了稳定自然的步态和平滑的走跑转换，在训练效率和最终性能上都优于现有最优方法。

Conclusion: RuN框架通过解耦控制任务，有效解决了人形机器人自然动态运动的挑战，为机器人运动控制提供了新的解决方案。

Abstract: Enabling humanoid robots to achieve natural and dynamic locomotion across a
wide range of speeds, including smooth transitions from walking to running,
presents a significant challenge. Existing deep reinforcement learning methods
typically require the policy to directly track a reference motion, forcing a
single policy to simultaneously learn motion imitation, velocity tracking, and
stability maintenance. To address this, we introduce RuN, a novel decoupled
residual learning framework. RuN decomposes the control task by pairing a
pre-trained Conditional Motion Generator, which provides a kinematically
natural motion prior, with a reinforcement learning policy that learns a
lightweight residual correction to handle dynamical interactions. Experiments
in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN
achieves stable, natural gaits and smooth walk-run transitions across a broad
velocity range (0-2.5 m/s), outperforming state-of-the-art methods in both
training efficiency and final performance.

</details>


### [19] [Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations](https://arxiv.org/abs/2509.20703)
*Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: 提出了JFTO框架，通过对象中心的方法从人类视频演示中学习机器人抓取和操作轨迹，解决了人机差异和关节可行性约束问题。


<details>
  <summary>Details</summary>
Motivation: 从人类视频演示学习为机器人操作提供了可扩展的替代方案，但由于人机差异和关节可行性约束，直接模仿人手运动存在挑战。

Method: 采用联合流轨迹优化框架，将演示视为对象中心指导，平衡三个目标：选择可行抓取姿势、生成与演示一致的对象轨迹、确保无碰撞执行。扩展流匹配到SE(3)进行概率建模。

Result: 在仿真和真实世界实验中验证了方法在多样化操作任务中的有效性。

Conclusion: JFTO框架成功解决了从人类视频演示学习机器人操作的问题，通过对象中心方法和概率建模实现了有效的抓取和轨迹生成。

Abstract: Learning from human video demonstrations offers a scalable alternative to
teleoperation or kinesthetic teaching, but poses challenges for robot
manipulators due to embodiment differences and joint feasibility constraints.
We address this problem by proposing the Joint Flow Trajectory Optimization
(JFTO) framework for grasp pose generation and object trajectory imitation
under the video-based Learning-from-Demonstration (LfD) paradigm. Rather than
directly imitating human hand motions, our method treats demonstrations as
object-centric guides, balancing three objectives: (i) selecting a feasible
grasp pose, (ii) generating object trajectories consistent with demonstrated
motions, and (iii) ensuring collision-free execution within robot kinematics.
To capture the multimodal nature of demonstrations, we extend flow matching to
$\SE(3)$ for probabilistic modeling of object trajectories, enabling
density-aware imitation that avoids mode collapse. The resulting optimization
integrates grasp similarity, trajectory likelihood, and collision penalties
into a unified differentiable objective. We validate our approach in both
simulation and real-world experiments across diverse real-world manipulation
tasks.

</details>


### [20] [Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework](https://arxiv.org/abs/2509.20705)
*Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti*

Main category: cs.RO

TL;DR: BIM2RDT框架将静态BIM转换为动态、机器人就绪的数字孪生，通过整合BIM数据、IoT传感器和机器人视觉数据，采用SG-ICP点云配准算法提升对齐精度，并集成实时安全监控。


<details>
  <summary>Details</summary>
Motivation: 解决现有BIM数据与实时现场条件之间的差距，提升建筑行业数字管理的安全性和效率。

Method: 整合BIM几何语义信息、IoT活动数据和机器人视觉空间数据，开发SG-ICP点云配准算法（利用LLM推理对象方向先验），结合YOLOE目标检测和Shi-Tomasi角点检测，集成实时HAV安全监控。

Result: SG-ICP相比标准ICP在遮挡场景下对齐RMSE降低64.3%-88.3%，HAV集成能在超过暴露限值时触发警告。

Conclusion: BIM2RDT框架成功将静态BIM转化为动态数字孪生，显著提升对齐精度和现场安全监控能力，为建筑行业数字化管理提供有效解决方案。

Abstract: The adoption of cyber-physical systems and jobsite intelligence that connects
design models, real-time site sensing, and autonomous field operations can
dramatically enhance digital management in the construction industry. This
paper introduces BIM2RDT (Building Information Models to Robot-Ready Site
Digital Twins), an agentic artificial intelligence (AI) framework designed to
transform static Building Information Modeling (BIM) into dynamic, robot-ready
digital twins (DTs) that prioritize safety during execution. The framework
bridges the gap between pre-existing BIM data and real-time site conditions by
integrating three key data streams: geometric and semantic information from BIM
models, activity data from IoT sensor networks, and visual-spatial data
collected by robots during site traversal. The methodology introduces
Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that
leverages large language model (LLM) reasoning. Unlike traditional methods,
SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors
based on BIM semantics, improving alignment accuracy by avoiding convergence on
local minima. This creates a feedback loop where robot-collected data updates
the DT, which in turn optimizes paths for missions. The framework employs YOLOE
object detection and Shi-Tomasi corner detection to identify and track
construction elements while using BIM geometry as a priori maps. The framework
also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping
sensor-detected safety events to the digital twin using IFC standards for
intervention. Experiments demonstrate SG-ICP's superiority over standard ICP,
achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with
occluded features, ensuring plausible orientations. HAV integration triggers
warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.

</details>


### [21] [Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor](https://arxiv.org/abs/2509.20709)
*Mani Amani,Reza Akhavian*

Main category: cs.RO

TL;DR: 提出了一种将自然语言指令与BIM语义地图融合的机器人任务规划框架，通过Beta-Bernoulli贝叶斯融合将LLM视为传感器，生成上下文感知的排斥增益来改进路径规划。


<details>
  <summary>Details</summary>
Motivation: 在建筑领域，BIM模型包含丰富的环境自然语言描述，需要将自然语言指令与机器人任务规划有效融合，实现更安全、上下文感知的路径规划。

Method: 使用Beta-Bernoulli贝叶斯融合方法，将LLM返回的危险分数作为伪计数来更新Beta分布的参数，生成连续、上下文感知的排斥增益，增强基于欧几里得距离的势场成本启发式。

Result: 仿真结果表明，该方法在路径鲁棒性和有效性方面都取得了定性和定量的改进。

Conclusion: 该方法提供了一种数值稳定的方式，能够链接多个自然命令和提示，使机器人能够规划更安全、上下文感知的路径，并可集成到任何学习或经典AI框架中。

Abstract: Integrating natural language (NL) prompts into robotic mission planning has
attracted significant interest in recent years. In the construction domain,
Building Information Models (BIM) encapsulate rich NL descriptions of the
environment. We present a novel framework that fuses NL directives with
BIM-derived semantic maps via a Beta-Bernoulli Bayesian fusion by interpreting
the LLM as a sensor: each obstacle's design-time repulsive coefficient is
treated as a Beta(alpha, beta) random variable and LLM-returned danger scores
are incorporated as pseudo-counts to update alpha and beta. The resulting
posterior mean yields a continuous, context-aware repulsive gain that augments
a Euclidean-distance-based potential field for cost heuristics. By adjusting
gains based on sentiment and context inferred from user prompts, our method
guides robots along safer, more context-aware paths. This provides a
numerically stable method that can chain multiple natural commands and prompts
from construction workers and foreman to enable planning while giving
flexibility to be integrated in any learned or classical AI framework.
Simulation results demonstrate that this Beta-Bernoulli fusion yields both
qualitative and quantitative improvements in path robustness and validity.

</details>


### [22] [RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking](https://arxiv.org/abs/2509.20717)
*Zhenguo Sun,Yibo Peng,Yuan Meng,Xukun Li,Bo-Sheng Huang,Zhenshan Bing,Xinlong Wang,Alois Knoll*

Main category: cs.RO

TL;DR: RobotDancing是一个简单可扩展的框架，通过预测残差关节目标来纠正动力学差异，实现人形机器人的长时程高动态运动跟踪。


<details>
  <summary>Details</summary>
Motivation: 人形机器人的长时程高动态运动跟踪仍然脆弱，因为绝对关节指令无法补偿模型-实际差异，导致误差累积。

Method: 采用端到端管道，使用单阶段强化学习设置，具有统一的观测、奖励和超参数配置，包括训练、模拟到模拟验证和零样本模拟到真实部署。

Result: 在Unitree G1上评估，能够跟踪多分钟的高能量行为（跳跃、旋转、侧手翻），并以高运动跟踪质量零样本部署到硬件。

Conclusion: RobotDancing框架能够有效解决人形机器人长时程高动态运动跟踪中的误差累积问题，实现零样本模拟到真实部署。

Abstract: Long-horizon, high-dynamic motion tracking on humanoids remains brittle
because absolute joint commands cannot compensate model-plant mismatch, leading
to error accumulation. We propose RobotDancing, a simple, scalable framework
that predicts residual joint targets to explicitly correct dynamics
discrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and
zero-shot sim-to-real--and uses a single-stage reinforcement learning (RL)
setup with a unified observation, reward, and hyperparameter configuration. We
evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and
validate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy
behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with
high motion tracking quality.

</details>


### [23] [SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning](https://arxiv.org/abs/2509.20739)
*Guoyang Zhao,Yudong Li,Weiqing Qi,Kai Zhang,Bonan Liu,Kai Chen,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言感知的SLAM-free导航框架，使用语义推理和轻量级拓扑表示替代传统SLAM的密集几何重建，通过分层感知和概率拓扑地图实现从全局推理到局部规划的导航。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM方法在足式机器人导航中存在对快速运动敏感、标定要求高、传感器漂移等问题，且缺乏语义推理能力。需要一种更鲁棒、语义驱动的导航方案。

Method: 采用纯视觉SLAM-free框架，包含分层视觉语言感知模块（融合场景级上下文和物体级线索）和语义概率拓扑地图，结合LLM全局推理选择子目标，视觉局部规划避障，与强化学习运动控制器集成。

Result: 在仿真和真实环境实验中，在语义准确性、规划质量和导航成功率方面均取得一致改进，消融研究验证了分层感知和精细局部规划的必要性。

Conclusion: 这项工作引入了SLAM-free、视觉语言驱动导航的新范式，将机器人探索从几何中心映射转向语义驱动决策。

Abstract: Conventional SLAM pipelines for legged robot navigation are fragile under
rapid motion, calibration demands, and sensor drift, while offering limited
semantic reasoning for task-driven exploration. To deal with these issues, we
propose a vision-only, SLAM-free navigation framework that replaces dense
geometry with semantic reasoning and lightweight topological representations. A
hierarchical vision-language perception module fuses scene-level context with
object-level cues for robust semantic inference. And a semantic-probabilistic
topological map supports coarse-to-fine planning: LLM-based global reasoning
for subgoal selection and vision-based local planning for obstacle avoidance.
Integrated with reinforcement-learning locomotion controllers, the framework is
deployable across diverse legged robot platforms. Experiments in simulation and
real-world settings demonstrate consistent improvements in semantic accuracy,
planning quality, and navigation success, while ablation studies further
showcase the necessity of both hierarchical perception and fine local planning.
This work introduces a new paradigm for SLAM-free, vision-language-driven
navigation, shifting robotic exploration from geometry-centric mapping to
semantics-driven decision making.

</details>


### [24] [MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM](https://arxiv.org/abs/2509.20757)
*Yuxuan Zhou,Xingxing Li,Shengyu Li,Zhuohao Yan,Chunxi Xia,Shaoquan Feng*

Main category: cs.RO

TL;DR: MASt3R-Fusion是一个多传感器辅助的视觉SLAM框架，将前馈点云回归与惯性测量和GNSS数据紧密集成，通过Sim(3)视觉对齐约束和分层因子图设计，实现实时姿态跟踪和全局一致的地图构建。


<details>
  <summary>Details</summary>
Motivation: 传统视觉SLAM系统在低纹理环境和挑战性视觉条件下表现不佳，而现有的基于神经网络的点云回归方法虽然能恢复高保真3D场景几何，但往往丢弃了概率多传感器信息融合的优势。

Method: 提出Sim(3)视觉对齐约束，将其集成到通用度量尺度SE(3)因子图中，采用分层因子图设计，支持实时滑动窗口优化和具有激进闭环的全局优化。

Result: 在公共基准和自收集数据集上的评估表明，相比现有的视觉中心多传感器SLAM系统，在精度和鲁棒性方面有显著提升。

Conclusion: MASt3R-Fusion框架成功整合了前馈点云回归与多传感器信息，实现了实时姿态跟踪、度量尺度结构感知和全局一致映射，代码将开源发布。

Abstract: Visual SLAM is a cornerstone technique in robotics, autonomous driving and
extended reality (XR), yet classical systems often struggle with low-texture
environments, scale ambiguity, and degraded performance under challenging
visual conditions. Recent advancements in feed-forward neural network-based
pointmap regression have demonstrated the potential to recover high-fidelity 3D
scene geometry directly from images, leveraging learned spatial priors to
overcome limitations of traditional multi-view geometry methods. However, the
widely validated advantages of probabilistic multi-sensor information fusion
are often discarded in these pipelines. In this work, we propose
MASt3R-Fusion,a multi-sensor-assisted visual SLAM framework that tightly
integrates feed-forward pointmap regression with complementary sensor
information, including inertial measurements and GNSS data. The system
introduces Sim(3)-based visualalignment constraints (in the Hessian form) into
a universal metric-scale SE(3) factor graph for effective information fusion. A
hierarchical factor graph design is developed, which allows both real-time
sliding-window optimization and global optimization with aggressive loop
closures, enabling real-time pose tracking, metric-scale structure perception
and globally consistent mapping. We evaluate our approach on both public
benchmarks and self-collected datasets, demonstrating substantial improvements
in accuracy and robustness over existing visual-centered multi-sensor SLAM
systems. The code will be released open-source to support reproducibility and
further research (https://github.com/GREAT-WHU/MASt3R-Fusion).

</details>


### [25] [Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning](https://arxiv.org/abs/2509.20766)
*Gawon Lee,Daesol Cho,H. Jin Kim*

Main category: cs.RO

TL;DR: MT-Lévy是一种新颖的多任务强化学习探索策略，通过结合跨任务行为共享和受Lévy飞行启发的时序扩展探索，显著提高了机器人环境中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习在机器人应用中面临数据收集成本高的问题，需要更高效的探索策略来提高样本效率。

Method: MT-Lévy结合了跨任务行为共享（利用相关任务训练的策略指导探索）和Lévy飞行启发的自适应探索策略，根据任务成功率动态调整探索水平。

Result: 实证结果表明MT-Lévy显著改善了探索效率和样本效率，在复杂机器人环境中实现了更高效的状态空间覆盖。

Conclusion: 结合行为共享和自适应探索策略可以显著提高多任务强化学习在机器人应用中的实用性。

Abstract: Multi-task reinforcement learning (MTRL) offers a promising approach to
improve sample efficiency and generalization by training agents across multiple
tasks, enabling knowledge sharing between them. However, applying MTRL to
robotics remains challenging due to the high cost of collecting diverse task
data. To address this, we propose MT-L\'evy, a novel exploration strategy that
enhances sample efficiency in MTRL environments by combining behavior sharing
across tasks with temporally extended exploration inspired by L\'evy flight.
MT-L\'evy leverages policies trained on related tasks to guide exploration
towards key states, while dynamically adjusting exploration levels based on
task success ratios. This approach enables more efficient state-space coverage,
even in complex robotics environments. Empirical results demonstrate that
MT-L\'evy significantly improves exploration and sample efficiency, supported
by quantitative and qualitative analyses. Ablation studies further highlight
the contribution of each component, showing that combining behavior sharing
with adaptive exploration strategies can significantly improve the practicality
of MTRL in robotics applications.

</details>


### [26] [SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation](https://arxiv.org/abs/2509.20839)
*Jiaxuan He,Jiamei Ren,Chongshang Yan,Wenjie Song*

Main category: cs.RO

TL;DR: SemSight是一个用于多层级场景语义的概率性鸟瞰图预测模型，能够联合推断结构布局、全局场景上下文和目标区域分布，在完成未探索区域语义地图的同时估计目标类别的概率图。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单个对象或几何占据地图，缺乏对房间级语义结构的建模能力。在目标驱动导航和自主探索中，对未知区域的合理预测对于高效导航和环境理解至关重要。

Method: 采用编码器-解码器网络作为核心架构，引入掩码约束监督策略。该策略应用未探索区域的二值掩码，使监督仅关注未知区域，迫使模型从观察到的上下文中推断语义结构。在2000个室内布局图上模拟前沿驱动探索，构建了包含40000个序列化自我中心观察与完整语义地图配对的多样化数据集。

Result: 实验结果表明，SemSight提高了未探索区域关键功能类别的预测性能，在结构一致性(SC)和区域识别准确率(PA)等指标上优于非掩码监督方法。在闭环仿真中还提高了导航效率，减少了引导机器人朝向目标区域的搜索步数。

Conclusion: SemSight通过概率性鸟瞰图预测和多层级语义建模，有效提升了未知区域语义预测的准确性和导航效率，为自主探索和目标驱动导航提供了更强大的环境理解能力。

Abstract: In target-driven navigation and autonomous exploration, reasonable prediction
of unknown regions is crucial for efficient navigation and environment
understanding. Existing methods mostly focus on single objects or geometric
occupancy maps, lacking the ability to model room-level semantic structures. We
propose SemSight, a probabilistic bird's-eye-view prediction model for
multi-level scene semantics. The model jointly infers structural layouts,
global scene context, and target area distributions, completing semantic maps
of unexplored areas while estimating probability maps for target categories. To
train SemSight, we simulate frontier-driven exploration on 2,000 indoor layout
graphs, constructing a diverse dataset of 40,000 sequential egocentric
observations paired with complete semantic maps. We adopt an encoder-decoder
network as the core architecture and introduce a mask-constrained supervision
strategy. This strategy applies a binary mask of unexplored areas so that
supervision focuses only on unknown regions, forcing the model to infer
semantic structures from the observed context. Experimental results show that
SemSight improves prediction performance for key functional categories in
unexplored regions and outperforms non-mask-supervised approaches on metrics
such as Structural Consistency (SC) and Region Recognition Accuracy (PA). It
also enhances navigation efficiency in closed-loop simulations, reducing the
number of search steps when guiding robots toward target areas.

</details>


### [27] [ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation](https://arxiv.org/abs/2509.20841)
*Dekun Lu,Wei Gao,Kui Jia*

Main category: cs.RO

TL;DR: 提出了CoMOK（链式移动定向关键点）作为机器人操作策略的动作表示方法，能够以端到端方式训练，支持多种操作任务，实现亚厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 端到端机器人操作策略相比传统模块化管道具有避免信息丢失和特征不对齐的优势，但现有基于VLM/VLA的神经网络在实际部署中性能不足，需要更通用、准确和可靠的端到端操作策略。

Method: 提出CoMOK（链式移动定向关键点）作为动作表示，扩展了标准末端执行器位姿表示，支持多种操作任务，定向关键点能够自然泛化到不同形状和大小的物体。

Result: 模拟和硬件实验证明该方法有效，能够处理多阶段任务、多模态机器人行为和可变形物体，实现亚厘米级精度。

Conclusion: CoMOK方法为机器人操作提供了一种通用、准确且可靠的端到端策略，在多种任务场景下表现出色。

Abstract: End-to-end robot manipulation policies offer significant potential for
enabling embodied agents to understand and interact with the world. Unlike
traditional modular pipelines, end-to-end learning mitigates key limitations
such as information loss between modules and feature misalignment caused by
isolated optimization targets. Despite these advantages, existing end-to-end
neural networks for robotic manipulation--including those based on large
VLM/VLA models--remain insufficiently performant for large-scale practical
deployment. In this paper, we take a step towards an end-to-end manipulation
policy that is generalizable, accurate and reliable. To achieve this goal, we
propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for
robotic manipulation. Our formulation is used as the action representation of a
neural policy, which can be trained in an end-to-end fashion. Such an action
representation is general, as it extends the standard end-effector pose action
representation and supports a diverse set of manipulation tasks in a unified
manner. The oriented keypoint in our method enables natural generalization to
objects with different shapes and sizes, while achieving sub-centimeter
accuracy. Moreover, our formulation can easily handle multi-stage tasks,
multi-modal robot behaviors, and deformable objects. Extensive simulated and
hardware experiments demonstrate the effectiveness of our method.

</details>


### [28] [MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases](https://arxiv.org/abs/2509.20843)
*Ziang Luo,Kangan Qian,Jiahua Wang,Yuechen Luo,Jinyu Miao,Zheng Fu,Yunlong Wang,Sicong Jiang,Zilin Huang,Yifei Hu,Yuhao Yang,Hao Ye,Mengmeng Yang,Xiaojian Dong,Kun Jiang,Diange Yang*

Main category: cs.RO

TL;DR: MTRDrive是一个集成程序化驾驶经验和动态工具包的新框架，旨在提升视觉语言模型在自动驾驶中的泛化能力和主动决策能力，在NAVSIM和Roadwork-VLM基准测试中取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在端到端自动驾驶中存在幻觉和泛化能力不足的问题，特别是在分布外场景下表现脆弱，需要提升其可靠性和实际部署能力。

Method: 提出闭环系统，结合基于记忆的经验检索机制和动态工具包，通过记忆-工具协同推理来增强环境交互、推理和决策能力。

Result: 在NAVSIM基准测试中，3B参数的MTRDrive模型达到88.3 PDMS（无思维链），驾驶指标得分79.8%，规划准确率82.6%；在Roadwork-VLM基准测试中零样本泛化达到80.2%驾驶指标得分。

Conclusion: MTRDrive展示了推动自动驾驶向更安全可靠系统发展的潜力，特别是在复杂场景下的鲁棒推理能力。

Abstract: Vision-Language Models(VLMs) have demonstrated significant potential for
end-to-end autonomous driving, yet a substantial gap remains between their
current capabilities and the reliability necessary for real-world deployment. A
critical challenge is their fragility, characterized by hallucinations and poor
generalization in out-of-distribution (OOD) scenarios. To bridge this gap, we
introduce MTRDrive, a novel framework that integrates procedural driving
experiences with a dynamic toolkit to enhance generalization and proactive
decision-making.
  MTRDrive addresses these limitations through a closed-loop system that
combines a memory-based experience retrieval mechanism with dynamic toolkits.
This synergy enables the model to interact more effectively with its
environment, improving both reasoning and decision-making capabilities with the
help of our memory-tool synergistic reasoning. Additionally, we introduce a new
benchmark based on complex Roadwork construction scenarios to rigorously
evaluate zero-shot generalization.
  Extensive experiments demonstrate the superior effectiveness of our approach.
On the public NAVSIM benchmark, our 3B-parameter MTRDrive model achieves an
exceptional PDMS of 88.3 without chain-of-thought and sets a state-of-the-art
performance bar on high-level planning, with a driving metric score of 79.8\%
and a planning accuracy of 82.6\%. Rigorous zero-shot evaluation on the new
Roadwork-VLM benchmark shows a strong ability to reason robustly in unseen
scenarios, achieving a driving metric score of 80.2\%. These results highlight
MTRDrive's potential to advance autonomous driving toward safer and more
reliable systems.

</details>


### [29] [Efficient Differentiable Contact Model with Long-range Influence](https://arxiv.org/abs/2509.20917)
*Xiaohan Ye,Kui Wu,Zherong Pan,Taku Komura*

Main category: cs.RO

TL;DR: 本文针对可微分物理模拟中梯度信息不稳定问题，提出了一套接触模型设计准则，并开发了一个满足所有准则的高效接触模型，显著提升了梯度优化在接触密集型任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 可微分物理模拟在下游应用中越来越重要，但其提供的梯度信息经常出现突变或消失，阻碍了基于梯度的优化器收敛。这种不稳定的梯度行为与接触模型的设计密切相关。

Method: 首先分析了接触模型与梯度行为的关系，提出了一套确保良好梯度信息的接触模型设计准则。然后开发了一个满足所有准则的实用接触模型，该模型在保持计算效率的同时提供稳定的梯度。

Result: 实验表明，即使从简单初始化开始，新接触模型也能发现复杂的接触密集型控制信号，成功执行各种下游运动和控制任务。

Conclusion: 通过设计满足特定属性的接触模型，可以显著改善可微分物理模拟中的梯度行为，为接触密集型任务提供更可靠的优化基础。

Abstract: With the maturation of differentiable physics, its role in various downstream
applications: such as model predictive control, robotic design optimization,
and neural PDE solvers, has become increasingly important. However, the
derivative information provided by differentiable simulators can exhibit abrupt
changes or vanish altogether, impeding the convergence of gradient-based
optimizers. In this work, we demonstrate that such erratic gradient behavior is
closely tied to the design of contact models. We further introduce a set of
properties that a contact model must satisfy to ensure well-behaved gradient
information. Lastly, we present a practical contact model for differentiable
rigid-body simulators that satisfies all of these properties while maintaining
computational efficiency. Our experiments show that, even from simple
initializations, our contact model can discover complex, contact-rich control
signals, enabling the successful execution of a range of downstream locomotion
and manipulation tasks.

</details>


### [30] [Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement](https://arxiv.org/abs/2509.20938)
*Jianbo Zhao,Taiyu Ban,Xiangjie Li,Xingtai Gui,Hangning Zhou,Lei Liu,Hongwei Zhao,Bin Li*

Main category: cs.RO

TL;DR: 提出TISA模块解决自动驾驶规划中的时空不对齐问题，通过DPO多目标后训练超越纯模仿学习，在NAVSIM数据集上达到89.8 PDMS的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在自动驾驶端到端规划中表现强劲，但受限于时空不对齐问题——规划器必须基于过去感知数据预测未来动作，造成世界观不一致，限制了性能上限。

Method: 1) TISA模块将初始环境特征投影到每个未来时间步的一致自车坐标系；2) 运动学动作预测头确保物理可行的轨迹；3) 多目标DPO后训练阶段提供针对特定驾驶行为的精细反馈。

Result: 在NAVSIM数据集上，该方法在自回归模型中达到了89.8 PDMS的最先进性能。

Conclusion: TISA模块有效解决了自动驾驶规划中的时空不对齐问题，结合DPO多目标训练超越了纯模仿学习的限制，为自回归规划器提供了更精细的学习信号。

Abstract: The inherent sequential modeling capabilities of autoregressive models make
them a formidable baseline for end-to-end planning in autonomous driving.
Nevertheless, their performance is constrained by a spatio-temporal
misalignment, as the planner must condition future actions on past sensory
data. This creates an inconsistent worldview, limiting the upper bound of
performance for an otherwise powerful approach. To address this, we propose a
Time-Invariant Spatial Alignment (TISA) module that learns to project initial
environmental features into a consistent ego-centric frame for each future time
step, effectively correcting the agent's worldview without explicit future
scene prediction. In addition, we employ a kinematic action prediction head
(i.e., acceleration and yaw rate) to ensure physically feasible trajectories.
Finally, we introduce a multi-objective post-training stage using Direct
Preference Optimization (DPO) to move beyond pure imitation. Our approach
provides targeted feedback on specific driving behaviors, offering a more
fine-grained learning signal than the single, overall objective used in
standard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM
dataset among autoregressive models. The video document is available at
https://tisa-dpo-e2e.github.io/.

</details>


### [31] [BactoBot: A Low-Cost, Bacteria-Inspired Soft Underwater Robot for Marine Exploration](https://arxiv.org/abs/2509.20964)
*Rubaiyat Tasnim Chowdhury,Nayan Bala,Ronojoy Roy,Tarek Mahmud*

Main category: cs.RO

TL;DR: BactoBot是一个低成本软体水下机器人，采用仿细菌鞭毛推进的12个柔性硅胶臂设计，具有固有柔顺性、冗余性和全向运动潜力，通过DIY方法制造并在水箱测试中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 传统刚性水下机器人对脆弱海洋生态系统构成风险，需要开发安全温和的海洋探索工具，特别是在资源受限环境中。

Method: 采用仿细菌鞭毛推进设计，12个柔性硅胶臂安装在3D打印十二面体框架上，使用食品级硅胶成型、3D打印和现成微控制器等DIY方法制造，开发了防水和浮力校准协议。

Result: 在受控水箱中成功测试，展示了前进运动和转向能力，验证了低成本复制复杂生物运动的可行性。

Conclusion: 该项目为环境友好型机器人工具奠定了基础，特别适用于资源受限的海洋科学研究，并确定了实现自主操作和现场部署的路径。

Abstract: Traditional rigid underwater vehicles pose risks to delicate marine
ecosystems. This paper presents BactoBot, a low-cost, soft underwater robot
designed for safe and gentle marine exploration. Inspired by bacterial
flagellar propulsion, BactoBot features 12 flexible, silicone-based arms
arranged on a 3D-printed dodecahedral frame. The design provides inherent
compliance, redundancy, and the potential for omnidirectional movement. The
prototype was fabricated using accessible DIY methods, including food-grade
silicone molding, 3D printing, and off-the-shelf microcontrollers.
Waterproofing and buoyancy calibration protocols were developed, and the robot
was successfully tested in a controlled water tank, demonstrating forward
motion and turning. The results validate the feasibility of replicating complex
biological locomotion at low cost. The project lays a foundation for
environmentally conscious robotic tools, particularly for marine science in
resource-constrained settings, and identifies pathways toward autonomous
operation and field deployment.

</details>


### [32] [AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation](https://arxiv.org/abs/2509.21006)
*Konstantin Gubernatorov,Artem Voronov,Roman Voronov,Sergei Pasynkov,Stepan Perminov,Ziang Guo,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: AnywhereVLA是一个用于移动操作的模块化框架，能够在未见过的室内环境中执行自然语言拾取放置任务，结合了经典SLAM与VLA操作，在嵌入式硬件上实现实时运行，达到46%的整体任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决在不可预测的室内环境中执行自然语言拾取放置任务的挑战，结合几何导航的可靠性和语言条件操作的灵活性。

Method: 使用模块化框架：用户文本提示解析为结构化任务图，结合经典SLAM（LiDAR和相机）、度量语义映射、任务感知边界探索策略，以及基于SmolVLA微调的操作头进行抓取和放置。

Result: 在多房间实验室环境中，系统在静态场景和正常人类运动下达到46%的整体任务成功率，并在嵌入式计算上保持实时吞吐量。

Conclusion: 通过结合经典堆栈和微调VLA操作，系统继承了基于几何导航的可靠性，同时获得了语言条件操作的灵活性和任务泛化能力。

Abstract: We address natural language pick-and-place in unseen, unpredictable indoor
environments with AnywhereVLA, a modular framework for mobile manipulation. A
user text prompt serves as an entry point and is parsed into a structured task
graph that conditions classical SLAM with LiDAR and cameras, metric semantic
mapping, and a task-aware frontier exploration policy. An approach planner then
selects visibility and reachability aware pre grasp base poses. For
interaction, a compact SmolVLA manipulation head is fine tuned on platform pick
and place trajectories for the SO-101 by TheRobotStudio, grounding local visual
context and sub-goals into grasp and place proposals. The full system runs
fully onboard on consumer-level hardware, with Jetson Orin NX for perception
and VLA and an Intel NUC for SLAM, exploration, and control, sustaining
real-time operation. We evaluated AnywhereVLA in a multi-room lab under static
scenes and normal human motion. In this setting, the system achieves a $46\%$
overall task success rate while maintaining throughput on embedded compute. By
combining a classical stack with a fine-tuned VLA manipulation, the system
inherits the reliability of geometry-based navigation with the agility and task
generalization of language-conditioned manipulation.

</details>


### [33] [Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting](https://arxiv.org/abs/2509.21020)
*Abdelaziz Shaarawy,Cansu Erdogan,Rustam Stolkin,Alireza Rastegarpanah*

Main category: cs.RO

TL;DR: 提出了一种四层任务与运动规划框架，用于电动汽车电池拆卸的多机器人协调，通过结合符号任务规划和基于演示学习的运动规划，显著提升了运动紧凑性和安全性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车电池拆卸需要精确的多机器人协调、短而可靠的运动，以及在杂乱动态场景中的鲁棒碰撞安全性。

Method: 采用四层任务与运动规划框架，结合符号任务规划和成本可及性分配，使用TP-GMM引导的运动规划器从演示中学习，通过立体视觉和YOLOv8进行实时组件定位，OctoMap 3D映射和FCL碰撞检测统一预测性数字孪生碰撞检查与反应性视觉避障。

Result: 在两个UR10e机器人上验证，相比默认RRTConnect基线，平均末端执行器路径长度减少63.3%，制造周期减少8.1%，单臂扫掠体积显著减小（R1: 0.583→0.139 m³; R2: 0.696→0.252 m³），相互重叠减少47%（0.064→0.034 m³）。

Conclusion: 该方法在非结构化动态环境中显著提高了多机器人电动汽车电池拆卸的自主性、精度和安全性。

Abstract: Electric-vehicle (EV) battery disassembly requires precise multi-robot
coordination, short and reliable motions, and robust collision safety in
cluttered, dynamic scenes. We propose a four-layer task-and-motion planning
(TAMP) framework that couples symbolic task planning and cost- and
accessibility-aware allocation with a TP-GMM-guided motion planner learned from
demonstrations. Stereo vision with YOLOv8 provides real-time component
localization, while OctoMap-based 3D mapping and FCL(Flexible Collision
Library) checks in MoveIt unify predictive digital-twin collision checking with
reactive, vision-based avoidance. Validated on two UR10e robots across cable,
busbar, service plug, and three leaf-cell removals, the approach yields
substantially more compact and safer motions than a default RRTConnect baseline
under identical perception and task assignments: average end-effector path
length drops by $-63.3\%$ and makespan by $-8.1\%$; per-arm swept volumes
shrink (R1: $0.583\rightarrow0.139\,\mathrm{m}^3$; R2:
$0.696\rightarrow0.252\,\mathrm{m}^3$), and mutual overlap decreases by $47\%$
($0.064\rightarrow0.034\,\mathrm{m}^3$). These results highlight improved
autonomy, precision, and safety for multi-robot EV battery disassembly in
unstructured, dynamic environments.

</details>


### [34] [KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models](https://arxiv.org/abs/2509.21027)
*Sibo Li,Qianyue Hao,Yu Shang,Yong Li*

Main category: cs.RO

TL;DR: KeyWorld是一个改进文本条件机器人世界模型的框架，通过将计算集中在少数语义关键帧上，使用轻量级卷积模型填充中间帧，实现5.68倍加速并提升生成视频的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人世界模型存在推理速度慢和生成轨迹物理合理性不足的问题，主要原因是帧到帧生成方法的冗余计算以及忽视关键过渡的语义重要性。

Method: 1) 通过迭代简化机器人运动轨迹识别关键帧；2) 训练DiT模型从文本任务描述生成物理意义关键帧；3) 使用轻量级插值器高效重建完整视频。

Result: 在LIBERO基准测试中，KeyWorld相比帧到帧生成基线实现5.68倍加速，在复杂任务上特别提升了生成视频的物理有效性。

Conclusion: 该方法为在实时机器人控制和其他需要高效有效世界模型的领域部署世界模型提供了一条实用路径。

Abstract: Robotic world models are a promising paradigm for forecasting future
environment states, yet their inference speed and the physical plausibility of
generated trajectories remain critical bottlenecks, limiting their real-world
applications. This stems from the redundancy of the prevailing frame-to-frame
generation approach, where the model conducts costly computation on similar
frames, as well as neglecting the semantic importance of key transitions. To
address this inefficiency, we propose KeyWorld, a framework that improves
text-conditioned robotic world models by concentrating transformers computation
on a few semantic key frames while employing a lightweight convolutional model
to fill the intermediate frames. Specifically, KeyWorld first identifies
significant transitions by iteratively simplifying the robot's motion
trajectories, obtaining the ground truth key frames. Then, a DiT model is
trained to reason and generate these physically meaningful key frames from
textual task descriptions. Finally, a lightweight interpolator efficiently
reconstructs the full video by inpainting all intermediate frames. Evaluations
on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\times$
acceleration compared to the frame-to-frame generation baseline, and focusing
on the motion-aware key frames further contributes to the physical validity of
the generated videos, especially on complex tasks. Our approach highlights a
practical path toward deploying world models in real-time robotic control and
other domains requiring both efficient and effective world models. Code is
released at https://anonymous.4open.science/r/Keyworld-E43D.

</details>


### [35] [MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation](https://arxiv.org/abs/2509.21045)
*Mahya Ramezani,M. Amin Alandihallaj,Barış Can Yalçın,Miguel Angel Olivares Mendez,Holger Voos*

Main category: cs.RO

TL;DR: 提出了一种结合强化学习和模型预测控制的框架，用于解决部分燃料箱卫星自主对接中的燃料晃动问题，通过PPO和SAC算法与MPC集成，提高控制鲁棒性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统对接控制面临微重力环境下燃料晃动引起的不可预测力，影响稳定性。需要开发能够处理燃料晃动干扰的鲁棒控制方法，以支持在轨加注和服务任务。

Method: 集成PPO和SAC强化学习算法与模型预测控制，利用MPC的预测能力加速RL训练并提高控制鲁棒性。通过零重力实验室实验验证平面稳定性，并通过高保真数值模拟验证6自由度对接。

Result: 仿真结果表明，SAC-MPC方法在对接精度、成功率和控制效率方面表现最优，优于单独的RL和PPO-MPC方法。

Conclusion: 该研究推进了燃料高效和抗干扰的卫星对接技术，提高了在轨加注和服务任务的可行性。

Abstract: This paper presents an integrated Reinforcement Learning (RL) and Model
Predictive Control (MPC) framework for autonomous satellite docking with a
partially filled fuel tank. Traditional docking control faces challenges due to
fuel sloshing in microgravity, which induces unpredictable forces affecting
stability. To address this, we integrate Proximal Policy Optimization (PPO) and
Soft Actor-Critic (SAC) RL algorithms with MPC, leveraging MPC's predictive
capabilities to accelerate RL training and improve control robustness. The
proposed approach is validated through Zero-G Lab of SnT experiments for planar
stabilization and high-fidelity numerical simulations for 6-DOF docking with
fuel sloshing dynamics. Simulation results demonstrate that SAC-MPC achieves
superior docking accuracy, higher success rates, and lower control effort,
outperforming standalone RL and PPO-MPC methods. This study advances
fuel-efficient and disturbance-resilient satellite docking, enhancing the
feasibility of on-orbit refueling and servicing missions.

</details>


### [36] [Normalizing Flows are Capable Visuomotor Policy Learning Models](https://arxiv.org/abs/2509.21073)
*Simon Kristoffersson Lind,Jialong Li,Maj Stenmark,Volker Krüger*

Main category: cs.RO

TL;DR: 提出了基于归一化流的Normalizing Flows Policy，作为扩散模型的替代方案，在保持性能的同时提供置信度测量和30倍推理速度提升


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的机器人策略存在推理计算成本高和无法量化输出不确定性的问题，而模型的可信度对可靠通用机器人至关重要

Method: 使用归一化流构建视觉运动策略学习模型，通过可逆变换提供统计上合理的置信度测量和高效推理

Result: 在四个模拟机器人任务中，性能与扩散策略相当或更好，样本效率提升，推理速度提升高达30倍

Conclusion: 归一化流是扩散模型的有力替代方案，能够提供置信度测量和高效推理，适用于通用机器人领域

Abstract: The field of general purpose robotics has recently embraced powerful
probabilistic models, such as diffusion models, to model and learn complex
behaviors. However, these models often come with significant trade-offs, namely
high computational costs for inference and a fundamental inability to quantify
output uncertainty. We argue that a model's trustworthiness, a critical factor
for reliable, general-purpose robotics, is inherently linked to its ability to
provide confidence measures.
  In this work, we introduce Normalizing Flows Policy, a novel visuomotor
policy learning model based on Normalizing Flows. We show that Normalizing
Flows are a natural and powerful alternative to diffusion models, providing
both a statistically sound measure of confidence and a highly efficient
inference process. Through comprehensive experiments across four distinct
simulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves
performance comparable to, and often surpassing, Diffusion Policy, and it does
so not only with improved sample efficiency but also with up to 30 times faster
inference. Additionally, our ablation study validates several key architectural
and training techniques that enable Normalizing Flows to perform well in this
domain.

</details>


### [37] [Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect for Accurate Edge Detection](https://arxiv.org/abs/2509.21085)
*Chenyu Zhao,Jingao Xu,Ciyu Ruan,Haoyang Wang,Shengbo Wang,Jiaqi Li,Jirong Zha,Weijie Hong,Zheng Yang,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.RO

TL;DR: AirTouch系统将地面效应从飞行控制的'敌人'转变为'朋友'，利用无人机姿态传感器读数检测地面效应变化，实现高效准确的边缘检测，无需额外传感器。


<details>
  <summary>Details</summary>
Motivation: 现有基于雷达或摄像头的无人机边缘检测方法部署成本高、计算负担重，而地面效应通常被视为飞行稳定性问题，本文旨在将其转化为新的感知模式。

Method: 通过理论分析、算法设计和系统实现，利用无人机基本姿态传感器读数和飞行命令检测地面效应变化，这些变化通常表明无人机飞越两种材料的边界。

Result: 系统实现了高检测精度，平均检测距离误差为0.051米，比基线方法性能提升86%，仅需43毫瓦功耗。

Conclusion: 地面效应可作为低成本、高效率边缘检测的新感知模式，在资源效率和检测能力方面具有独特优势。

Abstract: Drone-based rapid and accurate environmental edge detection is highly
advantageous for tasks such as disaster relief and autonomous navigation.
Current methods, using radars or cameras, raise deployment costs and burden
lightweight drones with high computational demands. In this paper, we propose
AirTouch, a system that transforms the ground effect from a stability "foe" in
traditional flight control views, into a "friend" for accurate and efficient
edge detection. Our key insight is that analyzing drone basic attitude sensor
readings and flight commands allows us to detect ground effect changes. Such
changes typically indicate the drone flying over a boundary of two materials,
making this information valuable for edge detection. We approach this insight
through theoretical analysis, algorithm design, and implementation, fully
leveraging the ground effect as a new sensing modality without compromising
drone flight stability, thereby achieving accurate and efficient scene edge
detection. We also compare this new sensing modality with vision-based methods
to clarify its exclusive advantages in resource efficiency and detection
capability. Extensive evaluations demonstrate that our system achieves a high
detection accuracy with mean detection distance errors of 0.051m, outperforming
the baseline method performance by 86%. With such detection performance, our
system requires only 43 mW power consumption, contributing to this new sensing
modality for low-cost and highly efficient edge detection.

</details>


### [38] [Cross-Modal Instructions for Robot Motion Generation](https://arxiv.org/abs/2509.21107)
*William Barron,Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: CrossInstruct框架使用跨模态指令（如文本标签）替代物理演示来指导机器人学习行为，通过整合视觉语言模型和精细指向模型生成3D运动轨迹，并支持后续强化学习优化。


<details>
  <summary>Details</summary>
Motivation: 传统机器人行为学习需要物理演示，数据收集困难且难以扩展。本文探索使用粗略标注（如文本标签）作为替代演示方式，以解决数据收集和扩展性问题。

Method: 提出CrossInstruct框架，将跨模态指令作为上下文输入到基础视觉语言模型（VLM）中，VLM迭代查询较小的微调模型，在多个2D视图中合成期望运动，然后融合成3D运动轨迹分布。

Result: 在基准仿真任务和真实硬件上的严格评估表明，CrossInstruct无需额外微调即可有效工作，并为后续强化学习提供了良好的策略初始化。

Conclusion: CrossInstruct通过跨模态指令实现了机器人行为的有效学习，超越了有限指令示例的环境限制，并为精细任务学习提供了高效途径。

Abstract: Teaching robots novel behaviors typically requires motion demonstrations via
teleoperation or kinaesthetic teaching, that is, physically guiding the robot.
While recent work has explored using human sketches to specify desired
behaviors, data collection remains cumbersome, and demonstration datasets are
difficult to scale. In this paper, we introduce an alternative paradigm,
Learning from Cross-Modal Instructions, where robots are shaped by
demonstrations in the form of rough annotations, which can contain free-form
text labels, and are used in lieu of physical motion. We introduce the
CrossInstruct framework, which integrates cross-modal instructions as examples
into the context input to a foundational vision-language model (VLM). The VLM
then iteratively queries a smaller, fine-tuned model, and synthesizes the
desired motion over multiple 2D views. These are then subsequently fused into a
coherent distribution over 3D motion trajectories in the robot's workspace. By
incorporating the reasoning of the large VLM with a fine-grained pointing
model, CrossInstruct produces executable robot behaviors that generalize beyond
the environment of in the limited set of instruction examples. We then
introduce a downstream reinforcement learning pipeline that leverages
CrossInstruct outputs to efficiently learn policies to complete fine-grained
tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and
real hardware, demonstrating effectiveness without additional fine-tuning and
providing a strong initialization for policies subsequently refined via
reinforcement learning.

</details>


### [39] [Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study](https://arxiv.org/abs/2509.21122)
*Mingjiang Liu,Hailong Huang*

Main category: cs.RO

TL;DR: 提出分层控制框架，用强化学习训练高层平衡策略，在无人机球平衡任务中优于PID控制器，优势源于更有效地利用状态观测信息。


<details>
  <summary>Details</summary>
Motivation: 解决无人机通过缆绳交互稳定球体在可移动梁上的平衡任务，探索强化学习在复杂控制任务中的潜力。

Method: 分层控制框架：高层平衡策略使用强化学习训练，低层无人机控制分离处理；与PID控制器在相同结构下进行对比。

Result: 强化学习策略在模拟中表现优于精心调参的PID控制器；优势主要来自更有效地利用丰富的状态观测信息，而非参数调优或非线性映射能力。

Conclusion: 综合状态表示在学习型系统中至关重要，增强感知能力可显著提升控制器性能。

Abstract: This paper addresses a drone ball-balancing task, in which a drone stabilizes
a ball atop a movable beam through cable-based interaction. We propose a
hierarchical control framework that decouples high-level balancing policy from
low-level drone control, and train a reinforcement learning (RL) policy to
handle the high-level decision-making. Simulation results show that the RL
policy achieves superior performance compared to carefully tuned PID
controllers within the same hierarchical structure. Through systematic
comparative analysis, we demonstrate that RL's advantage stems not from
improved parameter tuning or inherent nonlinear mapping capabilities, but from
its ability to effectively utilize richer state observations. These findings
underscore the critical role of comprehensive state representation in
learning-based systems and suggest that enhanced sensing could be instrumental
in improving controller performance.

</details>


### [40] [Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems](https://arxiv.org/abs/2509.21143)
*Junfeng Yan,Biao Wu,Meng Fang,Ling Chen*

Main category: cs.RO

TL;DR: 提出了首个针对车载GUI的高保真基准测试平台Automotive-ENV和地理感知多模态代理ASURADA，通过GPS上下文集成提升车载环境下的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态代理在通用GUI交互中表现良好，但在车载系统中的应用尚未充分探索。车载GUI面临驾驶员注意力有限、严格安全要求和复杂基于位置的交互模式等独特挑战。

Method: 开发Automotive-ENV基准测试平台，包含185个参数化任务，涵盖显式控制、隐式意图理解和安全感知任务。提出ASURADA地理感知多模态代理，集成GPS信息根据位置、环境条件和区域驾驶规范动态调整动作。

Result: 实验表明地理感知信息显著提高了安全感知任务的成功率，突出了基于位置的上下文在车载环境中的重要性。

Conclusion: Automotive-ENV基准测试平台和ASURADA代理为开发安全、自适应的车载代理提供了重要工具，强调了位置感知在车载GUI交互中的关键作用。

Abstract: Multimodal agents have demonstrated strong performance in general GUI
interactions, but their application in automotive systems has been largely
unexplored. In-vehicle GUIs present distinct challenges: drivers' limited
attention, strict safety requirements, and complex location-based interaction
patterns. To address these challenges, we introduce Automotive-ENV, the first
high-fidelity benchmark and interaction environment tailored for vehicle GUIs.
This platform defines 185 parameterized tasks spanning explicit control,
implicit intent understanding, and safety-aware tasks, and provides structured
multimodal observations with precise programmatic checks for reproducible
evaluation. Building on this benchmark, we propose ASURADA, a geo-aware
multimodal agent that integrates GPS-informed context to dynamically adjust
actions based on location, environmental conditions, and regional driving
norms. Experiments show that geo-aware information significantly improves
success on safety-aware tasks, highlighting the importance of location-based
context in automotive environments. We will release Automotive-ENV, complete
with all tasks and benchmarking tools, to further the development of safe and
adaptive in-vehicle agents.

</details>


### [41] [DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps](https://arxiv.org/abs/2509.21145)
*Md Faizal Karim,Vignesh Vembar,Keshab Patra,Gaurav Singh,K Madhava Krishna*

Main category: cs.RO

TL;DR: DAGDiff是一个端到端的双机械臂抓取框架，通过扩散模型在SE(3)×SE(3)空间中直接生成抓取对，利用分类器信号引导生成过程，确保稳定性和避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 可靠的双机械臂抓取对于操作大型复杂物体至关重要，但现有方法通常将任务分解为两个独立的抓取提案，依赖区域先验或启发式方法，限制了泛化能力且无法提供稳定性保证。

Method: 提出DAGDiff框架，直接在SE(3)×SE(3)空间中通过扩散过程去噪生成抓取对，集成几何、稳定性和碰撞感知的引导项，引导生成过程产生物理有效且满足力闭合要求的抓取。

Result: 通过分析力闭合检查、碰撞分析和大规模物理仿真全面评估DAGDiff，在各项指标上均优于先前工作，并在真实世界点云上直接生成双机械臂抓取，在异构双机械臂系统上成功执行。

Conclusion: DAGDiff通过扩散模型和分类器引导，有效解决了双机械臂抓取的稳定性、碰撞和泛化问题，为复杂物体操作提供了可靠的解决方案。

Abstract: Reliable dual-arm grasping is essential for manipulating large and complex
objects but remains a challenging problem due to stability, collision, and
generalization requirements. Prior methods typically decompose the task into
two independent grasp proposals, relying on region priors or heuristics that
limit generalization and provide no principled guarantee of stability. We
propose DAGDiff, an end-to-end framework that directly denoises to grasp pairs
in the SE(3) x SE(3) space. Our key insight is that stability and collision can
be enforced more effectively by guiding the diffusion process with classifier
signals, rather than relying on explicit region detection or object priors. To
this end, DAGDiff integrates geometry-, stability-, and collision-aware
guidance terms that steer the generative process toward grasps that are
physically valid and force-closure compliant. We comprehensively evaluate
DAGDiff through analytical force-closure checks, collision analysis, and
large-scale physics-based simulations, showing consistent improvements over
previous work on these metrics. Finally, we demonstrate that our framework
generates dual-arm grasps directly on real-world point clouds of previously
unseen objects, which are executed on a heterogeneous dual-arm setup where two
manipulators reliably grasp and lift them.

</details>


### [42] [Human-like Navigation in a World Built for Humans](https://arxiv.org/abs/2509.21189)
*Bhargav Chandaka,Gloria X. Wang,Haozhe Chen,Henry Che,Albert J. Zhai,Shenlong Wang*

Main category: cs.RO

TL;DR: ReasonNav是一个模块化导航系统，利用视觉语言模型的推理能力集成人类导航技能，在大型环境中实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航系统缺乏人类导航行为（如阅读标志、询问方向），导致在大型环境中导航效率低下。

Method: 设计基于导航地标的紧凑输入输出抽象，让视觉语言模型专注于语言理解和推理，集成人类导航技能。

Result: 在真实和模拟导航任务中评估显示，智能体成功运用高阶推理在大型复杂建筑中高效导航。

Conclusion: ReasonNav通过集成人类导航行为和视觉语言模型推理能力，显著提升了机器人在大型环境中的导航效率。

Abstract: When navigating in a man-made environment they haven't visited before--like
an office building--humans employ behaviors such as reading signs and asking
others for directions. These behaviors help humans reach their destinations
efficiently by reducing the need to search through large areas. Existing robot
navigation systems lack the ability to execute such behaviors and are thus
highly inefficient at navigating within large environments. We present
ReasonNav, a modular navigation system which integrates these human-like
navigation skills by leveraging the reasoning capabilities of a vision-language
model (VLM). We design compact input and output abstractions based on
navigation landmarks, allowing the VLM to focus on language understanding and
reasoning. We evaluate ReasonNav on real and simulated navigation tasks and
show that the agent successfully employs higher-order reasoning to navigate
efficiently in large, complex buildings.

</details>


### [43] [Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis](https://arxiv.org/abs/2509.21210)
*Ali Kafili Gavgani,Amin Talaeizadeh,Aria Alasty,Hossein Nejat Pishkenari,Esmaeil Najafi*

Main category: cs.RO

TL;DR: 本研究提出了几种新型多旋翼无人机配置，通过增加螺旋桨轴角度控制输入实现全向姿态控制，解决了传统多旋翼欠驱动系统的限制。设计了滑模控制器和新型PID控制器，并开发了定制控制分配策略来优化功耗。


<details>
  <summary>Details</summary>
Motivation: 传统多旋翼无人机是欠驱动系统，无法独立控制姿态和位置。本研究旨在通过增加螺旋桨轴角度控制来解决这一限制，实现全向控制能力。

Method: 提出了多种配置方案，详细推导了动力学模型，使用Simscape Multibody进行仿真验证。设计了滑模控制器和新型PID控制器（含重力补偿和线性/非线性分配器），并开发了定制控制分配策略来管理输入非仿射特性。

Result: 控制器能有效处理剧烈扰动和不确定性。通过仿真比较了不同配置和控制器的功耗性能，并进行了不确定性对控制系统影响的定性比较。

Conclusion: 本研究为未来研究人员设计全向无人机提供了路线图，为配置选择和控制器设计提供了实用见解，是Sharif AgRoLab项目SAC-1的一部分。

Abstract: Conventional multi-rotors are under-actuated systems, hindering them from
independently controlling attitude from position. In this study, we present
several distinct configurations that incorporate additional control inputs for
manipulating the angles of the propeller axes. This addresses the mentioned
limitations, making the systems "omniorientational". We comprehensively derived
detailed dynamic models for all introduced configurations and validated by a
methodology using Simscape Multibody simulations. Two controllers are designed:
a sliding mode controller for robust handling of disturbances and a novel
PID-based controller with gravity compensation integrating linear and
non-linear allocators, designed for computational efficiency. A custom control
allocation strategy is implemented to manage the input-non-affine nature of
these systems, seeking to maximize battery life by minimizing the "Power
Consumption Factor" defined in this study. Moreover, the controllers
effectively managed harsh disturbances and uncertainties. Simulations compare
and analyze the proposed configurations and controllers, majorly considering
their power consumption. Furthermore, we conduct a qualitative comparison to
evaluate the impact of different types of uncertainties on the control system,
highlighting areas for potential model or hardware improvements. The analysis
in this study provides a roadmap for future researchers to design
omniorientational drones based on their design objectives, offering practical
insights into configuration selection and controller design. This research
aligns with the project SAC-1, one of the objectives of Sharif AgRoLab.

</details>


### [44] [SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation](https://arxiv.org/abs/2509.21231)
*Jaehwi Jang,Zhuoheng Wang,Ziyi Zhou,Feiyang Wu,Ye Zhao*

Main category: cs.RO

TL;DR: 提出SEEC框架，通过模型增强的残差学习和扰动生成器，实现人形机器人末端执行器的精确稳定控制，能够适应未见过的运动控制器而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人末端执行器稳定控制的挑战，传统模型方法依赖精确动力学建模而难以处理现实因素，学习类方法容易过拟合训练条件且难以适应新场景。

Method: 采用模型增强的残差学习框架，结合模型引导的强化学习和扰动生成器，学习对下半身引起扰动的精确鲁棒补偿。

Result: 在不同仿真器中验证，并将训练策略转移到Booster T1人形机器人，实验表明该方法始终优于基线，并能鲁棒处理多样化的移动操作任务。

Conclusion: SEEC框架能够实现精确鲁棒的末端执行器稳定控制，且具有良好适应性，无需额外训练即可适应新的运动控制器。

Abstract: Arm end-effector stabilization is essential for humanoid loco-manipulation
tasks, yet it remains challenging due to the high degrees of freedom and
inherent dynamic instability of bipedal robot structures. Previous model-based
controllers achieve precise end-effector control but rely on precise dynamics
modeling and estimation, which often struggle to capture real-world factors
(e.g., friction and backlash) and thus degrade in practice. On the other hand,
learning-based methods can better mitigate these factors via exploration and
domain randomization, and have shown potential in real-world use. However, they
often overfit to training conditions, requiring retraining with the entire
body, and still struggle to adapt to unseen scenarios. To address these
challenges, we propose a novel stable end-effector control (SEEC) framework
with model-enhanced residual learning that learns to achieve precise and robust
end-effector compensation for lower-body induced disturbances through
model-guided reinforcement learning (RL) with a perturbation generator. This
design allows the upper-body policy to achieve accurate end-effector
stabilization as well as adapt to unseen locomotion controllers with no
additional training. We validate our framework in different simulators and
transfer trained policies to the Booster T1 humanoid robot. Experiments
demonstrate that our method consistently outperforms baselines and robustly
handles diverse and demanding loco-manipulation tasks.

</details>


### [45] [FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration](https://arxiv.org/abs/2509.21242)
*Yutong Li,Jieyi Zhang,Wenqiang Xu,Tutian Tang,Cewu Lu*

Main category: cs.RO

TL;DR: FSGlove是一个基于惯性的手部动作捕捉系统，能同时跟踪48个自由度并重建个性化手部形状，通过DiffHCal校准方法实现高精度运动捕捉和形状重建。


<details>
  <summary>Details</summary>
Motivation: 现有商业手套最多只能提供21个自由度，不足以捕捉复杂的手部动作，且忽略了对手部接触任务至关重要的形状变化。

Method: 系统在每个手指关节和手背配备IMU传感器，通过DiffHCal方法与参数化MANO模型集成，通过可微分优化在一次流线型校准中解决关节运动学、形状参数和传感器错位问题。

Result: 系统达到最先进的精度，关节角度误差小于2.7度，在形状重建和接触保真度方面优于商业替代方案，能够捕捉细微动作如指尖摩擦。

Conclusion: FSGlove通过统一运动学和接触保真度推进了手部跟踪技术，其开源硬件和软件设计确保与当前VR和机器人生态系统的兼容性。

Abstract: Accurate hand motion capture (MoCap) is vital for applications in robotics,
virtual reality, and biomechanics, yet existing systems face limitations in
capturing high-degree-of-freedom (DoF) joint kinematics and personalized hand
shape. Commercial gloves offer up to 21 DoFs, which are insufficient for
complex manipulations while neglecting shape variations that are critical for
contact-rich tasks. We present FSGlove, an inertial-based system that
simultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes
via DiffHCal, a novel calibration method. Each finger joint and the dorsum are
equipped with IMUs, enabling high-resolution motion sensing. DiffHCal
integrates with the parametric MANO model through differentiable optimization,
resolving joint kinematics, shape parameters, and sensor misalignment during a
single streamlined calibration. The system achieves state-of-the-art accuracy,
with joint angle errors of less than 2.7 degree, and outperforms commercial
alternatives in shape reconstruction and contact fidelity. FSGlove's
open-source hardware and software design ensures compatibility with current VR
and robotics ecosystems, while its ability to capture subtle motions (e.g.,
fingertip rubbing) bridges the gap between human dexterity and robotic
imitation. Evaluated against Nokov optical MoCap, FSGlove advances hand
tracking by unifying the kinematic and contact fidelity. Hardware design,
software, and more results are available at:
https://sites.google.com/view/fsglove.

</details>


### [46] [RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2509.21243)
*Jiyeon Koo,Taewan Cho,Hyunjoon Kang,Eunseom Pyo,Tae Gyun Oh,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: RetoVLA是一种轻量级视觉-语言-动作模型，通过重用Vision Transformer中被丢弃的Register Tokens来增强空间推理能力，在保持轻量结构的同时显著提升机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型虽然泛化能力强但计算成本高，传统轻量化方法会牺牲关键的空间推理能力，需要在效率和性能之间权衡。

Method: 提出RetoVA架构，重用Vision Transformer中为去除伪影而引入但随后被丢弃的Register Tokens，将这些包含重要空间信息的tokens直接注入到Action Expert中。

Result: 在7自由度机械臂上，复杂操作任务的成功率绝对提升了17.1个百分点，证明重用Register Tokens能有效增强空间推理。

Conclusion: 被丢弃的Register Tokens实际上是机器人智能中宝贵的未开发资源，直接重用它们能够在不牺牲性能的情况下实现模型轻量化。

Abstract: Recent Vision-Language-Action (VLA) models demonstrate remarkable
generalization in robotics but are restricted by their substantial size and
computational cost, limiting real-world deployment. However, conventional
lightweighting methods often sacrifice critical capabilities, particularly
spatial reasoning. This creates a trade-off between efficiency and performance.
To address this challenge, our work reuses Register Tokens, which were
introduced for artifact removal in Vision Transformers but subsequently
discarded. We suppose that these tokens contain essential spatial information
and propose RetoVLA, a novel architecture that reuses them directly by
injecting them into the Action Expert.
  RetoVLA maintains a lightweight structure while leveraging this repurposed
spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness
through a series of comprehensive experiments. On our custom-built 7-DOF robot
arm, the model achieves a 17.1%p absolute improvement in success rates for
complex manipulation tasks. Our results confirm that reusing Register Tokens
directly enhances spatial reasoning, demonstrating that what was previously
discarded as an artifact is in fact a valuable, unexplored resource for robotic
intelligence. A video demonstration is available at:
https://youtu.be/2CseBR-snZg

</details>


### [47] [BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives](https://arxiv.org/abs/2509.21256)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: 提出了BiNoMaP框架，通过双手机器人配置和几何感知优化方法，从视频演示中学习非抓取操作技能，实现类别级泛化。


<details>
  <summary>Details</summary>
Motivation: 非抓取操作在机器人领域研究不足，现有方法依赖单臂配置或外部辅助结构，且主要基于强化学习，存在局限性。

Method: 三阶段无强化学习框架：从视频提取双手运动轨迹，几何感知后优化生成可执行操作原语，参数化原语实现类别级泛化。

Result: 在多种代表性双人任务和不同物体类别上验证了BiNoMaP的有效性、效率、多功能性和优越的泛化能力。

Conclusion: BiNoMaP为双手机器人非抓取操作提供了通用解决方案，无需强化学习即可从演示中学习技能并实现泛化。

Abstract: Non-prehensile manipulation, encompassing ungraspable actions such as
pushing, poking, and pivoting, represents a critical yet underexplored domain
in robotics due to its contact-rich and analytically intractable nature. In
this work, we revisit this problem from two novel perspectives. First, we move
beyond the usual single-arm setup and the strong assumption of favorable
external dexterity such as walls, ramps, or edges. Instead, we advocate a
generalizable dual-arm configuration and establish a suite of Bimanual
Non-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the
prevailing RL-based paradigm and propose a three-stage, RL-free framework to
learn non-prehensile skills. Specifically, we begin by extracting bimanual hand
motion trajectories from video demonstrations. Due to visual inaccuracies and
morphological gaps, these coarse trajectories are difficult to transfer
directly to robotic end-effectors. To address this, we propose a geometry-aware
post-optimization algorithm that refines raw motions into executable
manipulation primitives that conform to specific motion patterns. Beyond
instance-level reproduction, we further enable category-level generalization by
parameterizing the learned primitives with object-relevant geometric
attributes, particularly size, resulting in adaptable and general parameterized
manipulation primitives. We validate BiNoMaP across a range of representative
bimanual tasks and diverse object categories, demonstrating its effectiveness,
efficiency, versatility, and superior generalization capability.

</details>


### [48] [\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)](https://arxiv.org/abs/2509.21264)
*Babak Salamat,Dominik Mattern,Sebastian-Sven Olzem,Gerhard Elsbacher,Christian Seidel,Andrea M. Tonello*

Main category: cs.RO

TL;DR: GMP³是一个多阶段全局路径规划框架，为无人机在复杂环境中生成动态可行的三维轨迹。该框架将传统路径规划从欧几里得位置空间扩展到SE(3)李群，联合学习平移运动和旋转动力学，并通过分布式代理协作实现全局路径优化。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划方法主要关注位置空间，难以处理无人机在复杂三维环境中的动态约束和姿态控制需求。需要开发能够同时考虑平移和旋转动力学，并支持多代理协作的路径规划框架。

Method: 提出基于李群SE(3)的路径规划框架，引入改进的Bellman算子支持强化学习策略更新，采用分布式架构使代理通过共识机制共享策略信息，每个代理优化其分配段并与邻居协作。

Result: 仿真和室内飞行实验验证了该方法在受限三维环境中的有效性，实现了可靠的障碍物避让和位置、姿态的平滑可行轨迹。

Conclusion: GMP³框架成功解决了无人机在复杂环境中的三维轨迹规划问题，通过李群表示和分布式协作实现了动态可行的路径生成，并通过DroneManager软件支持实时部署。

Abstract: We propose $\text{GMP}^{3}$, a multiphase global path planning framework that
generates dynamically feasible three-dimensional trajectories for unmanned
aerial vehicles (UAVs) operating in cluttered environments. The framework
extends traditional path planning from Euclidean position spaces to the Lie
group $\mathrm{SE}(3)$, allowing joint learning of translational motion and
rotational dynamics. A modified Bellman-based operator is introduced to support
reinforcement learning (RL) policy updates while leveraging prior trajectory
information for improved convergence. $\text{GMP}^{3}$ is designed as a
distributed framework in which agents influence each other and share policy
information along the trajectory: each agent refines its assigned segment and
shares with its neighbors via a consensus-based scheme, enabling cooperative
policy updates and convergence toward a path shaped globally even under
kinematic constraints. We also propose DroneManager, a modular ground control
software that interfaces the planner with real UAV platforms via the MAVLink
protocol, supporting real-time deployment and feedback. Simulation studies and
indoor flight experiments validate the effectiveness of the proposed method in
constrained 3D environments, demonstrating reliable obstacle avoidance and
smooth, feasible trajectories across both position and orientation. The
open-source implementation is available at
https://github.com/Domattee/DroneManager

</details>


### [49] [Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds](https://arxiv.org/abs/2509.21281)
*Luis Augenstein,Noémie Jaquier,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: 本文提出了GPHDM模型，通过将高斯过程动力学模型扩展到双曲流形并整合分类学感知的归纳偏置，来生成既保持运动层次结构又具有物理一致性的机器人运动。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成模型经常忽视生物力学研究中复杂的运动层次分类学信息，导致生成的运动与底层层次结构脱节。

Method: 扩展GPDM的动态先验到双曲流形，整合分类学感知的归纳偏置，提出三种新机制：两种概率递归方法和基于拉回度量测地线的方法。

Result: 在手部抓取分类学上的实验表明，GPHDM能够忠实编码底层分类学和时序动态，生成新颖的物理一致轨迹。

Conclusion: GPHDM成功地将运动层次结构和时序动态结合，为机器人运动生成提供了分类学结构化和物理一致的解决方案。

Abstract: Human-like motion generation for robots often draws inspiration from
biomechanical studies, which often categorize complex human motions into
hierarchical taxonomies. While these taxonomies provide rich structural
information about how movements relate to one another, this information is
frequently overlooked in motion generation models, leading to a disconnect
between the generated motions and their underlying hierarchical structure. This
paper introduces the \ac{gphdm}, a novel approach that learns latent
representations preserving both the hierarchical structure of motions and their
temporal dynamics to ensure physical consistency. Our model achieves this by
extending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to
the hyperbolic manifold and integrating it with taxonomy-aware inductive
biases. Building on this geometry- and taxonomy-aware frameworks, we propose
three novel mechanisms for generating motions that are both
taxonomically-structured and physically-consistent: two probabilistic recursive
approaches and a method based on pullback-metric geodesics. Experiments on
generating realistic motion sequences on the hand grasping taxonomy show that
the proposed GPHDM faithfully encodes the underlying taxonomy and temporal
dynamics, and generates novel physically-consistent trajectories.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [50] [Further Results on Rendering Geometric Intersection Graphs Sparse by Dispersion](https://arxiv.org/abs/2509.20903)
*Nicolás Honorato-Droguett,Kazuhiro Kurita,Tesshu Hanaka,Hirotaka Ono,Alexander Wolff*

Main category: cs.CG

TL;DR: 本文研究几何图编辑距离问题，提出算法在O(n log n)时间内将单位圆弧的交图转换为无边、无环或k-团自由图，证明了该问题在无权区间图上强NP难，并给出了加权单位区间交图的XP算法。


<details>
  <summary>Details</summary>
Motivation: 解决调度、可视化和地图标注等领域中的重叠移除问题，该问题可建模为图编辑距离问题，目标是通过移动对象最小化编辑成本。

Method: 使用几何图编辑距离模型，通过移动对象来编辑几何交图结构，提出针对单位圆弧交图的高效算法，并分析不同图类的计算复杂度。

Result: 开发了O(n log n)时间算法处理单位圆弧交图；证明了问题在无权区间图上强NP难；给出了加权单位区间交图的XP算法。

Conclusion: 几何图编辑距离问题在特定图类上可高效求解，但在一般图类上具有计算难度，为重叠移除问题提供了理论和算法基础。

Abstract: Removing overlaps is a central task in domains such as scheduling,
visibility, and map labelling. This task can be modelled using graphs, where
overlap removals correspond to enforcing a certain sparsity constraint on the
graph structure. We continue the study of the problem Geometric Graph Edit
Distance, where the aim is to minimise the total cost of editing a geometric
intersection graph to obtain a graph contained in a specific graph class. For
us, the edit operation is the movement of objects, and the cost is the movement
distance. We present an algorithm for rendering the intersection graph of a set
of unit circular arcs (i)~edgeless, (ii)~acyclic, and (iii)~$k$-clique-free in
$O(n\log n)$ time, where $n$ is the number of arcs. We also show that the
problem remains strongly NP-hard on unweighted interval graphs, solving an open
problem of [Honorato-Droguett et al., WADS 2025]. We complement this result by
showing that the problem is strongly NP-hard on tuples of $d$-balls and
$d$-cubes, for any $d\ge 2$. Finally, we present an XP algorithm (parameterised
by the number of maximal cliques) for rendering the intersection graph of a set
of weighted unit intervals edgeless.

</details>
